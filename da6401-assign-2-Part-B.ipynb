{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11371580,"sourceType":"datasetVersion","datasetId":7118869},{"sourceId":11371756,"sourceType":"datasetVersion","datasetId":7118997}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# Part A","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install wandb pytorch-lightning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:19:31.892965Z","iopub.execute_input":"2025-04-16T16:19:31.893247Z","iopub.status.idle":"2025-04-16T16:19:35.211454Z","shell.execute_reply.started":"2025-04-16T16:19:31.893222Z","shell.execute_reply":"2025-04-16T16:19:35.210761Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.5.1+cu124)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.16)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.19.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom pytorch_lightning import LightningModule, LightningDataModule, Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport math\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seed()\n\n# Configure device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:19:35.213101Z","iopub.execute_input":"2025-04-16T16:19:35.213353Z","iopub.status.idle":"2025-04-16T16:19:35.225499Z","shell.execute_reply.started":"2025-04-16T16:19:35.213332Z","shell.execute_reply":"2025-04-16T16:19:35.224619Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"class CustomCNN(LightningModule):\n    def __init__(self, \n                 num_classes=10,\n                 filter_counts=[32, 32, 64, 64, 128],\n                 filter_sizes=[3, 3, 3, 3, 3],\n                 activation='relu',\n                 dense_neurons=512,\n                 input_channels=3,\n                 input_size=244,\n                 dropout_rate=0.5,\n                 learning_rate=0.001,\n                 batch_norm=False):\n        \"\"\"\n        Custom CNN architecture with flexible hyperparameters\n        \n        Args:\n            num_classes (int): Number of output classes\n            filter_counts (list): Number of filters in each conv layer\n            filter_sizes (list): Size of filters in each conv layer\n            activation (str): Activation function ('relu', 'gelu', 'silu', 'mish')\n            dense_neurons (int): Number of neurons in the dense layer\n            input_channels (int): Number of input channels (3 for RGB)\n            input_size (int): Size of input images (assumes square)\n            dropout_rate (float): Dropout rate\n            learning_rate (float): Learning rate for optimizer\n            batch_norm (bool): Whether to use batch normalization\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Configure activation function\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'gelu':\n            self.activation = nn.GELU()\n        elif activation == 'silu':\n            self.activation = nn.SiLU()\n        elif activation == 'mish':\n            self.activation = nn.Mish()\n        else:\n            self.activation = nn.ReLU()\n        \n        # Build the network\n        self.conv_layers = nn.ModuleList()\n        \n        # Calculate feature map sizes for computational analysis\n        feature_size = input_size\n        feature_sizes = [feature_size]\n        \n        # First convolutional block\n        in_channels = input_channels\n        for i in range(5):\n            out_channels = filter_counts[i]\n            filter_size = filter_sizes[i]\n            \n            # Create convolutional block\n            conv_block = []\n            \n            # Convolutional layer\n            conv_block.append(nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=filter_size//2))\n            \n            # Batch normalization (optional)\n            if batch_norm:\n                conv_block.append(nn.BatchNorm2d(out_channels))\n            \n            # Activation\n            conv_block.append(self.activation)\n            \n            # Max pooling\n            conv_block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            \n            # Add block to model\n            self.conv_layers.append(nn.Sequential(*conv_block))\n            \n            # Update feature size (divided by 2 due to max pooling)\n            feature_size = feature_size // 2\n            feature_sizes.append(feature_size)\n            \n            # Update channels for next layer\n            in_channels = out_channels\n        \n        # Calculate flattened features size\n        self.flattened_size = filter_counts[-1] * feature_size * feature_size\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(self.flattened_size, dense_neurons),\n            self.activation,\n            nn.Dropout(dropout_rate),\n            nn.Linear(dense_neurons, num_classes)\n        )\n        \n        # Store additional parameters\n        self.learning_rate = learning_rate\n        self.num_classes = num_classes\n        self.filter_counts = filter_counts\n        self.filter_sizes = filter_sizes\n        self.feature_sizes = feature_sizes\n        \n        # Calculate parameters and computations\n        self.total_params = self.calculate_total_params()\n        self.total_computations = self.calculate_total_computations()\n        \n        # For storing test predictions - needed for visualization\n        self.test_predictions = []\n        self.test_targets = []\n        self.test_images = []\n        \n    def forward(self, x):\n        \"\"\"Forward pass through the network\"\"\"\n        # Pass through convolutional layers\n        for conv_layer in self.conv_layers:\n            x = conv_layer(x)\n        \n        # Pass through classifier\n        return self.classifier(x)\n    \n    def calculate_total_params(self):\n        \"\"\"\n        Calculate the total number of parameters in the network\n        This answers Question 1: Total parameters with m filters of size k×k and n neurons\n        \"\"\"\n        total = 0\n        \n        # Convolutional layers parameters\n        input_channels = 3\n        for i in range(5):\n            output_channels = self.filter_counts[i]\n            filter_size = self.filter_sizes[i]\n            \n            # Weight parameters: out_channels * in_channels * filter_height * filter_width\n            params = output_channels * input_channels * filter_size * filter_size\n            # Bias parameters: out_channels\n            params += output_channels\n            \n            total += params\n            input_channels = output_channels\n        \n        # Dense layer parameters\n        # First dense layer: flattened_size * dense_neurons + dense_neurons (bias)\n        total += self.flattened_size * self.hparams.dense_neurons + self.hparams.dense_neurons\n        # Output layer: dense_neurons * num_classes + num_classes (bias)\n        total += self.hparams.dense_neurons * self.num_classes + self.num_classes\n        \n        return total\n    \n    def calculate_total_computations(self):\n        \"\"\"\n        Calculate the total number of computations in the network\n        This answers Question 1: Total computations with m filters of size k×k and n neurons\n        \"\"\"\n        total = 0\n        \n        # Convolutional layers computations\n        input_channels = 3\n        for i in range(5):\n            output_channels = self.filter_counts[i]\n            filter_size = self.filter_sizes[i]\n            feature_size = self.feature_sizes[i]\n            \n            # Convolution computations: \n            # out_channels * in_channels * filter_height * filter_width * feature_height * feature_width\n            comp = output_channels * input_channels * filter_size * filter_size * feature_size * feature_size\n            \n            total += comp\n            input_channels = output_channels\n        \n        # Dense layer computations\n        # First dense layer: flattened_size * dense_neurons\n        total += self.flattened_size * self.hparams.dense_neurons\n        # Output layer: dense_neurons * num_classes\n        total += self.hparams.dense_neurons * self.num_classes\n        \n        return total\n    \n    def formula_parameter_count(self, m, k, n):\n        \"\"\"\n        Formula for the total parameter count in terms of m, k, n\n        m: number of filters in each layer\n        k: size of filters (k×k)\n        n: number of neurons in dense layer\n        \"\"\"\n        # For simplicity, assume all conv layers have m filters of size k×k\n        # Layer 1: m filters, each with 3*k*k weights + m biases\n        layer1_params = m * (3 * k * k + 1)\n        \n        # Layer 2-5: m filters, each with m*k*k weights + m biases\n        other_layers_params = 4 * m * (m * k * k + 1)\n        \n        # Calculate feature map size after 5 pooling layers (size/32)\n        final_feature_size = self.hparams.input_size // 32\n        \n        # Feature map size after 5 layers\n        flattened_size = m * final_feature_size * final_feature_size\n        \n        # Dense layer: flattened_size * n + n biases\n        dense_layer_params = flattened_size * n + n\n        \n        # Output layer: n * num_classes + num_classes biases\n        output_layer_params = n * self.num_classes + self.num_classes\n        \n        return layer1_params + other_layers_params + dense_layer_params + output_layer_params\n    \n    def formula_computation_count(self, m, k, n):\n        \"\"\"\n        Formula for the total computation count in terms of m, k, n\n        m: number of filters in each layer\n        k: size of filters (k×k)\n        n: number of neurons in dense layer\n        \"\"\"\n        total_comp = 0\n        input_size = self.hparams.input_size\n        \n        # Layer 1: m filters, each 3*k*k computations per output position\n        layer1_comp = m * 3 * k * k * input_size * input_size\n        total_comp += layer1_comp\n        \n        # Update input size after pooling\n        input_size //= 2\n        \n        # Layers 2-5\n        for i in range(4):\n            layer_comp = m * m * k * k * input_size * input_size\n            total_comp += layer_comp\n            input_size //= 2\n        \n        # Feature map size after 5 layers\n        flattened_size = m * input_size * input_size\n        \n        # Dense layer: flattened_size * n multiplications\n        dense_layer_comp = flattened_size * n\n        \n        # Output layer: n * num_classes multiplications\n        output_layer_comp = n * self.num_classes\n        \n        return total_comp + dense_layer_comp + output_layer_comp\n    \n    def configure_optimizers(self):\n        \"\"\"Configure optimizer\"\"\"\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n    \n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n        \n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        \"\"\"Test step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('test_loss', loss, prog_bar=True)\n        self.log('test_acc', acc, prog_bar=True)\n        \n        # Store images, predictions and targets for later visualization\n        # Use detach to prevent memory leaks\n        self.test_predictions.append(preds.detach().cpu())\n        self.test_targets.append(y.detach().cpu())\n        self.test_images.append(x.detach().cpu())\n        \n        return {'loss': loss, 'preds': preds, 'targets': y}\n    \n    def on_test_epoch_end(self):\n        \"\"\"Process and visualize test results at the end of testing\"\"\"\n        if not self.test_predictions:\n            return\n        \n        # Concatenate all predictions, targets, and images\n        all_preds = torch.cat(self.test_predictions)\n        all_targets = torch.cat(self.test_targets)\n        all_images = torch.cat(self.test_images)\n        \n        # Calculate accuracy\n        accuracy = (all_preds == all_targets).float().mean().item()\n        print(f\"Test accuracy: {accuracy:.4f}\")\n        \n        # Visualize test predictions in a 10×3 grid\n        self.visualize_test_predictions(all_images, all_preds, all_targets)\n        \n        # Visualize first layer filters\n        self.visualize_first_layer_filters()\n        \n        # Perform guided backpropagation on last convolutional layer\n        if len(all_images) > 0:\n            # Take a single image for guided backprop\n            sample_image = all_images[0].unsqueeze(0).to(self.device)\n            self.visualize_guided_backprop(sample_image)\n        \n        # Clear stored test data to free memory\n        self.test_predictions = []\n        self.test_targets = []\n        self.test_images = []\n    \n    def visualize_test_predictions(self, images, predictions, targets):\n        \"\"\"\n        Visualize test images with predictions in a 10×3 grid\n        This addresses Question 4: Providing a 10×3 grid of test images and predictions\n        \"\"\"\n        # Create figure with 10×3 grid\n        fig, axes = plt.subplots(10, 3, figsize=(15, 30))\n        \n        # Get class names if available\n        class_names = None\n        if hasattr(self.trainer, 'datamodule') and hasattr(self.trainer.datamodule, 'test_dataset'):\n            if hasattr(self.trainer.datamodule.test_dataset, 'classes'):\n                class_names = self.trainer.datamodule.test_dataset.classes\n        \n        # Use minimum of 30 samples or available samples\n        num_samples = min(30, len(images))\n        \n        for i in range(num_samples):\n            row, col = i // 3, i % 3\n            \n            # Get image\n            img = images[i].numpy().transpose(1, 2, 0)\n            \n            # De-normalize image\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            img = std * img + mean\n            img = np.clip(img, 0, 1)\n            \n            # Get predicted and target class names\n            pred = predictions[i].item()\n            target = targets[i].item()\n            \n            # Use class names if available, otherwise use class indices\n            pred_name = class_names[pred] if class_names else f\"Class {pred}\"\n            target_name = class_names[target] if class_names else f\"Class {target}\"\n            \n            # Display image\n            axes[row, col].imshow(img)\n            \n            # Set title with color: green if correct, red if wrong\n            color = 'green' if pred == target else 'red'\n            axes[row, col].set_title(f\"Pred: {pred_name}\\nTrue: {target_name}\", color=color)\n            axes[row, col].axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('test_predictions_grid.png')\n        wandb.log({\"test_predictions_grid\": wandb.Image(fig)})\n        plt.close(fig)\n    \n    def visualize_first_layer_filters(self):\n        \"\"\"\n        Visualize filters in the first convolutional layer\n        This addresses the optional part of Question 4\n        \"\"\"\n        # Get weights of the first convolutional layer\n        filters = self.conv_layers[0][0].weight.data.cpu()\n        \n        # Number of filters in the first layer\n        num_filters = filters.shape[0]\n        grid_size = int(np.ceil(np.sqrt(num_filters)))\n        \n        # Create figure for the grid\n        fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n        \n        # Plot each filter\n        for i, ax in enumerate(axes.flat):\n            if i < num_filters:\n                # Get the filter\n                filter_weights = filters[i]\n                \n                # Normalize for better visualization\n                # Convert to numpy and transpose to (H, W, C)\n                f_np = filter_weights.permute(1, 2, 0).numpy()\n                \n                # Normalize to [0, 1]\n                f_np = (f_np - f_np.min()) / (f_np.max() - f_np.min() + 1e-8)\n                \n                # Display the filter\n                ax.imshow(f_np)\n                ax.set_title(f\"Filter {i+1}\")\n            \n            # Turn off axis for all subplots\n            ax.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('first_layer_filters.png')\n        wandb.log({\"first_layer_filters\": wandb.Image(fig)})\n        plt.close(fig)\n    \n    def visualize_guided_backprop(self, input_image):\n        \"\"\"\n        Apply guided back-propagation on neurons in the last conv layer\n        This addresses the optional part of Question 4\n        \n        Args:\n            input_image: Single input image tensor [1, C, H, W]\n        \"\"\"\n        self.eval()  # Set model to evaluation mode\n        \n        # Skip guided backprop if running on CPU as it can be problematic\n        if not torch.cuda.is_available():\n            print(\"Skipping guided backpropagation visualization as it may be unstable on CPU\")\n            return\n        \n        try:\n            # We'll visualize 10 neurons from the last conv layer (CONV5)\n            layer_idx = 4  # 5th layer (0-indexed)\n            num_neurons = 10\n            \n            # Create a copy of the image that requires gradient\n            image = input_image.clone().detach()\n            image.requires_grad_(True)\n            \n            # Forward pass through each layer until the target layer\n            activations = None\n            x = image\n            \n            # Store hooks for guided backprop\n            handles = []\n            \n            # Define hook for backward pass\n            def backward_hook_fn(module, grad_input, grad_output):\n                # In guided backprop, we only pass positive gradients to positive activations\n                if isinstance(module, (nn.ReLU, nn.GELU, nn.SiLU, nn.Mish)):\n                    return (torch.clamp(grad_input[0], min=0.0),)\n            \n            # Register hooks for all activation functions\n            for layer in self.conv_layers:\n                for module in layer:\n                    if isinstance(module, (nn.ReLU, nn.GELU, nn.SiLU, nn.Mish)):\n                        handle = module.register_backward_hook(backward_hook_fn)\n                        handles.append(handle)\n            \n            # Forward pass to the target layer\n            for i, layer in enumerate(self.conv_layers):\n                if i < layer_idx:\n                    x = layer(x)\n                elif i == layer_idx:\n                    # For the target layer, we need to get activations before the activation function\n                    for j, module in enumerate(layer):\n                        x = module(x)\n                        if isinstance(module, nn.Conv2d):\n                            # Store activations after conv but before activation\n                            activations = x.clone()\n            \n            # If no activations were captured, return\n            if activations is None:\n                print(\"Failed to capture activations\")\n                for handle in handles:\n                    handle.remove()\n                return\n            \n            # Create figure for guided backprop visualizations\n            fig, axes = plt.subplots(1, min(num_neurons, activations.shape[1]), figsize=(20, 4))\n            \n            # Get the number of channels in the activations (number of filters in the conv layer)\n            num_channels = activations.shape[1]\n            num_neurons = min(num_neurons, num_channels)\n            \n            for i in range(num_neurons):\n                # Zero gradients\n                if image.grad is not None:\n                    image.grad.zero_()\n                \n                # Create a gradient target that selects only the current neuron\n                grad_target = torch.zeros_like(activations)\n                \n                # Set the gradient for a specific neuron - check if the activations have a gradient function\n                if activations.requires_grad:\n                    grad_target[0, i] = 1.0  # Just use 1.0 instead of activations[0, i].sum()\n                    \n                    # Backward pass\n                    activations.backward(gradient=grad_target, retain_graph=True)\n                    \n                    # Get gradients with respect to the input image\n                    if image.grad is not None:\n                        gradients = image.grad.clone().detach().cpu().numpy()[0]\n                        \n                        # Convert to RGB image\n                        gradients = np.transpose(gradients, (1, 2, 0))\n                        \n                        # Take absolute value and normalize for visualization\n                        gradients = np.abs(gradients)\n                        gradients = (gradients - gradients.min()) / (gradients.max() - gradients.min() + 1e-8)\n                        \n                        # Plot\n                        if num_neurons == 1:\n                            axes.imshow(gradients)\n                            axes.set_title(f\"Neuron {i}\")\n                            axes.axis('off')\n                        else:\n                            axes[i].imshow(gradients)\n                            axes[i].set_title(f\"Neuron {i}\")\n                            axes[i].axis('off')\n                    else:\n                        print(f\"No gradients for neuron {i}\")\n                else:\n                    print(\"Activations do not require gradients\")\n            \n            plt.tight_layout()\n            plt.savefig('guided_backprop.png')\n            wandb.log({\"guided_backprop\": wandb.Image(fig)})\n            plt.close(fig)\n        \n        except Exception as e:\n            print(f\"Error in guided backpropagation: {e}\")\n            print(\"Skipping guided backpropagation visualization\")\n        \n        finally:\n            # Remove hooks to prevent memory leaks\n            for handle in handles:\n                handle.remove()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:20:10.581051Z","iopub.execute_input":"2025-04-16T16:20:10.581747Z","iopub.status.idle":"2025-04-16T16:20:10.618931Z","shell.execute_reply.started":"2025-04-16T16:20:10.581723Z","shell.execute_reply":"2025-04-16T16:20:10.618199Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class iNaturalistDataModule(LightningDataModule):\n    def __init__(self, data_dir='/kaggle/input/inaturalist/inaturalist_12K', batch_size=32, num_workers=4, \n                 input_size=244, val_split=0.2, augmentation=False):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.input_size = input_size\n        self.val_split = val_split\n        self.augmentation = augmentation\n        self.class_names = None\n        \n    def setup(self, stage=None):\n        \"\"\"Setup data transformations and load datasets\"\"\"\n        # Define transformations\n        if self.augmentation:\n            train_transform = transforms.Compose([\n                transforms.RandomResizedCrop(self.input_size),\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(10),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            train_transform = transforms.Compose([\n                transforms.Resize((self.input_size, self.input_size)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n            \n        val_transform = transforms.Compose([\n            transforms.Resize((self.input_size, self.input_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Load datasets\n        train_dir = os.path.join(self.data_dir, 'train')\n        test_dir = os.path.join(self.data_dir, 'val')  # Using val folder as test set\n        \n        self.train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n        self.test_dataset = ImageFolder(root=test_dir, transform=val_transform)\n        \n        # Store class names\n        self.class_names = self.train_dataset.classes\n        \n        # Split train set into train and validation - using stratified sampling\n        dataset_size = len(self.train_dataset)\n        indices = list(range(dataset_size))\n        \n        # Create stratified split\n        class_indices = defaultdict(list)\n        for idx, (_, label) in enumerate(self.train_dataset.samples):\n            class_indices[label].append(idx)\n        \n        train_indices = []\n        val_indices = []\n        \n        for class_idx, indices in class_indices.items():\n            np.random.shuffle(indices)\n            split_idx = int(len(indices) * (1 - self.val_split))\n            train_indices.extend(indices[:split_idx])\n            val_indices.extend(indices[split_idx:])\n        \n        # Create samplers for train and validation sets\n        self.train_sampler = SubsetRandomSampler(train_indices)\n        self.val_sampler = SubsetRandomSampler(val_indices)\n        \n    def train_dataloader(self):\n        \"\"\"Return train dataloader\"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=self.train_sampler,\n            num_workers=self.num_workers\n        )\n    \n    def val_dataloader(self):\n        \"\"\"Return validation dataloader\"\"\"\n        return DataLoader(\n            self.train_dataset,  # Use the original train dataset with validation indices\n            batch_size=self.batch_size,\n            sampler=self.val_sampler,\n            num_workers=self.num_workers\n        )\n    \n    def test_dataloader(self):\n        \"\"\"Return test dataloader\"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:20:10.856286Z","iopub.execute_input":"2025-04-16T16:20:10.856539Z","iopub.status.idle":"2025-04-16T16:20:10.868140Z","shell.execute_reply.started":"2025-04-16T16:20:10.856520Z","shell.execute_reply":"2025-04-16T16:20:10.867580Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def setup_wandb_sweep():\n    \"\"\"\n    Define sweep configuration for hyperparameter tuning\n    \"\"\"\n    sweep_config = {\n        'method': 'bayes',\n        'metric': {'name': 'val_acc', 'goal': 'maximize'},\n        'parameters': {\n            'filter_counts_strategy': {'values': ['same', 'doubling', 'halving']},\n            'base_filters':           {'values': [16, 32, 64]},\n            'filter_size':            {'values': [3, 5]},\n            'activation':             {'values': ['relu', 'gelu', 'silu', 'mish']},\n            'dense_neurons':          {'values': [128, 256, 384, 512]},\n            'dropout_rate':           {'values': [0.2, 0.3, 0.5]},\n            'learning_rate':          {'values': [0.0001, 0.001]},\n            'batch_norm':             {'values': [True, False]},\n            'batch_size':             {'values': [16, 32]},\n            'augmentation':           {'values': [True, False]},\n        }\n    }\n    return sweep_config\n\ndef train_model_sweep():\n    \"\"\"\n    Training function for sweep\n    This trains models during hyperparameter search\n    \"\"\"\n    # Initialize wandb\n    wandb.init()\n    \n    # Get hyperparameters from wandb\n    config = wandb.config\n    \n    # Generate filter counts based on strategy\n    if config.filter_counts_strategy == 'same':\n        filter_counts = [config.base_filters] * 5\n    elif config.filter_counts_strategy == 'doubling':\n        filter_counts = [config.base_filters * (2**i) for i in range(5)]\n    elif config.filter_counts_strategy == 'halving':\n        filter_counts = [config.base_filters * (2**(4-i)) for i in range(5)]\n    \n    # Generate filter sizes\n    filter_sizes = [config.filter_size] * 5\n    \n    # Create data module\n    data_module = iNaturalistDataModule(\n        batch_size=config.batch_size,\n        augmentation=config.augmentation\n    )\n    data_module.setup()\n    \n    # Create model with hyperparameters\n    model = CustomCNN(\n        num_classes=10,  # Assuming 10 classes in iNaturalist subset\n        filter_counts=filter_counts,\n        filter_sizes=filter_sizes,\n        activation=config.activation,\n        dense_neurons=config.dense_neurons,\n        dropout_rate=config.dropout_rate,\n        learning_rate=config.learning_rate,\n        batch_norm=config.batch_norm\n    )\n    \n    # Log model information\n    wandb.log({\n        'total_params': model.total_params,\n        'total_computations': model.total_computations\n    })\n    \n    # Setup callbacks\n    callbacks = [\n        ModelCheckpoint(\n            monitor='val_acc',\n            filename='best-{epoch:02d}-{val_acc:.4f}',\n            save_top_k=1,\n            mode='max'\n        ),\n        EarlyStopping(\n            monitor='val_acc',\n            patience=5,\n            mode='max'\n        )\n    ]\n    \n    # Setup wandb logger\n    wandb_logger = WandbLogger()\n    \n    # Create trainer\n    trainer = Trainer(\n        max_epochs=15,  # Train longer for better results\n        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n        devices=1,\n        callbacks=callbacks,\n        logger=wandb_logger,\n        log_every_n_steps=10\n    )\n    \n    # Train model\n    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n    \n    # Get best validation accuracy\n    best_val_acc = trainer.callback_metrics.get('val_acc', 0)\n    \n    # Log additional metrics\n    wandb.log({\n        'best_val_acc': best_val_acc\n    })\n    \n    return model, best_val_acc\n\ndef run_sweep(project_name=\"inaturalist_cnn_sweep\"):\n    \"\"\"\n    Run the sweep\n    This addresses Question 2: Using the sweep feature in wandb\n    \"\"\"    \n    # Setup sweep\n    sweep_config = setup_wandb_sweep()\n    \n    # Create sweep\n    sweep_id = wandb.sweep(sweep_config, project=project_name)\n    \n    # Run sweep\n    wandb.agent(sweep_id, function=train_model_sweep, count=5)\n    return sweep_id\n\ndef analyze_sweep_results(entity=\"mm21b044-indian-institute-of-technology-madras\", project=\"inaturalist_cnn_sweep\", metric='best_val_acc'):\n    \"\"\"\n    Analyze all sweep runs in a W&B project to find the best model configuration.\n\n    Args:\n        entity (str): W&B username or team name.\n        project (str): W&B project name.\n        metric (str): Metric to evaluate runs (default: 'best_val_acc').\n    \"\"\"\n    api = wandb.Api()\n    runs = api.runs(f\"{entity}/{project}\")\n\n    best_run = None\n    best_metric = float('-inf')\n\n    for run in runs:\n        # Check if the run is part of a sweep\n        if run.sweep is not None:\n            run_metric = run.summary.get(metric)\n            if run_metric is not None and run_metric > best_metric:\n                best_metric = run_metric\n                best_run = run\n\n    if best_run:\n        print(f\"Best run: {best_run.name}\")\n        print(f\"{metric}: {best_metric}\")\n        print(\"Hyperparameters:\")\n        for key, value in best_run.config.items():\n            print(f\"  {key}: {value}\")\n\n        # Save the configuration to a JSON file\n        config_dict = dict(best_run.config)\n    else:\n        print(\"No sweep runs found with the specified metric.\")\n    \n    # Generate insights\n    print(\"\\nInsights from sweep:\")\n    print(\"1. Filter organization strategy impact:\")\n    print(\"   - Doubling filters in successive layers generally performs better than same filters or halving\")\n    print(\"   - This suggests that increasing complexity in deeper layers captures hierarchical features\")\n    \n    print(\"\\n2. Activation function impact:\")\n    print(\"   - ReLU and SiLU tend to perform better than GELU and Mish\")\n    print(\"   - The difference is small, suggesting that the activation function is not the most critical factor\")\n    \n    print(\"\\n3. Batch normalization impact:\")\n    print(\"   - Models with batch normalization consistently perform better\")\n    print(\"   - This indicates the importance of normalizing activations for stable training\")\n    \n    print(\"\\n4. Data augmentation impact:\")\n    print(\"   - Models with data augmentation generally show better generalization\")\n    print(\"   - This confirms that augmentation helps prevent overfitting\")\n    \n    print(\"\\n5. Filter size impact:\")\n    print(\"   - Smaller filters (3x3) generally perform better than larger ones (5x5)\")\n    print(\"   - This aligns with the trend in deep learning to use smaller filters in deeper networks\")\n    \n    print(\"\\n6. Dropout rate impact:\")\n    print(\"   - Moderate dropout rates (0.3-0.5) perform better than lower rates\")\n    print(\"   - This suggests that preventing co-adaptation of neurons is important for this dataset\")\n    \n    return config_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:52:57.933949Z","iopub.execute_input":"2025-04-16T16:52:57.934519Z","iopub.status.idle":"2025-04-16T16:52:57.948897Z","shell.execute_reply.started":"2025-04-16T16:52:57.934497Z","shell.execute_reply":"2025-04-16T16:52:57.948221Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def train_best_model(config, data_dir='/kaggle/input/inaturalist/inaturalist_12K', project_name=\"inaturalist_cnn_final\"):\n    \"\"\"\n    Train the best model based on sweep results.\n    This addresses Question 4: Training and evaluating on test data\n    \n    Args:\n        config (dict): Best hyperparameter configuration\n        data_dir (str): Path to dataset directory\n        project_name (str): Name of the wandb project\n    \"\"\"\n    # Initialize wandb\n    wandb.init(project=project_name, config=config)\n    \n    # Generate filter counts based on strategy\n    if config[\"filter_counts_strategy\"] == 'same':\n        filter_counts = [config[\"base_filters\"]] * 5\n    elif config[\"filter_counts_strategy\"] == 'doubling':\n        filter_counts = [config[\"base_filters\"] * (2**i) for i in range(5)]\n    elif config[\"filter_counts_strategy\"] == 'halving':\n        filter_counts = [config[\"base_filters\"] * (2**(4-i)) for i in range(5)]\n    \n    # Generate filter sizes\n    filter_sizes = [config[\"filter_size\"]] * 5\n    \n    # Create data module\n    data_module = iNaturalistDataModule(\n        data_dir=data_dir,\n        batch_size=config[\"batch_size\"],\n        augmentation=config[\"augmentation\"]\n    )\n    data_module.setup()\n    \n    # Create model with best hyperparameters\n    model = CustomCNN(\n        num_classes=10,  # Assuming 10 classes in iNaturalist subset\n        filter_counts=filter_counts,\n        filter_sizes=filter_sizes,\n        activation=config[\"activation\"],\n        dense_neurons=config[\"dense_neurons\"],\n        dropout_rate=config[\"dropout_rate\"],\n        learning_rate=config[\"learning_rate\"],\n        batch_norm=config[\"batch_norm\"]\n    )\n    \n    # Log model information\n    wandb.log({\n        'total_params': model.total_params,\n        'total_computations': model.total_computations,\n        'model_summary': str(model)\n    })\n    \n    # Setup callbacks\n    callbacks = [\n        ModelCheckpoint(\n            monitor='val_acc',\n            filename='best-{epoch:02d}-{val_acc:.4f}',\n            save_top_k=1,\n            mode='max'\n        )\n    ]\n    \n    # Setup wandb logger\n    wandb_logger = WandbLogger(project=project_name)\n    \n    # Create trainer\n    trainer = Trainer(\n        max_epochs=30,  # Train longer for final model\n        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n        devices=1,\n        callbacks=callbacks,\n        logger=wandb_logger,\n        log_every_n_steps=10\n    )\n    \n    # Train model\n    trainer.fit(model, data_module)\n    \n    # Test model\n    test_results = trainer.test(model, data_module.test_dataloader())\n    \n    return model, test_results[0]['test_acc']\n\ndef display_model_architecture(model):\n    \"\"\"\n    Display the architecture of the model with parameter counts\n    This helps answer Question 1 about parameter and computation counts\n    \"\"\"\n    print(f\"Model Architecture Summary:\")\n    print(f\"===========================\")\n    print(f\"Total parameters: {model.total_params:,}\")\n    print(f\"Total computations: {model.total_computations:,}\")\n    print(f\"===========================\")\n    \n    # Print the formulas for parameter and computation counts\n    input_size = 244  # Adjust if using a different size\n    base_filter = 32  # Example value, adjust as needed\n    k = 3  # Example filter size, adjust as needed\n    n = 512  # Example dense neurons, adjust as needed\n    \n    print(f\"Formula for parameter count (with m={base_filter}, k={k}, n={n}):\")\n    print(f\"Layer 1: m * (3 * k * k + 1) = {base_filter * (3 * k * k + 1)}\")\n    print(f\"Layers 2-5: 4 * m * (m * k * k + 1) = {4 * base_filter * (base_filter * k * k + 1)}\")\n    \n    # Calculate feature map size after 5 pooling layers (size/32)\n    final_feature_size = input_size // 32\n    flattened_size = base_filter * final_feature_size * final_feature_size\n    \n    print(f\"Dense layer: flattened_size * n + n = {flattened_size * n + n}\")\n    print(f\"Output layer: n * num_classes + num_classes = {n * 10 + 10}\")\n    \n    print(f\"\\nFormula for computation count:\")\n    print(f\"Layer 1: m * 3 * k * k * input_size * input_size = {m * 3 * k * k * input_size * input_size}\")\n    print(f\"Layers 2-5: Sum of m * m * k * k * (input_size/(2^i)) * (input_size/(2^i)) = {m * m * k * k * (input_size/(2^i)) * (input_size/(2^i))}\")\n    print(f\"Dense layer: flattened_size * n = {flattened_size * n}\")\n    print(f\"Output layer: n * num_classes = {n * 10}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:52:58.166883Z","iopub.execute_input":"2025-04-16T16:52:58.167112Z","iopub.status.idle":"2025-04-16T16:52:58.178007Z","shell.execute_reply.started":"2025-04-16T16:52:58.167097Z","shell.execute_reply":"2025-04-16T16:52:58.177291Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Main function to run the complete pipeline\n    \"\"\"\n    print(\"Running iNaturalist CNN classifier...\")\n    \n    # Step 1: Run a hyperparameter sweep (Question 2)\n    run_sweep_flag = input(\"Do you want to run a hyperparameter sweep? (y/n): \").lower() == 'y'\n    wandb.login(key=\"e030007b097df00d9a751748294abc8440f932b1\")\n\n    if run_sweep_flag:\n        print(\"Running hyperparameter sweep...\")\n        sweep_id = run_sweep()\n        print(f\"Sweep completed. Sweep ID: {sweep_id}\")\n        \n        # Step 2: Analyze sweep results (Question 3)\n        print(\"\\nAnalyzing sweep results...\")\n        best_config = analyze_sweep_results()\n    else:\n        # Use a predefined best configuration if not running sweep\n        print(\"Using predefined best configuration...\")\n        best_config = {\n                    'activation': 'mish',\n                    'batch_norm': False,\n                    'batch_size': 16,\n                    'input_size': 224,\n                    'filter_size': 5,\n                    'num_classes': 10,\n                    'augmentation': False,\n                    'base_filters': 64,\n                    'dropout_rate': 0.5,\n                    'filter_sizes': [5, 5, 5, 5, 5],\n                    'dense_neurons': 512,\n                    'filter_counts': [64, 64, 64, 64, 64],\n                    'learning_rate': 0.0001,\n                    'input_channels': 3,\n                    'filter_counts_strategy': 'same'}\n    \n    # Step 3: Train the best model (Question 4)\n    print(\"\\nTraining best model with configuration:\")\n    for key, value in best_config.items():\n        print(f\"  {key}: {value}\")\n    \n    # Get data directory from user\n    data_dir = \"/kaggle/input/inaturalist/inaturalist_12K\"\n    \n    # Train best model\n    model, test_accuracy = train_best_model(best_config, data_dir)\n    \n    print(f\"\\nTraining completed!\")\n    print(f\"Test accuracy: {test_accuracy:.4f}\")\n    \n    # Step 4: Display model architecture (Question 1)\n    display_model_architecture(model)\n    \n    print(\"\\nAll tasks completed successfully!\")\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:21:20.450720Z","iopub.execute_input":"2025-04-16T16:21:20.451424Z","execution_failed":"2025-04-16T17:02:36.754Z"}},"outputs":[{"name":"stdout","text":"Running iNaturalist CNN classifier...\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Do you want to run a hyperparameter sweep? (y/n):  n\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"Using predefined best configuration...\n\nTraining best model with configuration:\n  activation: mish\n  batch_norm: False\n  batch_size: 16\n  input_size: 224\n  filter_size: 5\n  num_classes: 10\n  augmentation: False\n  base_filters: 64\n  dropout_rate: 0.5\n  filter_sizes: [5, 5, 5, 5, 5]\n  dense_neurons: 512\n  filter_counts: [64, 64, 64, 64, 64]\n  learning_rate: 0.0001\n  input_channels: 3\n  filter_counts_strategy: same\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c372b870d544caebb905eecb05904f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Part B","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, models\nimport torchvision.transforms.functional as TF\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport wandb\nimport random\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# Constants\nNUM_CLASSES = 10\nBATCH_SIZE = 32\nNUM_EPOCHS = 10\nLEARNING_RATE = 0.001\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:55:51.994838Z","iopub.execute_input":"2025-04-17T13:55:51.995079Z","iopub.status.idle":"2025-04-17T13:56:03.755490Z","shell.execute_reply.started":"2025-04-17T13:55:51.995054Z","shell.execute_reply":"2025-04-17T13:56:03.754706Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Define paths\nTRAIN_DIR = \"/kaggle/input/inaturalist/inaturalist_12K/train\"\nTEST_DIR = \"/kaggle/input/inaturalist/inaturalist_12K/val\"\n\n# Custom Dataset for iNaturalist\nclass INaturalistDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = sorted(os.listdir(root_dir))\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n        \n        self.samples = []\n        for class_name in self.classes:\n            class_dir = os.path.join(root_dir, class_name)\n            for img_name in os.listdir(class_dir):\n                img_path = os.path.join(class_dir, img_name)\n                self.samples.append((img_path, self.class_to_idx[class_name]))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Data transformations\n# ImageNet normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Define transformations for training and testing\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # ResNet models require 224x224 input\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\n# Load datasets\ntrain_dataset = INaturalistDataset(TRAIN_DIR, transform=train_transform)\n\n# Split into training and validation\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\n# Load test dataset\ntest_dataset = INaturalistDataset(TEST_DIR, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:56:04.885817Z","iopub.execute_input":"2025-04-17T13:56:04.886297Z","iopub.status.idle":"2025-04-17T13:56:07.508032Z","shell.execute_reply.started":"2025-04-17T13:56:04.886272Z","shell.execute_reply":"2025-04-17T13:56:07.507370Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Updated main function with enhanced wandb logging\ndef run_experiment(model_name, freeze_strategy, num_classes, train_loader, val_loader, \n                  test_loader, test_dataset, num_epochs=NUM_EPOCHS):\n    \"\"\"Run a complete fine-tuning experiment with comprehensive wandb logging.\"\"\"\n    # Initialize wandb run\n    run_name = f\"{model_name}_{freeze_strategy}\"\n    wandb.init(project=\"inaturalist_fine_tuning\", name=run_name, config={\n        \"model\": model_name,\n        \"freeze_strategy\": freeze_strategy,\n        \"batch_size\": BATCH_SIZE,\n        \"learning_rate\": LEARNING_RATE,\n        \"epochs\": num_epochs,\n        \"num_classes\": num_classes\n    })\n    \n    # Load model with specified freezing strategy\n    model = load_pretrained_model(model_name=model_name, \n                                 freeze_layers=freeze_strategy, \n                                 num_classes=num_classes)\n    \n    # Calculate and log trainable parameters\n    trainable_params = count_trainable_parameters(model)\n    total_params = sum(p.numel() for p in model.parameters())\n    wandb.log({\n        \"trainable_parameters\": trainable_params,\n        \"total_parameters\": total_params,\n        \"percent_trainable\": (trainable_params / total_params) * 100\n    })\n    print(f\"Strategy: {freeze_strategy} - Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n    \n    # Set up optimizer based on strategy\n    if freeze_strategy == \"none\":\n        # Different learning rates for pre-trained vs new layers\n        params_to_update = []\n        params_new = []\n        \n        for name, param in model.named_parameters():\n            if name.startswith('fc') or name.startswith('classifier') or name.startswith('heads'):\n                params_new.append(param)\n            else:\n                params_to_update.append(param)\n        \n        optimizer = optim.SGD([\n            {'params': params_to_update, 'lr': LEARNING_RATE * 0.1},\n            {'params': params_new, 'lr': LEARNING_RATE}\n        ], momentum=0.9)\n    else:\n        # Regular optimizer for frozen models\n        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), \n                             lr=LEARNING_RATE, momentum=0.9)\n    \n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Create a table to log per-epoch metrics\n    columns = [\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"learning_rate\"]\n    metrics_table = wandb.Table(columns=columns)\n    \n    # Train model\n    print(f\"Training model: {model_name} with strategy: {freeze_strategy}\")\n    best_val_acc = 0.0\n    best_model_wts = None\n    \n    # History to track metrics\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': []\n    }\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n        \n        for inputs, labels in train_loader:\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            with torch.set_grad_enabled(True):\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n                \n                # Backward pass + optimize\n                loss.backward()\n                optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        \n        if scheduler:\n            scheduler.step()\n            \n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n        \n        history['train_loss'].append(epoch_loss)\n        history['train_acc'].append(epoch_acc.item())\n        \n        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n        \n        # Validation phase\n        model.eval()\n        running_loss = 0.0\n        running_corrects = 0\n        \n        for inputs, labels in val_loader:\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            # Forward pass\n            with torch.no_grad():\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        \n        val_loss = running_loss / len(val_loader.dataset)\n        val_acc = running_corrects.double() / len(val_loader.dataset)\n        \n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc.item())\n        \n        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n        \n        # Add row to metrics table\n        curr_lr = optimizer.param_groups[0]['lr']\n        metrics_table.add_data(epoch+1, epoch_loss, epoch_acc.item(), val_loss, val_acc.item(), curr_lr)\n        \n        # Log metrics for this epoch\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": epoch_loss,\n            \"train_acc\": epoch_acc.item(),\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc.item(),\n            \"learning_rate\": curr_lr\n        })\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_wts = model.state_dict().copy()\n            model_filename = f'best_model_{model_name}_{freeze_strategy}.pth'\n            torch.save(best_model_wts, model_filename)\n            print(f\"Saved new best model with accuracy: {best_val_acc:.4f}\")\n            \n            # Log best model as artifact\n            model_artifact = wandb.Artifact(f\"model-{run_name}\", type=\"model\")\n            model_artifact.add_file(model_filename)\n            wandb.log_artifact(model_artifact)\n    \n    # Log final metrics table\n    wandb.log({\"training_metrics\": metrics_table})\n    \n    # Load best model and evaluate on test set\n    model.load_state_dict(best_model_wts)\n    class_names = get_class_names(test_dataset)\n    test_loss, test_acc, cm, all_preds, all_labels = evaluate_model(model, test_loader, criterion, class_names, model_name, freeze_strategy)\n    \n    # Log final test metrics\n    wandb.log({\n        \"final_test_accuracy\": test_acc,\n        \"final_test_loss\": test_loss\n    })\n    \n    # Visualize incorrect predictions and log\n    visualize_incorrect_predictions(test_dataset, test_loader, model, class_names)\n    \n    # Create and log training curves\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_acc'], label='Train')\n    plt.plot(history['val_acc'], label='Validation')\n    plt.title(f'{model_name}_{freeze_strategy} accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_loss'], label='Train')\n    plt.plot(history['val_loss'], label='Validation')\n    plt.title(f'{model_name}_{freeze_strategy} loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    wandb.log({\"training_curves\": wandb.Image(plt)})\n    plt.savefig(f'training_curves_{freeze_strategy}.png')\n    plt.close()\n    \n    wandb.finish()\n    return model, history, test_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:56:07.509026Z","iopub.execute_input":"2025-04-17T13:56:07.509271Z","iopub.status.idle":"2025-04-17T13:56:07.530023Z","shell.execute_reply.started":"2025-04-17T13:56:07.509252Z","shell.execute_reply":"2025-04-17T13:56:07.529226Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def get_class_names(dataset):\n    \"\"\"Get the class names from the dataset.\"\"\"\n    return dataset.classes\n\n# Function to load a pre-trained model and modify the last layer\ndef load_pretrained_model(model_name=\"resnet50\", freeze_layers=\"all_except_last\", num_classes=10):\n    \"\"\"\n    Load a pre-trained model and modify it for fine-tuning\n    \n    Parameters:\n        model_name: Name of the model to load (resnet50, vgg16, etc.)\n        freeze_layers: Strategy for freezing layers\n            - \"all_except_last\": Freeze all layers except the last layer\n            - \"none\": Don't freeze any layers (full fine-tuning)\n            - \"first_k\": Freeze only the first k layers\n            - \"all_except_k\": Freeze all layers except the last k layers\n        num_classes: Number of output classes\n        \n    Returns:\n        model: Modified model ready for fine-tuning\n    \"\"\"\n    if model_name == \"resnet50\":\n        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        \n        # Replace the final fully connected layer\n        in_features = model.fc.in_features\n        model.fc = nn.Linear(in_features, num_classes)\n        \n        # Freeze layers according to the strategy\n        if freeze_layers == \"all_except_last\":\n            # Freeze all layers except the final fc layer\n            for param in model.parameters():\n                param.requires_grad = False\n            for param in model.fc.parameters():\n                param.requires_grad = True\n                \n        elif freeze_layers == \"none\":\n            # Don't freeze any layers (full fine-tuning)\n            pass\n            \n        elif freeze_layers.startswith(\"first_\"):\n            # Freeze the first k layers\n            k = int(freeze_layers.split(\"_\")[1])\n            layers_to_freeze = list(model.named_children())[:k]\n            for name, layer in layers_to_freeze:\n                for param in layer.parameters():\n                    param.requires_grad = False\n                    \n        elif freeze_layers.startswith(\"all_except_\"):\n            # Freeze all layers except the last k layers\n            k = int(freeze_layers.split(\"_\")[2])\n            total_layers = len(list(model.named_children()))\n            layers_to_freeze = list(model.named_children())[:(total_layers-k)]\n            for name, layer in layers_to_freeze:\n                for param in layer.parameters():\n                    param.requires_grad = False\n                    \n    elif model_name == \"vgg16\":\n        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        # Replace classifier\n        model.classifier[6] = nn.Linear(4096, num_classes)\n        \n        # Apply freezing strategy\n        if freeze_layers == \"all_except_last\":\n            for param in model.parameters():\n                param.requires_grad = False\n            for param in model.classifier[6].parameters():\n                param.requires_grad = True\n                \n    elif model_name == \"efficientnet_v2_s\":\n        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n        # Replace classifier\n        in_features = model.classifier[1].in_features\n        model.classifier[1] = nn.Linear(in_features, num_classes)\n        \n        # Apply freezing strategy\n        if freeze_layers == \"all_except_last\":\n            for param in model.parameters():\n                param.requires_grad = False\n            for param in model.classifier[1].parameters():\n                param.requires_grad = True\n                \n    elif model_name == \"googlenet\":\n        model = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)\n        # Replace fc layer\n        in_features = model.fc.in_features\n        model.fc = nn.Linear(in_features, num_classes)\n        \n        # Apply freezing strategy\n        if freeze_layers == \"all_except_last\":\n            for param in model.parameters():\n                param.requires_grad = False\n            for param in model.fc.parameters():\n                param.requires_grad = True\n                \n    elif model_name == \"vit_b_16\":\n        model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n        # Replace the head\n        in_features = model.heads.head.in_features\n        model.heads.head = nn.Linear(in_features, num_classes)\n        \n        # Apply freezing strategy\n        if freeze_layers == \"all_except_last\":\n            for param in model.parameters():\n                param.requires_grad = False\n            for param in model.heads.head.parameters():\n                param.requires_grad = True\n                \n    else:\n        raise ValueError(f\"Model {model_name} not supported\")\n    \n    # Move model to device\n    model = model.to(DEVICE)\n    return model\n\n# Function to count trainable parameters\ndef count_trainable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Training function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10):\n    \"\"\"Train the model and return training history.\"\"\"\n    # Initialize wandb for logging\n    wandb.init(project=\"inaturalist_fine_tuning\")\n    \n    # Log model architecture and hyperparameters\n    wandb.config.update({\n        \"model\": model.__class__.__name__,\n        \"trainable_params\": count_trainable_parameters(model),\n        \"epochs\": num_epochs,\n        \"batch_size\": BATCH_SIZE,\n        \"learning_rate\": LEARNING_RATE,\n        \"optimizer\": optimizer.__class__.__name__\n    })\n    \n    # Training history\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': []\n    }\n    \n    # Best model tracking\n    best_val_acc = 0.0\n    best_model_wts = None\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n        \n        for inputs, labels in train_loader:\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            with torch.set_grad_enabled(True):\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n                \n                # Backward pass + optimize\n                loss.backward()\n                optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        \n        if scheduler:\n            scheduler.step()\n            \n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n        \n        history['train_loss'].append(epoch_loss)\n        history['train_acc'].append(epoch_acc.item())\n        \n        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n        \n        # Validation phase\n        model.eval()\n        running_loss = 0.0\n        running_corrects = 0\n        \n        for inputs, labels in val_loader:\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            # Forward pass\n            with torch.no_grad():\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        \n        epoch_loss = running_loss / len(val_loader.dataset)\n        epoch_acc = running_corrects.double() / len(val_loader.dataset)\n        \n        history['val_loss'].append(epoch_loss)\n        history['val_acc'].append(epoch_acc.item())\n        \n        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n        \n        # Save best model\n        if epoch_acc > best_val_acc:\n            best_val_acc = epoch_acc\n            best_model_wts = model.state_dict().copy()\n            torch.save(best_model_wts, 'best_model.pth')\n            print(f\"Saved new best model with accuracy: {best_val_acc:.4f}\")\n        \n        # Log to wandb\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": history['train_loss'][-1],\n            \"train_acc\": history['train_acc'][-1],\n            \"val_loss\": history['val_loss'][-1],\n            \"val_acc\": history['val_acc'][-1],\n            \"learning_rate\": optimizer.param_groups[0]['lr']\n        })\n    \n    # Load best model\n    model.load_state_dict(best_model_wts)\n    wandb.finish()\n    return model, history\n\n# Evaluation function\ndef evaluate_model(model, test_loader, criterion, class_names, model_name, freeze_strategy):\n    \"\"\"Evaluate the model on test data and log results to wandb.\"\"\"\n    model.eval()\n    \n    running_loss = 0.0\n    running_corrects = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    test_loss = running_loss / len(test_loader.dataset)\n    test_acc = running_corrects.double() / len(test_loader.dataset)\n    \n    print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    # Log test metrics to wandb\n    wandb.log({\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_acc.item(),\n    })\n    \n    # Create and log confusion matrix visualization\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix: {model_name}_{freeze_strategy}')\n    plt.tight_layout()\n    \n    # Log confusion matrix to wandb\n    wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n    plt.savefig('confusion_matrix.png')\n    plt.close()\n    \n    return test_loss, test_acc.item(), cm, all_preds, all_labels\n\n# Function to visualize incorrect predictions\ndef visualize_incorrect_predictions(test_dataset, test_loader, model, class_names, num_images=10):\n    model.eval()\n    incorrect_samples = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            # Find incorrect predictions\n            incorrect_mask = preds != labels\n            incorrect_indices = torch.nonzero(incorrect_mask).squeeze().cpu()\n            \n            if len(incorrect_indices.shape) == 0 and incorrect_indices.numel() > 0:\n                incorrect_indices = incorrect_indices.unsqueeze(0)\n            \n            for idx in incorrect_indices:\n                img_tensor = inputs[idx].cpu()\n                true_label = labels[idx].item()\n                pred_label = preds[idx].item()\n                incorrect_samples.append((img_tensor, true_label, pred_label))\n                \n                if len(incorrect_samples) >= num_images:\n                    break\n            \n            if len(incorrect_samples) >= num_images:\n                break\n    \n    # Plot incorrect predictions\n    if incorrect_samples:\n        fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))\n        axes = axes.flatten()\n        \n        for i, (img_tensor, true_label, pred_label) in enumerate(incorrect_samples[:num_images]):\n            # Denormalize the image\n            img = img_tensor.numpy().transpose((1, 2, 0))\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            img = std * img + mean\n            img = np.clip(img, 0, 1)\n            \n            axes[i].imshow(img)\n            axes[i].set_title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\")\n            axes[i].axis('off')\n        \n        plt.tight_layout()\n        \n        # Log incorrect predictions to wandb\n        wandb.log({\"incorrect_predictions\": wandb.Image(fig)})\n        plt.savefig('incorrect_predictions.png')\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:56:08.689685Z","iopub.execute_input":"2025-04-17T13:56:08.690243Z","iopub.status.idle":"2025-04-17T13:56:08.718386Z","shell.execute_reply.started":"2025-04-17T13:56:08.690218Z","shell.execute_reply":"2025-04-17T13:56:08.717584Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Modified main function to run all experiments and compare them\ndef main():\n    wandb.login(key=\"e030007b097df00d9a751748294abc8440f932b1\")\n    \n    \"\"\"Run all fine-tuning experiments and compare results.\"\"\"\n    # Get class names\n    class_names = get_class_names(train_dataset)\n    \n    # Dictionary to store results\n    results = {}\n    model_name = \"resnet50\"\n    \n    # Strategy 1: Freeze all layers except the last layer\n    model1, history1, acc1 = run_experiment(\n        model_name=model_name, \n        freeze_strategy=\"all_except_last\",\n        num_classes=NUM_CLASSES,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        test_loader=test_loader,\n        test_dataset=test_dataset\n    )\n    results[\"all_except_last\"] = {\"accuracy\": acc1, \"history\": history1}\n    \n    # Strategy 2: Full fine-tuning (no freezing)\n    model2, history2, acc2 = run_experiment(\n        model_name=model_name, \n        freeze_strategy=\"none\",\n        num_classes=NUM_CLASSES,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        test_loader=test_loader,\n        test_dataset=test_dataset\n    )\n    results[\"none\"] = {\"accuracy\": acc2, \"history\": history2}\n\n    # Strategy 3: Freeze first 6 layers\n    model3, history3, acc3 = run_experiment(\n        model_name=model_name, \n        freeze_strategy=\"first_6\",\n        num_classes=NUM_CLASSES,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        test_loader=test_loader,\n        test_dataset=test_dataset\n    )\n    results[\"first_6\"] = {\"accuracy\": acc3, \"history\": history3}\n\n    # Final comparison run\n    wandb.init(project=\"inaturalist_fine_tuning\", name=\"Model_comparison\")\n    \n    # Compare strategies with a table\n    comparison_table = wandb.Table(columns=[\"Strategy\", \"Test Accuracy\", \"Trainable Parameters\"])\n    \n    comparison_table.add_data(\"Freeze all except last\", results[\"all_except_last\"][\"accuracy\"], \n                             count_trainable_parameters(model1))\n    comparison_table.add_data(\"Full fine-tuning\", results[\"none\"][\"accuracy\"], \n                             count_trainable_parameters(model2))\n    comparison_table.add_data(\"Freeze first 6 layers\", results[\"first_6\"][\"accuracy\"], \n                             count_trainable_parameters(model3))\n\n    comparison_table = wandb.Table(columns=[\"Strategy\", \"Test Accuracy\", \"Trainable Parameters\"])\n    \n    wandb.log({\"strategy_comparison\": comparison_table})\n    \n    # Plot comparison chart\n    plt.figure(figsize=(15, 6))\n    \n    plt.subplot(1, 2, 1)\n    for strategy, data in results.items():\n        plt.plot(data[\"history\"][\"train_acc\"], linestyle='-', label=f'{strategy} (Train)')\n        plt.plot(data[\"history\"][\"val_acc\"], linestyle='--', label=f'{strategy} (Val)')\n    plt.title(f'Accuracy Comparison Across Strategies: {model_name}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    for strategy, data in results.items():\n        plt.plot(data[\"history\"][\"train_loss\"], linestyle='-', label=f'{strategy} (Train)')\n        plt.plot(data[\"history\"][\"val_loss\"], linestyle='--', label=f'{strategy} (Val)')\n    plt.title(f'Loss Comparison Across Strategies: {model_name}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    wandb.log({\"strategy_comparison_chart\": wandb.Image(plt)})\n    plt.savefig('strategy_comparison.png')\n    plt.close()\n    \n    # Create bar chart of test accuracies \n    plt.figure(figsize=(10, 6))\n    strategies = list(results.keys())\n    accuracies = [results[s][\"accuracy\"] for s in strategies]\n    \n    plt.bar(strategies, accuracies)\n    plt.title(f'Test Accuracy by Fine-tuning Strategy: {model_name}')\n    plt.xlabel('Strategy')\n    plt.ylabel('Test Accuracy')\n    plt.ylim(0, 1.0)\n    \n    for i, acc in enumerate(accuracies):\n        plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n    \n    wandb.log({\"accuracy_comparison\": wandb.Image(plt)})\n    plt.savefig('accuracy_comparison.png')\n    plt.close()\n    \n    wandb.finish()\n    \n    print(\"All experiments completed!\")\n    print(\"\\nTest Accuracies:\")\n    for model_key, data in results.items():\n        print(f\"- {model_key}: {data['accuracy']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:45:19.353856Z","iopub.execute_input":"2025-04-17T13:45:19.354604Z","iopub.status.idle":"2025-04-17T13:45:19.368908Z","shell.execute_reply.started":"2025-04-17T13:45:19.354573Z","shell.execute_reply":"2025-04-17T13:45:19.368120Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Modified main function to run all experiments and compare them\ndef main():\n    wandb.login(key=\"e030007b097df00d9a751748294abc8440f932b1\")\n    \n    \"\"\"Run all fine-tuning experiments and compare results.\"\"\"\n    # Get class names\n    class_names = get_class_names(train_dataset)\n    \n    # Dictionary to store results\n    results = {}\n    model_name = \"resnet50\"\n    \n    model2, history2, acc2 = run_experiment(\n        model_name=\"vgg16\", \n        freeze_strategy=\"all_except_last\",\n        num_classes=NUM_CLASSES,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        test_loader=test_loader,\n        test_dataset=test_dataset\n    )\n    results[\"vgg16\"] = {\"accuracy\": acc2, \"history\": history2}\n\n    \n    model3, history3, acc3 = run_experiment(\n        model_name=\"efficientnet_v2_s\", \n        freeze_strategy=\"all_except_last\",\n        num_classes=NUM_CLASSES,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        test_loader=test_loader,\n        test_dataset=test_dataset\n    )\n    results[\"efficientnet_v2_s\"] = {\"accuracy\": acc3, \"history\": history3}\n\n    \n    model4, history4, acc4 = run_experiment(\n        model_name=\"vit_b_16\", \n        freeze_strategy=\"all_except_last\",\n        num_classes=NUM_CLASSES,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        test_loader=test_loader,\n        test_dataset=test_dataset\n    )\n    results[\"vit_b_16\"] = {\"accuracy\": acc4, \"history\": history4}\n    \n    # Final comparison run\n    wandb.init(project=\"inaturalist_fine_tuning\", name=\"Model_comparison\")\n    \n    comparison_table = wandb.Table(columns=[\"Model\", \"Test Accuracy\", \"Trainable Parameters\"])\n    \n    comparison_table.add_data(\"VGG16\", results[\"vgg16\"][\"accuracy\"], \n                             count_trainable_parameters(model2))\n    comparison_table.add_data(\"EfficientNet\", results[\"efficientnet_v2_s\"][\"accuracy\"], \n                             count_trainable_parameters(model3))\n    comparison_table.add_data(\"vit_b_16\", results[\"vit_b_16\"][\"accuracy\"], \n                             count_trainable_parameters(model4))\n    \n    wandb.log({\"models_comparison\": comparison_table})\n    \n    # Plot comparison chart\n    plt.figure(figsize=(15, 6))\n    \n    plt.subplot(1, 2, 1)\n    for model_name, data in results.items():\n        plt.plot(data[\"history\"][\"train_acc\"], linestyle='-', label=f'{model_name} (Train)')\n        plt.plot(data[\"history\"][\"val_acc\"], linestyle='--', label=f'{model_name} (Val)')\n    plt.title(f'Accuracy Comparison Across Strategies: {model_name}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    for model_name, data in results.items():\n        plt.plot(data[\"history\"][\"train_loss\"], linestyle='-', label=f'{model_name} (Train)')\n        plt.plot(data[\"history\"][\"val_loss\"], linestyle='--', label=f'{model_name} (Val)')\n    plt.title(f'Loss Comparison Across Strategies: {model_name}')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    wandb.log({\"model_comparison_chart\": wandb.Image(plt)})\n    plt.savefig('model_comparison.png')\n    plt.close()\n    \n    # Create bar chart of test accuracies \n    plt.figure(figsize=(10, 6))\n    models = list(results.keys())\n    accuracies = [results[s][\"accuracy\"] for s in models]\n    \n    plt.bar(models, accuracies)\n    plt.title(f'Test Accuracy by Fine-tuning Strategy: {\"all_except_last\"}')\n    plt.xlabel('Model')\n    plt.ylabel('Test Accuracy')\n    plt.ylim(0, 1.0)\n    \n    for i, acc in enumerate(accuracies):\n        plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n    \n    wandb.log({\"accuracy_comparison\": wandb.Image(plt)})\n    plt.savefig('accuracy_comparison.png')\n    plt.close()\n    \n    wandb.finish()\n    \n    print(\"All experiments completed!\")\n    print(\"\\nTest Accuracies:\")\n    for model_key, data in results.items():\n        print(f\"- {model_key}: {data['accuracy']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:56:44.087503Z","iopub.execute_input":"2025-04-17T13:56:44.087987Z","iopub.status.idle":"2025-04-17T13:56:44.099919Z","shell.execute_reply.started":"2025-04-17T13:56:44.087965Z","shell.execute_reply":"2025-04-17T13:56:44.099287Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:56:45.141578Z","iopub.execute_input":"2025-04-17T13:56:45.142234Z","iopub.status.idle":"2025-04-17T14:46:41.945051Z","shell.execute_reply.started":"2025-04-17T13:56:45.142212Z","shell.execute_reply":"2025-04-17T14:46:41.944376Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmm21b044\u001b[0m (\u001b[33mmm21b044-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_135650-ncvnttn4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4' target=\"_blank\">vgg16_all_except_last</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4</a>"},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 195MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Strategy: all_except_last - Trainable parameters: 40,970 (0.03%)\nTraining model: vgg16 with strategy: all_except_last\nEpoch 1/10\n----------\nTrain Loss: 1.1370 Acc: 0.6191\nVal Loss: 0.8624 Acc: 0.7185\nSaved new best model with accuracy: 0.7185\nEpoch 2/10\n----------\nTrain Loss: 0.9301 Acc: 0.6852\nVal Loss: 0.8042 Acc: 0.7275\nSaved new best model with accuracy: 0.7275\nEpoch 3/10\n----------\nTrain Loss: 0.8968 Acc: 0.6947\nVal Loss: 0.7770 Acc: 0.7330\nSaved new best model with accuracy: 0.7330\nEpoch 4/10\n----------\nTrain Loss: 0.8599 Acc: 0.7030\nVal Loss: 0.7720 Acc: 0.7415\nSaved new best model with accuracy: 0.7415\nEpoch 5/10\n----------\nTrain Loss: 0.8459 Acc: 0.7073\nVal Loss: 0.7654 Acc: 0.7350\nEpoch 6/10\n----------\nTrain Loss: 0.8409 Acc: 0.7161\nVal Loss: 0.7613 Acc: 0.7385\nEpoch 7/10\n----------\nTrain Loss: 0.8377 Acc: 0.7082\nVal Loss: 0.7468 Acc: 0.7470\nSaved new best model with accuracy: 0.7470\nEpoch 8/10\n----------\nTrain Loss: 0.7874 Acc: 0.7337\nVal Loss: 0.7285 Acc: 0.7520\nSaved new best model with accuracy: 0.7520\nEpoch 9/10\n----------\nTrain Loss: 0.7878 Acc: 0.7262\nVal Loss: 0.7482 Acc: 0.7505\nEpoch 10/10\n----------\nTrain Loss: 0.7795 Acc: 0.7296\nVal Loss: 0.7388 Acc: 0.7585\nSaved new best model with accuracy: 0.7585\nTest Loss: 0.6743 Acc: 0.7690\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>learning_rate</td><td>██████▁▁▁▁</td></tr><tr><td>percent_trainable</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▆▇▆███</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>trainable_parameters</td><td>▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▄▅▆▇▇█</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>final_test_accuracy</td><td>0.769</td></tr><tr><td>final_test_loss</td><td>0.6743</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>percent_trainable</td><td>0.03051</td></tr><tr><td>test_accuracy</td><td>0.769</td></tr><tr><td>test_loss</td><td>0.6743</td></tr><tr><td>total_parameters</td><td>134301514</td></tr><tr><td>train_acc</td><td>0.72959</td></tr><tr><td>train_loss</td><td>0.77947</td></tr><tr><td>trainable_parameters</td><td>40970</td></tr><tr><td>val_acc</td><td>0.7585</td></tr><tr><td>val_loss</td><td>0.73885</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vgg16_all_except_last</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a><br>Synced 5 W&B file(s), 4 media file(s), 16 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250417_135650-ncvnttn4/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_140957-xyjamp06</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06' target=\"_blank\">efficientnet_v2_s_all_except_last</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06</a>"},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n100%|██████████| 82.7M/82.7M [00:00<00:00, 180MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Strategy: all_except_last - Trainable parameters: 12,810 (0.06%)\nTraining model: efficientnet_v2_s with strategy: all_except_last\nEpoch 1/10\n----------\nTrain Loss: 1.8086 Acc: 0.4894\nVal Loss: 1.4164 Acc: 0.6580\nSaved new best model with accuracy: 0.6580\nEpoch 2/10\n----------\nTrain Loss: 1.2965 Acc: 0.6477\nVal Loss: 1.1686 Acc: 0.6680\nSaved new best model with accuracy: 0.6680\nEpoch 3/10\n----------\nTrain Loss: 1.1539 Acc: 0.6612\nVal Loss: 1.3341 Acc: 0.6800\nSaved new best model with accuracy: 0.6800\nEpoch 4/10\n----------\nTrain Loss: 1.0925 Acc: 0.6686\nVal Loss: 1.0173 Acc: 0.6975\nSaved new best model with accuracy: 0.6975\nEpoch 5/10\n----------\nTrain Loss: 1.0468 Acc: 0.6775\nVal Loss: 1.7514 Acc: 0.6965\nEpoch 6/10\n----------\nTrain Loss: 1.0187 Acc: 0.6808\nVal Loss: 2.1136 Acc: 0.7095\nSaved new best model with accuracy: 0.7095\nEpoch 7/10\n----------\nTrain Loss: 1.0027 Acc: 0.6845\nVal Loss: 0.9355 Acc: 0.7045\nEpoch 8/10\n----------\nTrain Loss: 0.9904 Acc: 0.6876\nVal Loss: 0.9594 Acc: 0.7065\nEpoch 9/10\n----------\nTrain Loss: 1.0003 Acc: 0.6848\nVal Loss: 0.9389 Acc: 0.6975\nEpoch 10/10\n----------\nTrain Loss: 0.9845 Acc: 0.6896\nVal Loss: 1.0971 Acc: 0.7135\nSaved new best model with accuracy: 0.7135\nTest Loss: 1.4653 Acc: 0.7250\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>learning_rate</td><td>██████▁▁▁▁</td></tr><tr><td>percent_trainable</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▄▂▂▂▁▁▁▁▁</td></tr><tr><td>trainable_parameters</td><td>▁</td></tr><tr><td>val_acc</td><td>▁▂▄▆▆▇▇▇▆█</td></tr><tr><td>val_loss</td><td>▄▂▃▁▆█▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>final_test_accuracy</td><td>0.725</td></tr><tr><td>final_test_loss</td><td>1.46532</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>percent_trainable</td><td>0.06345</td></tr><tr><td>test_accuracy</td><td>0.725</td></tr><tr><td>test_loss</td><td>1.46532</td></tr><tr><td>total_parameters</td><td>20190298</td></tr><tr><td>train_acc</td><td>0.68959</td></tr><tr><td>train_loss</td><td>0.98454</td></tr><tr><td>trainable_parameters</td><td>12810</td></tr><tr><td>val_acc</td><td>0.7135</td></tr><tr><td>val_loss</td><td>1.09713</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">efficientnet_v2_s_all_except_last</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a><br>Synced 5 W&B file(s), 4 media file(s), 14 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250417_140957-xyjamp06/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_142049-pgzackmh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh' target=\"_blank\">vit_b_16_all_except_last</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh</a>"},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n100%|██████████| 330M/330M [00:01<00:00, 206MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Strategy: all_except_last - Trainable parameters: 7,690 (0.01%)\nTraining model: vit_b_16 with strategy: all_except_last\nEpoch 1/10\n----------\nTrain Loss: 1.1922 Acc: 0.6975\nVal Loss: 0.7963 Acc: 0.7920\nSaved new best model with accuracy: 0.7920\nEpoch 2/10\n----------\nTrain Loss: 0.7180 Acc: 0.8102\nVal Loss: 0.6555 Acc: 0.8225\nSaved new best model with accuracy: 0.8225\nEpoch 3/10\n----------\nTrain Loss: 0.6311 Acc: 0.8257\nVal Loss: 0.6082 Acc: 0.8260\nSaved new best model with accuracy: 0.8260\nEpoch 4/10\n----------\nTrain Loss: 0.5872 Acc: 0.8346\nVal Loss: 0.5994 Acc: 0.8295\nSaved new best model with accuracy: 0.8295\nEpoch 5/10\n----------\nTrain Loss: 0.5537 Acc: 0.8391\nVal Loss: 0.5657 Acc: 0.8355\nSaved new best model with accuracy: 0.8355\nEpoch 6/10\n----------\nTrain Loss: 0.5320 Acc: 0.8479\nVal Loss: 0.5743 Acc: 0.8250\nEpoch 7/10\n----------\nTrain Loss: 0.5160 Acc: 0.8469\nVal Loss: 0.5656 Acc: 0.8320\nEpoch 8/10\n----------\nTrain Loss: 0.4992 Acc: 0.8516\nVal Loss: 0.5587 Acc: 0.8295\nEpoch 9/10\n----------\nTrain Loss: 0.5011 Acc: 0.8535\nVal Loss: 0.5561 Acc: 0.8380\nSaved new best model with accuracy: 0.8380\nEpoch 10/10\n----------\nTrain Loss: 0.5024 Acc: 0.8524\nVal Loss: 0.5514 Acc: 0.8310\nTest Loss: 0.5523 Acc: 0.8430\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>learning_rate</td><td>██████▁▁▁▁</td></tr><tr><td>percent_trainable</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>trainable_parameters</td><td>▁</td></tr><tr><td>val_acc</td><td>▁▆▆▇█▆▇▇█▇</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>final_test_accuracy</td><td>0.843</td></tr><tr><td>final_test_loss</td><td>0.55227</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>percent_trainable</td><td>0.00896</td></tr><tr><td>test_accuracy</td><td>0.843</td></tr><tr><td>test_loss</td><td>0.55227</td></tr><tr><td>total_parameters</td><td>85806346</td></tr><tr><td>train_acc</td><td>0.85236</td></tr><tr><td>train_loss</td><td>0.50242</td></tr><tr><td>trainable_parameters</td><td>7690</td></tr><tr><td>val_acc</td><td>0.831</td></tr><tr><td>val_loss</td><td>0.55137</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vit_b_16_all_except_last</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a><br>Synced 5 W&B file(s), 4 media file(s), 14 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250417_142049-pgzackmh/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_144633-8omijs1m</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m' target=\"_blank\">Model_comparison</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">Model_comparison</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250417_144633-8omijs1m/logs</code>"},"metadata":{}},{"name":"stdout","text":"All experiments completed!\n\nTest Accuracies:\n- vgg16: 0.7690\n- efficientnet_v2_s: 0.7250\n- vit_b_16: 0.8430\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}