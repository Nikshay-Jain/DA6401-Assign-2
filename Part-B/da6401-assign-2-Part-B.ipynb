{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:55:51.995079Z",
     "iopub.status.busy": "2025-04-17T13:55:51.994838Z",
     "iopub.status.idle": "2025-04-17T13:56:03.755490Z",
     "shell.execute_reply": "2025-04-17T13:56:03.754706Z",
     "shell.execute_reply.started": "2025-04-17T13:55:51.995054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:56:04.886297Z",
     "iopub.status.busy": "2025-04-17T13:56:04.885817Z",
     "iopub.status.idle": "2025-04-17T13:56:07.508032Z",
     "shell.execute_reply": "2025-04-17T13:56:07.507370Z",
     "shell.execute_reply.started": "2025-04-17T13:56:04.886272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "TRAIN_DIR = \"/kaggle/input/inaturalist/inaturalist_12K/train\"\n",
    "TEST_DIR = \"/kaggle/input/inaturalist/inaturalist_12K/val\"\n",
    "\n",
    "# Custom Dataset for iNaturalist\n",
    "class INaturalistDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Data transformations\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define transformations for training and testing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet models require 224x224 input\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = INaturalistDataset(TRAIN_DIR, transform=train_transform)\n",
    "\n",
    "# Split into training and validation\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = INaturalistDataset(TEST_DIR, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:56:07.509271Z",
     "iopub.status.busy": "2025-04-17T13:56:07.509026Z",
     "iopub.status.idle": "2025-04-17T13:56:07.530023Z",
     "shell.execute_reply": "2025-04-17T13:56:07.529226Z",
     "shell.execute_reply.started": "2025-04-17T13:56:07.509252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Updated main function with enhanced wandb logging\n",
    "def run_experiment(model_name, freeze_strategy, num_classes, train_loader, val_loader, \n",
    "                  test_loader, test_dataset, num_epochs=NUM_EPOCHS):\n",
    "    \"\"\"Run a complete fine-tuning experiment with comprehensive wandb logging.\"\"\"\n",
    "    # Initialize wandb run\n",
    "    run_name = f\"{model_name}_{freeze_strategy}\"\n",
    "    wandb.init(project=\"inaturalist_fine_tuning\", name=run_name, config={\n",
    "        \"model\": model_name,\n",
    "        \"freeze_strategy\": freeze_strategy,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"num_classes\": num_classes\n",
    "    })\n",
    "    \n",
    "    # Load model with specified freezing strategy\n",
    "    model = load_pretrained_model(model_name=model_name, \n",
    "                                 freeze_layers=freeze_strategy, \n",
    "                                 num_classes=num_classes)\n",
    "    \n",
    "    # Calculate and log trainable parameters\n",
    "    trainable_params = count_trainable_parameters(model)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    wandb.log({\n",
    "        \"trainable_parameters\": trainable_params,\n",
    "        \"total_parameters\": total_params,\n",
    "        \"percent_trainable\": (trainable_params / total_params) * 100\n",
    "    })\n",
    "    print(f\"Strategy: {freeze_strategy} - Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "    \n",
    "    # Set up optimizer based on strategy\n",
    "    if freeze_strategy == \"none\":\n",
    "        # Different learning rates for pre-trained vs new layers\n",
    "        params_to_update = []\n",
    "        params_new = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith('fc') or name.startswith('classifier') or name.startswith('heads'):\n",
    "                params_new.append(param)\n",
    "            else:\n",
    "                params_to_update.append(param)\n",
    "        \n",
    "        optimizer = optim.SGD([\n",
    "            {'params': params_to_update, 'lr': LEARNING_RATE * 0.1},\n",
    "            {'params': params_new, 'lr': LEARNING_RATE}\n",
    "        ], momentum=0.9)\n",
    "    else:\n",
    "        # Regular optimizer for frozen models\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                             lr=LEARNING_RATE, momentum=0.9)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create a table to log per-epoch metrics\n",
    "    columns = [\"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"learning_rate\"]\n",
    "    metrics_table = wandb.Table(columns=columns)\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training model: {model_name} with strategy: {freeze_strategy}\")\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    # History to track metrics\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass + optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc.item())\n",
    "        \n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Add row to metrics table\n",
    "        curr_lr = optimizer.param_groups[0]['lr']\n",
    "        metrics_table.add_data(epoch+1, epoch_loss, epoch_acc.item(), val_loss, val_acc.item(), curr_lr)\n",
    "        \n",
    "        # Log metrics for this epoch\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": epoch_loss,\n",
    "            \"train_acc\": epoch_acc.item(),\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc.item(),\n",
    "            \"learning_rate\": curr_lr\n",
    "        })\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            model_filename = f'best_model_{model_name}_{freeze_strategy}.pth'\n",
    "            torch.save(best_model_wts, model_filename)\n",
    "            print(f\"Saved new best model with accuracy: {best_val_acc:.4f}\")\n",
    "            \n",
    "            # Log best model as artifact\n",
    "            model_artifact = wandb.Artifact(f\"model-{run_name}\", type=\"model\")\n",
    "            model_artifact.add_file(model_filename)\n",
    "            wandb.log_artifact(model_artifact)\n",
    "    \n",
    "    # Log final metrics table\n",
    "    wandb.log({\"training_metrics\": metrics_table})\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    class_names = get_class_names(test_dataset)\n",
    "    test_loss, test_acc, cm, all_preds, all_labels = evaluate_model(model, test_loader, criterion, class_names, model_name, freeze_strategy)\n",
    "    \n",
    "    # Log final test metrics\n",
    "    wandb.log({\n",
    "        \"final_test_accuracy\": test_acc,\n",
    "        \"final_test_loss\": test_loss\n",
    "    })\n",
    "    \n",
    "    # Visualize incorrect predictions and log\n",
    "    visualize_incorrect_predictions(test_dataset, test_loader, model, class_names)\n",
    "    \n",
    "    # Create and log training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title(f'{model_name}_{freeze_strategy} accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title(f'{model_name}_{freeze_strategy} loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    wandb.log({\"training_curves\": wandb.Image(plt)})\n",
    "    plt.savefig(f'training_curves_{freeze_strategy}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    wandb.finish()\n",
    "    return model, history, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:56:08.690243Z",
     "iopub.status.busy": "2025-04-17T13:56:08.689685Z",
     "iopub.status.idle": "2025-04-17T13:56:08.718386Z",
     "shell.execute_reply": "2025-04-17T13:56:08.717584Z",
     "shell.execute_reply.started": "2025-04-17T13:56:08.690218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_class_names(dataset):\n",
    "    \"\"\"Get the class names from the dataset.\"\"\"\n",
    "    return dataset.classes\n",
    "\n",
    "# Function to load a pre-trained model and modify the last layer\n",
    "def load_pretrained_model(model_name=\"resnet50\", freeze_layers=\"all_except_last\", num_classes=10):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model and modify it for fine-tuning\n",
    "    \n",
    "    Parameters:\n",
    "        model_name: Name of the model to load (resnet50, vgg16, etc.)\n",
    "        freeze_layers: Strategy for freezing layers\n",
    "            - \"all_except_last\": Freeze all layers except the last layer\n",
    "            - \"none\": Don't freeze any layers (full fine-tuning)\n",
    "            - \"first_k\": Freeze only the first k layers\n",
    "            - \"all_except_k\": Freeze all layers except the last k layers\n",
    "        num_classes: Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        model: Modified model ready for fine-tuning\n",
    "    \"\"\"\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        # Replace the final fully connected layer\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # Freeze layers according to the strategy\n",
    "        if freeze_layers == \"all_except_last\":\n",
    "            # Freeze all layers except the final fc layer\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        elif freeze_layers == \"none\":\n",
    "            # Don't freeze any layers (full fine-tuning)\n",
    "            pass\n",
    "            \n",
    "        elif freeze_layers.startswith(\"first_\"):\n",
    "            # Freeze the first k layers\n",
    "            k = int(freeze_layers.split(\"_\")[1])\n",
    "            layers_to_freeze = list(model.named_children())[:k]\n",
    "            for name, layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "        elif freeze_layers.startswith(\"all_except_\"):\n",
    "            # Freeze all layers except the last k layers\n",
    "            k = int(freeze_layers.split(\"_\")[2])\n",
    "            total_layers = len(list(model.named_children()))\n",
    "            layers_to_freeze = list(model.named_children())[:(total_layers-k)]\n",
    "            for name, layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "    elif model_name == \"vgg16\":\n",
    "        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        # Replace classifier\n",
    "        model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        # Apply freezing strategy\n",
    "        if freeze_layers == \"all_except_last\":\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.classifier[6].parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    elif model_name == \"efficientnet_v2_s\":\n",
    "        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "        # Replace classifier\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # Apply freezing strategy\n",
    "        if freeze_layers == \"all_except_last\":\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.classifier[1].parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    elif model_name == \"googlenet\":\n",
    "        model = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "        # Replace fc layer\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # Apply freezing strategy\n",
    "        if freeze_layers == \"all_except_last\":\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    elif model_name == \"vit_b_16\":\n",
    "        model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        # Replace the head\n",
    "        in_features = model.heads.head.in_features\n",
    "        model.heads.head = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # Apply freezing strategy\n",
    "        if freeze_layers == \"all_except_last\":\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.heads.head.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "# Function to count trainable parameters\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10):\n",
    "    \"\"\"Train the model and return training history.\"\"\"\n",
    "    # Initialize wandb for logging\n",
    "    wandb.init(project=\"inaturalist_fine_tuning\")\n",
    "    \n",
    "    # Log model architecture and hyperparameters\n",
    "    wandb.config.update({\n",
    "        \"model\": model.__class__.__name__,\n",
    "        \"trainable_params\": count_trainable_parameters(model),\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"optimizer\": optimizer.__class__.__name__\n",
    "    })\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Best model tracking\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass + optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "        \n",
    "        history['val_loss'].append(epoch_loss)\n",
    "        history['val_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_acc > best_val_acc:\n",
    "            best_val_acc = epoch_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            torch.save(best_model_wts, 'best_model.pth')\n",
    "            print(f\"Saved new best model with accuracy: {best_val_acc:.4f}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": history['train_loss'][-1],\n",
    "            \"train_acc\": history['train_acc'][-1],\n",
    "            \"val_loss\": history['val_loss'][-1],\n",
    "            \"val_acc\": history['val_acc'][-1],\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    wandb.finish()\n",
    "    return model, history\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, class_names, model_name, freeze_strategy):\n",
    "    \"\"\"Evaluate the model on test data and log results to wandb.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    test_acc = running_corrects.double() / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Log test metrics to wandb\n",
    "    wandb.log({\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_acc.item(),\n",
    "    })\n",
    "    \n",
    "    # Create and log confusion matrix visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix: {model_name}_{freeze_strategy}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log confusion matrix to wandb\n",
    "    wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return test_loss, test_acc.item(), cm, all_preds, all_labels\n",
    "\n",
    "# Function to visualize incorrect predictions\n",
    "def visualize_incorrect_predictions(test_dataset, test_loader, model, class_names, num_images=10):\n",
    "    model.eval()\n",
    "    incorrect_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Find incorrect predictions\n",
    "            incorrect_mask = preds != labels\n",
    "            incorrect_indices = torch.nonzero(incorrect_mask).squeeze().cpu()\n",
    "            \n",
    "            if len(incorrect_indices.shape) == 0 and incorrect_indices.numel() > 0:\n",
    "                incorrect_indices = incorrect_indices.unsqueeze(0)\n",
    "            \n",
    "            for idx in incorrect_indices:\n",
    "                img_tensor = inputs[idx].cpu()\n",
    "                true_label = labels[idx].item()\n",
    "                pred_label = preds[idx].item()\n",
    "                incorrect_samples.append((img_tensor, true_label, pred_label))\n",
    "                \n",
    "                if len(incorrect_samples) >= num_images:\n",
    "                    break\n",
    "            \n",
    "            if len(incorrect_samples) >= num_images:\n",
    "                break\n",
    "    \n",
    "    # Plot incorrect predictions\n",
    "    if incorrect_samples:\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (img_tensor, true_label, pred_label) in enumerate(incorrect_samples[:num_images]):\n",
    "            # Denormalize the image\n",
    "            img = img_tensor.numpy().transpose((1, 2, 0))\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img = std * img + mean\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\")\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Log incorrect predictions to wandb\n",
    "        wandb.log({\"incorrect_predictions\": wandb.Image(fig)})\n",
    "        plt.savefig('incorrect_predictions.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:45:19.354604Z",
     "iopub.status.busy": "2025-04-17T13:45:19.353856Z",
     "iopub.status.idle": "2025-04-17T13:45:19.368908Z",
     "shell.execute_reply": "2025-04-17T13:45:19.368120Z",
     "shell.execute_reply.started": "2025-04-17T13:45:19.354573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Modified main function to run all experiments and compare them\n",
    "def main():\n",
    "    wandb.login(key=\"e030007b097df00d9a751748294abc8440f932b1\")\n",
    "    \n",
    "    \"\"\"Run all fine-tuning experiments and compare results.\"\"\"\n",
    "    # Get class names\n",
    "    class_names = get_class_names(train_dataset)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    model_name = \"resnet50\"\n",
    "    \n",
    "    # Strategy 1: Freeze all layers except the last layer\n",
    "    model1, history1, acc1 = run_experiment(\n",
    "        model_name=model_name, \n",
    "        freeze_strategy=\"all_except_last\",\n",
    "        num_classes=NUM_CLASSES,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        test_dataset=test_dataset\n",
    "    )\n",
    "    results[\"all_except_last\"] = {\"accuracy\": acc1, \"history\": history1}\n",
    "    \n",
    "    # Strategy 2: Full fine-tuning (no freezing)\n",
    "    model2, history2, acc2 = run_experiment(\n",
    "        model_name=model_name, \n",
    "        freeze_strategy=\"none\",\n",
    "        num_classes=NUM_CLASSES,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        test_dataset=test_dataset\n",
    "    )\n",
    "    results[\"none\"] = {\"accuracy\": acc2, \"history\": history2}\n",
    "\n",
    "    # Strategy 3: Freeze first 6 layers\n",
    "    model3, history3, acc3 = run_experiment(\n",
    "        model_name=model_name, \n",
    "        freeze_strategy=\"first_6\",\n",
    "        num_classes=NUM_CLASSES,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        test_dataset=test_dataset\n",
    "    )\n",
    "    results[\"first_6\"] = {\"accuracy\": acc3, \"history\": history3}\n",
    "\n",
    "    # Final comparison run\n",
    "    wandb.init(project=\"inaturalist_fine_tuning\", name=\"Model_comparison\")\n",
    "    \n",
    "    # Compare strategies with a table\n",
    "    comparison_table = wandb.Table(columns=[\"Strategy\", \"Test Accuracy\", \"Trainable Parameters\"])\n",
    "    \n",
    "    comparison_table.add_data(\"Freeze all except last\", results[\"all_except_last\"][\"accuracy\"], \n",
    "                             count_trainable_parameters(model1))\n",
    "    comparison_table.add_data(\"Full fine-tuning\", results[\"none\"][\"accuracy\"], \n",
    "                             count_trainable_parameters(model2))\n",
    "    comparison_table.add_data(\"Freeze first 6 layers\", results[\"first_6\"][\"accuracy\"], \n",
    "                             count_trainable_parameters(model3))\n",
    "\n",
    "    comparison_table = wandb.Table(columns=[\"Strategy\", \"Test Accuracy\", \"Trainable Parameters\"])\n",
    "    \n",
    "    wandb.log({\"strategy_comparison\": comparison_table})\n",
    "    \n",
    "    # Plot comparison chart\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for strategy, data in results.items():\n",
    "        plt.plot(data[\"history\"][\"train_acc\"], linestyle='-', label=f'{strategy} (Train)')\n",
    "        plt.plot(data[\"history\"][\"val_acc\"], linestyle='--', label=f'{strategy} (Val)')\n",
    "    plt.title(f'Accuracy Comparison Across Strategies: {model_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for strategy, data in results.items():\n",
    "        plt.plot(data[\"history\"][\"train_loss\"], linestyle='-', label=f'{strategy} (Train)')\n",
    "        plt.plot(data[\"history\"][\"val_loss\"], linestyle='--', label=f'{strategy} (Val)')\n",
    "    plt.title(f'Loss Comparison Across Strategies: {model_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    wandb.log({\"strategy_comparison_chart\": wandb.Image(plt)})\n",
    "    plt.savefig('strategy_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create bar chart of test accuracies \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    strategies = list(results.keys())\n",
    "    accuracies = [results[s][\"accuracy\"] for s in strategies]\n",
    "    \n",
    "    plt.bar(strategies, accuracies)\n",
    "    plt.title(f'Test Accuracy by Fine-tuning Strategy: {model_name}')\n",
    "    plt.xlabel('Strategy')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.ylim(0, 1.0)\n",
    "    \n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "    \n",
    "    wandb.log({\"accuracy_comparison\": wandb.Image(plt)})\n",
    "    plt.savefig('accuracy_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    print(\"All experiments completed!\")\n",
    "    print(\"\\nTest Accuracies:\")\n",
    "    for model_key, data in results.items():\n",
    "        print(f\"- {model_key}: {data['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:56:44.087987Z",
     "iopub.status.busy": "2025-04-17T13:56:44.087503Z",
     "iopub.status.idle": "2025-04-17T13:56:44.099919Z",
     "shell.execute_reply": "2025-04-17T13:56:44.099287Z",
     "shell.execute_reply.started": "2025-04-17T13:56:44.087965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Modified main function to run all experiments and compare them\n",
    "def main():\n",
    "    key = \"\"      # Add your WandB API key here\n",
    "    wandb.login(key=key)\n",
    "    \n",
    "    \"\"\"Run all fine-tuning experiments and compare results.\"\"\"\n",
    "    # Get class names\n",
    "    class_names = get_class_names(train_dataset)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    model_name = \"resnet50\"\n",
    "    \n",
    "    model2, history2, acc2 = run_experiment(\n",
    "        model_name=\"vgg16\", \n",
    "        freeze_strategy=\"all_except_last\",\n",
    "        num_classes=NUM_CLASSES,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        test_dataset=test_dataset\n",
    "    )\n",
    "    results[\"vgg16\"] = {\"accuracy\": acc2, \"history\": history2}\n",
    "\n",
    "    \n",
    "    model3, history3, acc3 = run_experiment(\n",
    "        model_name=\"efficientnet_v2_s\", \n",
    "        freeze_strategy=\"all_except_last\",\n",
    "        num_classes=NUM_CLASSES,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        test_dataset=test_dataset\n",
    "    )\n",
    "    results[\"efficientnet_v2_s\"] = {\"accuracy\": acc3, \"history\": history3}\n",
    "\n",
    "    \n",
    "    model4, history4, acc4 = run_experiment(\n",
    "        model_name=\"vit_b_16\", \n",
    "        freeze_strategy=\"all_except_last\",\n",
    "        num_classes=NUM_CLASSES,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        test_dataset=test_dataset\n",
    "    )\n",
    "    results[\"vit_b_16\"] = {\"accuracy\": acc4, \"history\": history4}\n",
    "    \n",
    "    # Final comparison run\n",
    "    wandb.init(project=\"inaturalist_fine_tuning\", name=\"Model_comparison\")\n",
    "    \n",
    "    comparison_table = wandb.Table(columns=[\"Model\", \"Test Accuracy\", \"Trainable Parameters\"])\n",
    "    \n",
    "    comparison_table.add_data(\"VGG16\", results[\"vgg16\"][\"accuracy\"], \n",
    "                             count_trainable_parameters(model2))\n",
    "    comparison_table.add_data(\"EfficientNet\", results[\"efficientnet_v2_s\"][\"accuracy\"], \n",
    "                             count_trainable_parameters(model3))\n",
    "    comparison_table.add_data(\"vit_b_16\", results[\"vit_b_16\"][\"accuracy\"], \n",
    "                             count_trainable_parameters(model4))\n",
    "    \n",
    "    wandb.log({\"models_comparison\": comparison_table})\n",
    "    \n",
    "    # Plot comparison chart\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for model_name, data in results.items():\n",
    "        plt.plot(data[\"history\"][\"train_acc\"], linestyle='-', label=f'{model_name} (Train)')\n",
    "        plt.plot(data[\"history\"][\"val_acc\"], linestyle='--', label=f'{model_name} (Val)')\n",
    "    plt.title(f'Accuracy Comparison Across Strategies: {model_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for model_name, data in results.items():\n",
    "        plt.plot(data[\"history\"][\"train_loss\"], linestyle='-', label=f'{model_name} (Train)')\n",
    "        plt.plot(data[\"history\"][\"val_loss\"], linestyle='--', label=f'{model_name} (Val)')\n",
    "    plt.title(f'Loss Comparison Across Strategies: {model_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    wandb.log({\"model_comparison_chart\": wandb.Image(plt)})\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create bar chart of test accuracies \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    models = list(results.keys())\n",
    "    accuracies = [results[s][\"accuracy\"] for s in models]\n",
    "    \n",
    "    plt.bar(models, accuracies)\n",
    "    plt.title(f'Test Accuracy by Fine-tuning Strategy: {\"all_except_last\"}')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.ylim(0, 1.0)\n",
    "    \n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center')\n",
    "    \n",
    "    wandb.log({\"accuracy_comparison\": wandb.Image(plt)})\n",
    "    plt.savefig('accuracy_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    print(\"All experiments completed!\")\n",
    "    print(\"\\nTest Accuracies:\")\n",
    "    for model_key, data in results.items():\n",
    "        print(f\"- {model_key}: {data['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:56:45.142234Z",
     "iopub.status.busy": "2025-04-17T13:56:45.141578Z",
     "iopub.status.idle": "2025-04-17T14:46:41.945051Z",
     "shell.execute_reply": "2025-04-17T14:46:41.944376Z",
     "shell.execute_reply.started": "2025-04-17T13:56:45.142212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmm21b044\u001b[0m (\u001b[33mmm21b044-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_135650-ncvnttn4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4' target=\"_blank\">vgg16_all_except_last</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 195MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: all_except_last - Trainable parameters: 40,970 (0.03%)\n",
      "Training model: vgg16 with strategy: all_except_last\n",
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 1.1370 Acc: 0.6191\n",
      "Val Loss: 0.8624 Acc: 0.7185\n",
      "Saved new best model with accuracy: 0.7185\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 0.9301 Acc: 0.6852\n",
      "Val Loss: 0.8042 Acc: 0.7275\n",
      "Saved new best model with accuracy: 0.7275\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 0.8968 Acc: 0.6947\n",
      "Val Loss: 0.7770 Acc: 0.7330\n",
      "Saved new best model with accuracy: 0.7330\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 0.8599 Acc: 0.7030\n",
      "Val Loss: 0.7720 Acc: 0.7415\n",
      "Saved new best model with accuracy: 0.7415\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 0.8459 Acc: 0.7073\n",
      "Val Loss: 0.7654 Acc: 0.7350\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 0.8409 Acc: 0.7161\n",
      "Val Loss: 0.7613 Acc: 0.7385\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 0.8377 Acc: 0.7082\n",
      "Val Loss: 0.7468 Acc: 0.7470\n",
      "Saved new best model with accuracy: 0.7470\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 0.7874 Acc: 0.7337\n",
      "Val Loss: 0.7285 Acc: 0.7520\n",
      "Saved new best model with accuracy: 0.7520\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 0.7878 Acc: 0.7262\n",
      "Val Loss: 0.7482 Acc: 0.7505\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 0.7795 Acc: 0.7296\n",
      "Val Loss: 0.7388 Acc: 0.7585\n",
      "Saved new best model with accuracy: 0.7585\n",
      "Test Loss: 0.6743 Acc: 0.7690\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>learning_rate</td><td>██████▁▁▁▁</td></tr><tr><td>percent_trainable</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▆▇▆███</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>trainable_parameters</td><td>▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▄▅▆▇▇█</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>final_test_accuracy</td><td>0.769</td></tr><tr><td>final_test_loss</td><td>0.6743</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>percent_trainable</td><td>0.03051</td></tr><tr><td>test_accuracy</td><td>0.769</td></tr><tr><td>test_loss</td><td>0.6743</td></tr><tr><td>total_parameters</td><td>134301514</td></tr><tr><td>train_acc</td><td>0.72959</td></tr><tr><td>train_loss</td><td>0.77947</td></tr><tr><td>trainable_parameters</td><td>40970</td></tr><tr><td>val_acc</td><td>0.7585</td></tr><tr><td>val_loss</td><td>0.73885</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vgg16_all_except_last</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/ncvnttn4</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a><br>Synced 5 W&B file(s), 4 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250417_135650-ncvnttn4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_140957-xyjamp06</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06' target=\"_blank\">efficientnet_v2_s_all_except_last</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n",
      "100%|██████████| 82.7M/82.7M [00:00<00:00, 180MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: all_except_last - Trainable parameters: 12,810 (0.06%)\n",
      "Training model: efficientnet_v2_s with strategy: all_except_last\n",
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 1.8086 Acc: 0.4894\n",
      "Val Loss: 1.4164 Acc: 0.6580\n",
      "Saved new best model with accuracy: 0.6580\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 1.2965 Acc: 0.6477\n",
      "Val Loss: 1.1686 Acc: 0.6680\n",
      "Saved new best model with accuracy: 0.6680\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 1.1539 Acc: 0.6612\n",
      "Val Loss: 1.3341 Acc: 0.6800\n",
      "Saved new best model with accuracy: 0.6800\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 1.0925 Acc: 0.6686\n",
      "Val Loss: 1.0173 Acc: 0.6975\n",
      "Saved new best model with accuracy: 0.6975\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 1.0468 Acc: 0.6775\n",
      "Val Loss: 1.7514 Acc: 0.6965\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 1.0187 Acc: 0.6808\n",
      "Val Loss: 2.1136 Acc: 0.7095\n",
      "Saved new best model with accuracy: 0.7095\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 1.0027 Acc: 0.6845\n",
      "Val Loss: 0.9355 Acc: 0.7045\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 0.9904 Acc: 0.6876\n",
      "Val Loss: 0.9594 Acc: 0.7065\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 1.0003 Acc: 0.6848\n",
      "Val Loss: 0.9389 Acc: 0.6975\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 0.9845 Acc: 0.6896\n",
      "Val Loss: 1.0971 Acc: 0.7135\n",
      "Saved new best model with accuracy: 0.7135\n",
      "Test Loss: 1.4653 Acc: 0.7250\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>learning_rate</td><td>██████▁▁▁▁</td></tr><tr><td>percent_trainable</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▄▂▂▂▁▁▁▁▁</td></tr><tr><td>trainable_parameters</td><td>▁</td></tr><tr><td>val_acc</td><td>▁▂▄▆▆▇▇▇▆█</td></tr><tr><td>val_loss</td><td>▄▂▃▁▆█▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>final_test_accuracy</td><td>0.725</td></tr><tr><td>final_test_loss</td><td>1.46532</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>percent_trainable</td><td>0.06345</td></tr><tr><td>test_accuracy</td><td>0.725</td></tr><tr><td>test_loss</td><td>1.46532</td></tr><tr><td>total_parameters</td><td>20190298</td></tr><tr><td>train_acc</td><td>0.68959</td></tr><tr><td>train_loss</td><td>0.98454</td></tr><tr><td>trainable_parameters</td><td>12810</td></tr><tr><td>val_acc</td><td>0.7135</td></tr><tr><td>val_loss</td><td>1.09713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficientnet_v2_s_all_except_last</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/xyjamp06</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a><br>Synced 5 W&B file(s), 4 media file(s), 14 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250417_140957-xyjamp06/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_142049-pgzackmh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh' target=\"_blank\">vit_b_16_all_except_last</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
      "100%|██████████| 330M/330M [00:01<00:00, 206MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: all_except_last - Trainable parameters: 7,690 (0.01%)\n",
      "Training model: vit_b_16 with strategy: all_except_last\n",
      "Epoch 1/10\n",
      "----------\n",
      "Train Loss: 1.1922 Acc: 0.6975\n",
      "Val Loss: 0.7963 Acc: 0.7920\n",
      "Saved new best model with accuracy: 0.7920\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train Loss: 0.7180 Acc: 0.8102\n",
      "Val Loss: 0.6555 Acc: 0.8225\n",
      "Saved new best model with accuracy: 0.8225\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train Loss: 0.6311 Acc: 0.8257\n",
      "Val Loss: 0.6082 Acc: 0.8260\n",
      "Saved new best model with accuracy: 0.8260\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train Loss: 0.5872 Acc: 0.8346\n",
      "Val Loss: 0.5994 Acc: 0.8295\n",
      "Saved new best model with accuracy: 0.8295\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train Loss: 0.5537 Acc: 0.8391\n",
      "Val Loss: 0.5657 Acc: 0.8355\n",
      "Saved new best model with accuracy: 0.8355\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train Loss: 0.5320 Acc: 0.8479\n",
      "Val Loss: 0.5743 Acc: 0.8250\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train Loss: 0.5160 Acc: 0.8469\n",
      "Val Loss: 0.5656 Acc: 0.8320\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train Loss: 0.4992 Acc: 0.8516\n",
      "Val Loss: 0.5587 Acc: 0.8295\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train Loss: 0.5011 Acc: 0.8535\n",
      "Val Loss: 0.5561 Acc: 0.8380\n",
      "Saved new best model with accuracy: 0.8380\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train Loss: 0.5024 Acc: 0.8524\n",
      "Val Loss: 0.5514 Acc: 0.8310\n",
      "Test Loss: 0.5523 Acc: 0.8430\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final_test_accuracy</td><td>▁</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>learning_rate</td><td>██████▁▁▁▁</td></tr><tr><td>percent_trainable</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>trainable_parameters</td><td>▁</td></tr><tr><td>val_acc</td><td>▁▆▆▇█▆▇▇█▇</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>final_test_accuracy</td><td>0.843</td></tr><tr><td>final_test_loss</td><td>0.55227</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>percent_trainable</td><td>0.00896</td></tr><tr><td>test_accuracy</td><td>0.843</td></tr><tr><td>test_loss</td><td>0.55227</td></tr><tr><td>total_parameters</td><td>85806346</td></tr><tr><td>train_acc</td><td>0.85236</td></tr><tr><td>train_loss</td><td>0.50242</td></tr><tr><td>trainable_parameters</td><td>7690</td></tr><tr><td>val_acc</td><td>0.831</td></tr><tr><td>val_loss</td><td>0.55137</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vit_b_16_all_except_last</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/pgzackmh</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a><br>Synced 5 W&B file(s), 4 media file(s), 14 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250417_142049-pgzackmh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_144633-8omijs1m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m' target=\"_blank\">Model_comparison</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Model_comparison</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning/runs/8omijs1m</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_fine_tuning</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250417_144633-8omijs1m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All experiments completed!\n",
      "\n",
      "Test Accuracies:\n",
      "- vgg16: 0.7690\n",
      "- efficientnet_v2_s: 0.7250\n",
      "- vit_b_16: 0.8430\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7118869,
     "sourceId": 11371580,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7118997,
     "sourceId": 11371756,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
