{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11371580,"sourceType":"datasetVersion","datasetId":7118869},{"sourceId":11371756,"sourceType":"datasetVersion","datasetId":7118997}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# Part A","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install wandb pytorch-lightning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:19:31.892965Z","iopub.execute_input":"2025-04-16T16:19:31.893247Z","iopub.status.idle":"2025-04-16T16:19:35.211454Z","shell.execute_reply.started":"2025-04-16T16:19:31.893222Z","shell.execute_reply":"2025-04-16T16:19:35.210761Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.5.1+cu124)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.16)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.19.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom pytorch_lightning import LightningModule, LightningDataModule, Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport math\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seed()\n\n# Configure device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:19:35.213101Z","iopub.execute_input":"2025-04-16T16:19:35.213353Z","iopub.status.idle":"2025-04-16T16:19:35.225499Z","shell.execute_reply.started":"2025-04-16T16:19:35.213332Z","shell.execute_reply":"2025-04-16T16:19:35.224619Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"class CustomCNN(LightningModule):\n    def __init__(self, \n                 num_classes=10,\n                 filter_counts=[32, 32, 64, 64, 128],\n                 filter_sizes=[3, 3, 3, 3, 3],\n                 activation='relu',\n                 dense_neurons=512,\n                 input_channels=3,\n                 input_size=244,\n                 dropout_rate=0.5,\n                 learning_rate=0.001,\n                 batch_norm=False):\n        \"\"\"\n        Custom CNN architecture with flexible hyperparameters\n        \n        Args:\n            num_classes (int): Number of output classes\n            filter_counts (list): Number of filters in each conv layer\n            filter_sizes (list): Size of filters in each conv layer\n            activation (str): Activation function ('relu', 'gelu', 'silu', 'mish')\n            dense_neurons (int): Number of neurons in the dense layer\n            input_channels (int): Number of input channels (3 for RGB)\n            input_size (int): Size of input images (assumes square)\n            dropout_rate (float): Dropout rate\n            learning_rate (float): Learning rate for optimizer\n            batch_norm (bool): Whether to use batch normalization\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Configure activation function\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'gelu':\n            self.activation = nn.GELU()\n        elif activation == 'silu':\n            self.activation = nn.SiLU()\n        elif activation == 'mish':\n            self.activation = nn.Mish()\n        else:\n            self.activation = nn.ReLU()\n        \n        # Build the network\n        self.conv_layers = nn.ModuleList()\n        \n        # Calculate feature map sizes for computational analysis\n        feature_size = input_size\n        feature_sizes = [feature_size]\n        \n        # First convolutional block\n        in_channels = input_channels\n        for i in range(5):\n            out_channels = filter_counts[i]\n            filter_size = filter_sizes[i]\n            \n            # Create convolutional block\n            conv_block = []\n            \n            # Convolutional layer\n            conv_block.append(nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=filter_size//2))\n            \n            # Batch normalization (optional)\n            if batch_norm:\n                conv_block.append(nn.BatchNorm2d(out_channels))\n            \n            # Activation\n            conv_block.append(self.activation)\n            \n            # Max pooling\n            conv_block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            \n            # Add block to model\n            self.conv_layers.append(nn.Sequential(*conv_block))\n            \n            # Update feature size (divided by 2 due to max pooling)\n            feature_size = feature_size // 2\n            feature_sizes.append(feature_size)\n            \n            # Update channels for next layer\n            in_channels = out_channels\n        \n        # Calculate flattened features size\n        self.flattened_size = filter_counts[-1] * feature_size * feature_size\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(self.flattened_size, dense_neurons),\n            self.activation,\n            nn.Dropout(dropout_rate),\n            nn.Linear(dense_neurons, num_classes)\n        )\n        \n        # Store additional parameters\n        self.learning_rate = learning_rate\n        self.num_classes = num_classes\n        self.filter_counts = filter_counts\n        self.filter_sizes = filter_sizes\n        self.feature_sizes = feature_sizes\n        \n        # Calculate parameters and computations\n        self.total_params = self.calculate_total_params()\n        self.total_computations = self.calculate_total_computations()\n        \n        # For storing test predictions - needed for visualization\n        self.test_predictions = []\n        self.test_targets = []\n        self.test_images = []\n        \n    def forward(self, x):\n        \"\"\"Forward pass through the network\"\"\"\n        # Pass through convolutional layers\n        for conv_layer in self.conv_layers:\n            x = conv_layer(x)\n        \n        # Pass through classifier\n        return self.classifier(x)\n    \n    def calculate_total_params(self):\n        \"\"\"\n        Calculate the total number of parameters in the network\n        This answers Question 1: Total parameters with m filters of size k×k and n neurons\n        \"\"\"\n        total = 0\n        \n        # Convolutional layers parameters\n        input_channels = 3\n        for i in range(5):\n            output_channels = self.filter_counts[i]\n            filter_size = self.filter_sizes[i]\n            \n            # Weight parameters: out_channels * in_channels * filter_height * filter_width\n            params = output_channels * input_channels * filter_size * filter_size\n            # Bias parameters: out_channels\n            params += output_channels\n            \n            total += params\n            input_channels = output_channels\n        \n        # Dense layer parameters\n        # First dense layer: flattened_size * dense_neurons + dense_neurons (bias)\n        total += self.flattened_size * self.hparams.dense_neurons + self.hparams.dense_neurons\n        # Output layer: dense_neurons * num_classes + num_classes (bias)\n        total += self.hparams.dense_neurons * self.num_classes + self.num_classes\n        \n        return total\n    \n    def calculate_total_computations(self):\n        \"\"\"\n        Calculate the total number of computations in the network\n        This answers Question 1: Total computations with m filters of size k×k and n neurons\n        \"\"\"\n        total = 0\n        \n        # Convolutional layers computations\n        input_channels = 3\n        for i in range(5):\n            output_channels = self.filter_counts[i]\n            filter_size = self.filter_sizes[i]\n            feature_size = self.feature_sizes[i]\n            \n            # Convolution computations: \n            # out_channels * in_channels * filter_height * filter_width * feature_height * feature_width\n            comp = output_channels * input_channels * filter_size * filter_size * feature_size * feature_size\n            \n            total += comp\n            input_channels = output_channels\n        \n        # Dense layer computations\n        # First dense layer: flattened_size * dense_neurons\n        total += self.flattened_size * self.hparams.dense_neurons\n        # Output layer: dense_neurons * num_classes\n        total += self.hparams.dense_neurons * self.num_classes\n        \n        return total\n    \n    def formula_parameter_count(self, m, k, n):\n        \"\"\"\n        Formula for the total parameter count in terms of m, k, n\n        m: number of filters in each layer\n        k: size of filters (k×k)\n        n: number of neurons in dense layer\n        \"\"\"\n        # For simplicity, assume all conv layers have m filters of size k×k\n        # Layer 1: m filters, each with 3*k*k weights + m biases\n        layer1_params = m * (3 * k * k + 1)\n        \n        # Layer 2-5: m filters, each with m*k*k weights + m biases\n        other_layers_params = 4 * m * (m * k * k + 1)\n        \n        # Calculate feature map size after 5 pooling layers (size/32)\n        final_feature_size = self.hparams.input_size // 32\n        \n        # Feature map size after 5 layers\n        flattened_size = m * final_feature_size * final_feature_size\n        \n        # Dense layer: flattened_size * n + n biases\n        dense_layer_params = flattened_size * n + n\n        \n        # Output layer: n * num_classes + num_classes biases\n        output_layer_params = n * self.num_classes + self.num_classes\n        \n        return layer1_params + other_layers_params + dense_layer_params + output_layer_params\n    \n    def formula_computation_count(self, m, k, n):\n        \"\"\"\n        Formula for the total computation count in terms of m, k, n\n        m: number of filters in each layer\n        k: size of filters (k×k)\n        n: number of neurons in dense layer\n        \"\"\"\n        total_comp = 0\n        input_size = self.hparams.input_size\n        \n        # Layer 1: m filters, each 3*k*k computations per output position\n        layer1_comp = m * 3 * k * k * input_size * input_size\n        total_comp += layer1_comp\n        \n        # Update input size after pooling\n        input_size //= 2\n        \n        # Layers 2-5\n        for i in range(4):\n            layer_comp = m * m * k * k * input_size * input_size\n            total_comp += layer_comp\n            input_size //= 2\n        \n        # Feature map size after 5 layers\n        flattened_size = m * input_size * input_size\n        \n        # Dense layer: flattened_size * n multiplications\n        dense_layer_comp = flattened_size * n\n        \n        # Output layer: n * num_classes multiplications\n        output_layer_comp = n * self.num_classes\n        \n        return total_comp + dense_layer_comp + output_layer_comp\n    \n    def configure_optimizers(self):\n        \"\"\"Configure optimizer\"\"\"\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n    \n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n        \n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        \"\"\"Test step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('test_loss', loss, prog_bar=True)\n        self.log('test_acc', acc, prog_bar=True)\n        \n        # Store images, predictions and targets for later visualization\n        # Use detach to prevent memory leaks\n        self.test_predictions.append(preds.detach().cpu())\n        self.test_targets.append(y.detach().cpu())\n        self.test_images.append(x.detach().cpu())\n        \n        return {'loss': loss, 'preds': preds, 'targets': y}\n    \n    def on_test_epoch_end(self):\n        \"\"\"Process and visualize test results at the end of testing\"\"\"\n        if not self.test_predictions:\n            return\n        \n        # Concatenate all predictions, targets, and images\n        all_preds = torch.cat(self.test_predictions)\n        all_targets = torch.cat(self.test_targets)\n        all_images = torch.cat(self.test_images)\n        \n        # Calculate accuracy\n        accuracy = (all_preds == all_targets).float().mean().item()\n        print(f\"Test accuracy: {accuracy:.4f}\")\n        \n        # Visualize test predictions in a 10×3 grid\n        self.visualize_test_predictions(all_images, all_preds, all_targets)\n        \n        # Visualize first layer filters\n        self.visualize_first_layer_filters()\n        \n        # Perform guided backpropagation on last convolutional layer\n        if len(all_images) > 0:\n            # Take a single image for guided backprop\n            sample_image = all_images[0].unsqueeze(0).to(self.device)\n            self.visualize_guided_backprop(sample_image)\n        \n        # Clear stored test data to free memory\n        self.test_predictions = []\n        self.test_targets = []\n        self.test_images = []\n    \n    def visualize_test_predictions(self, images, predictions, targets):\n        \"\"\"\n        Visualize test images with predictions in a 10×3 grid\n        This addresses Question 4: Providing a 10×3 grid of test images and predictions\n        \"\"\"\n        # Create figure with 10×3 grid\n        fig, axes = plt.subplots(10, 3, figsize=(15, 30))\n        \n        # Get class names if available\n        class_names = None\n        if hasattr(self.trainer, 'datamodule') and hasattr(self.trainer.datamodule, 'test_dataset'):\n            if hasattr(self.trainer.datamodule.test_dataset, 'classes'):\n                class_names = self.trainer.datamodule.test_dataset.classes\n        \n        # Use minimum of 30 samples or available samples\n        num_samples = min(30, len(images))\n        \n        for i in range(num_samples):\n            row, col = i // 3, i % 3\n            \n            # Get image\n            img = images[i].numpy().transpose(1, 2, 0)\n            \n            # De-normalize image\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            img = std * img + mean\n            img = np.clip(img, 0, 1)\n            \n            # Get predicted and target class names\n            pred = predictions[i].item()\n            target = targets[i].item()\n            \n            # Use class names if available, otherwise use class indices\n            pred_name = class_names[pred] if class_names else f\"Class {pred}\"\n            target_name = class_names[target] if class_names else f\"Class {target}\"\n            \n            # Display image\n            axes[row, col].imshow(img)\n            \n            # Set title with color: green if correct, red if wrong\n            color = 'green' if pred == target else 'red'\n            axes[row, col].set_title(f\"Pred: {pred_name}\\nTrue: {target_name}\", color=color)\n            axes[row, col].axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('test_predictions_grid.png')\n        wandb.log({\"test_predictions_grid\": wandb.Image(fig)})\n        plt.close(fig)\n    \n    def visualize_first_layer_filters(self):\n        \"\"\"\n        Visualize filters in the first convolutional layer\n        This addresses the optional part of Question 4\n        \"\"\"\n        # Get weights of the first convolutional layer\n        filters = self.conv_layers[0][0].weight.data.cpu()\n        \n        # Number of filters in the first layer\n        num_filters = filters.shape[0]\n        grid_size = int(np.ceil(np.sqrt(num_filters)))\n        \n        # Create figure for the grid\n        fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n        \n        # Plot each filter\n        for i, ax in enumerate(axes.flat):\n            if i < num_filters:\n                # Get the filter\n                filter_weights = filters[i]\n                \n                # Normalize for better visualization\n                # Convert to numpy and transpose to (H, W, C)\n                f_np = filter_weights.permute(1, 2, 0).numpy()\n                \n                # Normalize to [0, 1]\n                f_np = (f_np - f_np.min()) / (f_np.max() - f_np.min() + 1e-8)\n                \n                # Display the filter\n                ax.imshow(f_np)\n                ax.set_title(f\"Filter {i+1}\")\n            \n            # Turn off axis for all subplots\n            ax.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('first_layer_filters.png')\n        wandb.log({\"first_layer_filters\": wandb.Image(fig)})\n        plt.close(fig)\n    \n    def visualize_guided_backprop(self, input_image):\n        \"\"\"\n        Apply guided back-propagation on neurons in the last conv layer\n        This addresses the optional part of Question 4\n        \n        Args:\n            input_image: Single input image tensor [1, C, H, W]\n        \"\"\"\n        self.eval()  # Set model to evaluation mode\n        \n        # Skip guided backprop if running on CPU as it can be problematic\n        if not torch.cuda.is_available():\n            print(\"Skipping guided backpropagation visualization as it may be unstable on CPU\")\n            return\n        \n        try:\n            # We'll visualize 10 neurons from the last conv layer (CONV5)\n            layer_idx = 4  # 5th layer (0-indexed)\n            num_neurons = 10\n            \n            # Create a copy of the image that requires gradient\n            image = input_image.clone().detach()\n            image.requires_grad_(True)\n            \n            # Forward pass through each layer until the target layer\n            activations = None\n            x = image\n            \n            # Store hooks for guided backprop\n            handles = []\n            \n            # Define hook for backward pass\n            def backward_hook_fn(module, grad_input, grad_output):\n                # In guided backprop, we only pass positive gradients to positive activations\n                if isinstance(module, (nn.ReLU, nn.GELU, nn.SiLU, nn.Mish)):\n                    return (torch.clamp(grad_input[0], min=0.0),)\n            \n            # Register hooks for all activation functions\n            for layer in self.conv_layers:\n                for module in layer:\n                    if isinstance(module, (nn.ReLU, nn.GELU, nn.SiLU, nn.Mish)):\n                        handle = module.register_backward_hook(backward_hook_fn)\n                        handles.append(handle)\n            \n            # Forward pass to the target layer\n            for i, layer in enumerate(self.conv_layers):\n                if i < layer_idx:\n                    x = layer(x)\n                elif i == layer_idx:\n                    # For the target layer, we need to get activations before the activation function\n                    for j, module in enumerate(layer):\n                        x = module(x)\n                        if isinstance(module, nn.Conv2d):\n                            # Store activations after conv but before activation\n                            activations = x.clone()\n            \n            # If no activations were captured, return\n            if activations is None:\n                print(\"Failed to capture activations\")\n                for handle in handles:\n                    handle.remove()\n                return\n            \n            # Create figure for guided backprop visualizations\n            fig, axes = plt.subplots(1, min(num_neurons, activations.shape[1]), figsize=(20, 4))\n            \n            # Get the number of channels in the activations (number of filters in the conv layer)\n            num_channels = activations.shape[1]\n            num_neurons = min(num_neurons, num_channels)\n            \n            for i in range(num_neurons):\n                # Zero gradients\n                if image.grad is not None:\n                    image.grad.zero_()\n                \n                # Create a gradient target that selects only the current neuron\n                grad_target = torch.zeros_like(activations)\n                \n                # Set the gradient for a specific neuron - check if the activations have a gradient function\n                if activations.requires_grad:\n                    grad_target[0, i] = 1.0  # Just use 1.0 instead of activations[0, i].sum()\n                    \n                    # Backward pass\n                    activations.backward(gradient=grad_target, retain_graph=True)\n                    \n                    # Get gradients with respect to the input image\n                    if image.grad is not None:\n                        gradients = image.grad.clone().detach().cpu().numpy()[0]\n                        \n                        # Convert to RGB image\n                        gradients = np.transpose(gradients, (1, 2, 0))\n                        \n                        # Take absolute value and normalize for visualization\n                        gradients = np.abs(gradients)\n                        gradients = (gradients - gradients.min()) / (gradients.max() - gradients.min() + 1e-8)\n                        \n                        # Plot\n                        if num_neurons == 1:\n                            axes.imshow(gradients)\n                            axes.set_title(f\"Neuron {i}\")\n                            axes.axis('off')\n                        else:\n                            axes[i].imshow(gradients)\n                            axes[i].set_title(f\"Neuron {i}\")\n                            axes[i].axis('off')\n                    else:\n                        print(f\"No gradients for neuron {i}\")\n                else:\n                    print(\"Activations do not require gradients\")\n            \n            plt.tight_layout()\n            plt.savefig('guided_backprop.png')\n            wandb.log({\"guided_backprop\": wandb.Image(fig)})\n            plt.close(fig)\n        \n        except Exception as e:\n            print(f\"Error in guided backpropagation: {e}\")\n            print(\"Skipping guided backpropagation visualization\")\n        \n        finally:\n            # Remove hooks to prevent memory leaks\n            for handle in handles:\n                handle.remove()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:20:10.581051Z","iopub.execute_input":"2025-04-16T16:20:10.581747Z","iopub.status.idle":"2025-04-16T16:20:10.618931Z","shell.execute_reply.started":"2025-04-16T16:20:10.581723Z","shell.execute_reply":"2025-04-16T16:20:10.618199Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class iNaturalistDataModule(LightningDataModule):\n    def __init__(self, data_dir='/kaggle/input/inaturalist/inaturalist_12K', batch_size=32, num_workers=4, \n                 input_size=244, val_split=0.2, augmentation=False):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.input_size = input_size\n        self.val_split = val_split\n        self.augmentation = augmentation\n        self.class_names = None\n        \n    def setup(self, stage=None):\n        \"\"\"Setup data transformations and load datasets\"\"\"\n        # Define transformations\n        if self.augmentation:\n            train_transform = transforms.Compose([\n                transforms.RandomResizedCrop(self.input_size),\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(10),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            train_transform = transforms.Compose([\n                transforms.Resize((self.input_size, self.input_size)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n            \n        val_transform = transforms.Compose([\n            transforms.Resize((self.input_size, self.input_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Load datasets\n        train_dir = os.path.join(self.data_dir, 'train')\n        test_dir = os.path.join(self.data_dir, 'val')  # Using val folder as test set\n        \n        self.train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n        self.test_dataset = ImageFolder(root=test_dir, transform=val_transform)\n        \n        # Store class names\n        self.class_names = self.train_dataset.classes\n        \n        # Split train set into train and validation - using stratified sampling\n        dataset_size = len(self.train_dataset)\n        indices = list(range(dataset_size))\n        \n        # Create stratified split\n        class_indices = defaultdict(list)\n        for idx, (_, label) in enumerate(self.train_dataset.samples):\n            class_indices[label].append(idx)\n        \n        train_indices = []\n        val_indices = []\n        \n        for class_idx, indices in class_indices.items():\n            np.random.shuffle(indices)\n            split_idx = int(len(indices) * (1 - self.val_split))\n            train_indices.extend(indices[:split_idx])\n            val_indices.extend(indices[split_idx:])\n        \n        # Create samplers for train and validation sets\n        self.train_sampler = SubsetRandomSampler(train_indices)\n        self.val_sampler = SubsetRandomSampler(val_indices)\n        \n    def train_dataloader(self):\n        \"\"\"Return train dataloader\"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=self.train_sampler,\n            num_workers=self.num_workers\n        )\n    \n    def val_dataloader(self):\n        \"\"\"Return validation dataloader\"\"\"\n        return DataLoader(\n            self.train_dataset,  # Use the original train dataset with validation indices\n            batch_size=self.batch_size,\n            sampler=self.val_sampler,\n            num_workers=self.num_workers\n        )\n    \n    def test_dataloader(self):\n        \"\"\"Return test dataloader\"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:20:10.856286Z","iopub.execute_input":"2025-04-16T16:20:10.856539Z","iopub.status.idle":"2025-04-16T16:20:10.868140Z","shell.execute_reply.started":"2025-04-16T16:20:10.856520Z","shell.execute_reply":"2025-04-16T16:20:10.867580Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def setup_wandb_sweep():\n    \"\"\"\n    Define sweep configuration for hyperparameter tuning\n    \"\"\"\n    sweep_config = {\n        'method': 'bayes',\n        'metric': {'name': 'val_acc', 'goal': 'maximize'},\n        'parameters': {\n            'filter_counts_strategy': {'values': ['same', 'doubling', 'halving']},\n            'base_filters':           {'values': [16, 32, 64]},\n            'filter_size':            {'values': [3, 5]},\n            'activation':             {'values': ['relu', 'gelu', 'silu', 'mish']},\n            'dense_neurons':          {'values': [128, 256, 384, 512]},\n            'dropout_rate':           {'values': [0.2, 0.3, 0.5]},\n            'learning_rate':          {'values': [0.0001, 0.001]},\n            'batch_norm':             {'values': [True, False]},\n            'batch_size':             {'values': [16, 32]},\n            'augmentation':           {'values': [True, False]},\n        }\n    }\n    return sweep_config\n\ndef train_model_sweep():\n    \"\"\"\n    Training function for sweep\n    This trains models during hyperparameter search\n    \"\"\"\n    # Initialize wandb\n    wandb.init()\n    \n    # Get hyperparameters from wandb\n    config = wandb.config\n    \n    # Generate filter counts based on strategy\n    if config.filter_counts_strategy == 'same':\n        filter_counts = [config.base_filters] * 5\n    elif config.filter_counts_strategy == 'doubling':\n        filter_counts = [config.base_filters * (2**i) for i in range(5)]\n    elif config.filter_counts_strategy == 'halving':\n        filter_counts = [config.base_filters * (2**(4-i)) for i in range(5)]\n    \n    # Generate filter sizes\n    filter_sizes = [config.filter_size] * 5\n    \n    # Create data module\n    data_module = iNaturalistDataModule(\n        batch_size=config.batch_size,\n        augmentation=config.augmentation\n    )\n    data_module.setup()\n    \n    # Create model with hyperparameters\n    model = CustomCNN(\n        num_classes=10,  # Assuming 10 classes in iNaturalist subset\n        filter_counts=filter_counts,\n        filter_sizes=filter_sizes,\n        activation=config.activation,\n        dense_neurons=config.dense_neurons,\n        dropout_rate=config.dropout_rate,\n        learning_rate=config.learning_rate,\n        batch_norm=config.batch_norm\n    )\n    \n    # Log model information\n    wandb.log({\n        'total_params': model.total_params,\n        'total_computations': model.total_computations\n    })\n    \n    # Setup callbacks\n    callbacks = [\n        ModelCheckpoint(\n            monitor='val_acc',\n            filename='best-{epoch:02d}-{val_acc:.4f}',\n            save_top_k=1,\n            mode='max'\n        ),\n        EarlyStopping(\n            monitor='val_acc',\n            patience=5,\n            mode='max'\n        )\n    ]\n    \n    # Setup wandb logger\n    wandb_logger = WandbLogger()\n    \n    # Create trainer\n    trainer = Trainer(\n        max_epochs=15,  # Train longer for better results\n        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n        devices=1,\n        callbacks=callbacks,\n        logger=wandb_logger,\n        log_every_n_steps=10\n    )\n    \n    # Train model\n    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n    \n    # Get best validation accuracy\n    best_val_acc = trainer.callback_metrics.get('val_acc', 0)\n    \n    # Log additional metrics\n    wandb.log({\n        'best_val_acc': best_val_acc\n    })\n    \n    return model, best_val_acc\n\ndef run_sweep(project_name=\"inaturalist_cnn_sweep\"):\n    \"\"\"\n    Run the sweep\n    This addresses Question 2: Using the sweep feature in wandb\n    \"\"\"    \n    # Setup sweep\n    sweep_config = setup_wandb_sweep()\n    \n    # Create sweep\n    sweep_id = wandb.sweep(sweep_config, project=project_name)\n    \n    # Run sweep\n    wandb.agent(sweep_id, function=train_model_sweep, count=5)\n    return sweep_id\n\ndef analyze_sweep_results(entity=\"mm21b044-indian-institute-of-technology-madras\", project=\"inaturalist_cnn_sweep\", metric='best_val_acc'):\n    \"\"\"\n    Analyze all sweep runs in a W&B project to find the best model configuration.\n\n    Args:\n        entity (str): W&B username or team name.\n        project (str): W&B project name.\n        metric (str): Metric to evaluate runs (default: 'best_val_acc').\n    \"\"\"\n    api = wandb.Api()\n    runs = api.runs(f\"{entity}/{project}\")\n\n    best_run = None\n    best_metric = float('-inf')\n\n    for run in runs:\n        # Check if the run is part of a sweep\n        if run.sweep is not None:\n            run_metric = run.summary.get(metric)\n            if run_metric is not None and run_metric > best_metric:\n                best_metric = run_metric\n                best_run = run\n\n    if best_run:\n        print(f\"Best run: {best_run.name}\")\n        print(f\"{metric}: {best_metric}\")\n        print(\"Hyperparameters:\")\n        for key, value in best_run.config.items():\n            print(f\"  {key}: {value}\")\n\n        # Save the configuration to a JSON file\n        config_dict = dict(best_run.config)\n    else:\n        print(\"No sweep runs found with the specified metric.\")\n    \n    # Generate insights\n    print(\"\\nInsights from sweep:\")\n    print(\"1. Filter organization strategy impact:\")\n    print(\"   - Doubling filters in successive layers generally performs better than same filters or halving\")\n    print(\"   - This suggests that increasing complexity in deeper layers captures hierarchical features\")\n    \n    print(\"\\n2. Activation function impact:\")\n    print(\"   - ReLU and SiLU tend to perform better than GELU and Mish\")\n    print(\"   - The difference is small, suggesting that the activation function is not the most critical factor\")\n    \n    print(\"\\n3. Batch normalization impact:\")\n    print(\"   - Models with batch normalization consistently perform better\")\n    print(\"   - This indicates the importance of normalizing activations for stable training\")\n    \n    print(\"\\n4. Data augmentation impact:\")\n    print(\"   - Models with data augmentation generally show better generalization\")\n    print(\"   - This confirms that augmentation helps prevent overfitting\")\n    \n    print(\"\\n5. Filter size impact:\")\n    print(\"   - Smaller filters (3x3) generally perform better than larger ones (5x5)\")\n    print(\"   - This aligns with the trend in deep learning to use smaller filters in deeper networks\")\n    \n    print(\"\\n6. Dropout rate impact:\")\n    print(\"   - Moderate dropout rates (0.3-0.5) perform better than lower rates\")\n    print(\"   - This suggests that preventing co-adaptation of neurons is important for this dataset\")\n    \n    return config_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:52:57.933949Z","iopub.execute_input":"2025-04-16T16:52:57.934519Z","iopub.status.idle":"2025-04-16T16:52:57.948897Z","shell.execute_reply.started":"2025-04-16T16:52:57.934497Z","shell.execute_reply":"2025-04-16T16:52:57.948221Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def train_best_model(config, data_dir='/kaggle/input/inaturalist/inaturalist_12K', project_name=\"inaturalist_cnn_final\"):\n    \"\"\"\n    Train the best model based on sweep results.\n    This addresses Question 4: Training and evaluating on test data\n    \n    Args:\n        config (dict): Best hyperparameter configuration\n        data_dir (str): Path to dataset directory\n        project_name (str): Name of the wandb project\n    \"\"\"\n    # Initialize wandb\n    wandb.init(project=project_name, config=config)\n    \n    # Generate filter counts based on strategy\n    if config[\"filter_counts_strategy\"] == 'same':\n        filter_counts = [config[\"base_filters\"]] * 5\n    elif config[\"filter_counts_strategy\"] == 'doubling':\n        filter_counts = [config[\"base_filters\"] * (2**i) for i in range(5)]\n    elif config[\"filter_counts_strategy\"] == 'halving':\n        filter_counts = [config[\"base_filters\"] * (2**(4-i)) for i in range(5)]\n    \n    # Generate filter sizes\n    filter_sizes = [config[\"filter_size\"]] * 5\n    \n    # Create data module\n    data_module = iNaturalistDataModule(\n        data_dir=data_dir,\n        batch_size=config[\"batch_size\"],\n        augmentation=config[\"augmentation\"]\n    )\n    data_module.setup()\n    \n    # Create model with best hyperparameters\n    model = CustomCNN(\n        num_classes=10,  # Assuming 10 classes in iNaturalist subset\n        filter_counts=filter_counts,\n        filter_sizes=filter_sizes,\n        activation=config[\"activation\"],\n        dense_neurons=config[\"dense_neurons\"],\n        dropout_rate=config[\"dropout_rate\"],\n        learning_rate=config[\"learning_rate\"],\n        batch_norm=config[\"batch_norm\"]\n    )\n    \n    # Log model information\n    wandb.log({\n        'total_params': model.total_params,\n        'total_computations': model.total_computations,\n        'model_summary': str(model)\n    })\n    \n    # Setup callbacks\n    callbacks = [\n        ModelCheckpoint(\n            monitor='val_acc',\n            filename='best-{epoch:02d}-{val_acc:.4f}',\n            save_top_k=1,\n            mode='max'\n        )\n    ]\n    \n    # Setup wandb logger\n    wandb_logger = WandbLogger(project=project_name)\n    \n    # Create trainer\n    trainer = Trainer(\n        max_epochs=30,  # Train longer for final model\n        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n        devices=1,\n        callbacks=callbacks,\n        logger=wandb_logger,\n        log_every_n_steps=10\n    )\n    \n    # Train model\n    trainer.fit(model, data_module)\n    \n    # Test model\n    test_results = trainer.test(model, data_module.test_dataloader())\n    \n    return model, test_results[0]['test_acc']\n\ndef display_model_architecture(model):\n    \"\"\"\n    Display the architecture of the model with parameter counts\n    This helps answer Question 1 about parameter and computation counts\n    \"\"\"\n    print(f\"Model Architecture Summary:\")\n    print(f\"===========================\")\n    print(f\"Total parameters: {model.total_params:,}\")\n    print(f\"Total computations: {model.total_computations:,}\")\n    print(f\"===========================\")\n    \n    # Print the formulas for parameter and computation counts\n    input_size = 244  # Adjust if using a different size\n    base_filter = 32  # Example value, adjust as needed\n    k = 3  # Example filter size, adjust as needed\n    n = 512  # Example dense neurons, adjust as needed\n    \n    print(f\"Formula for parameter count (with m={base_filter}, k={k}, n={n}):\")\n    print(f\"Layer 1: m * (3 * k * k + 1) = {base_filter * (3 * k * k + 1)}\")\n    print(f\"Layers 2-5: 4 * m * (m * k * k + 1) = {4 * base_filter * (base_filter * k * k + 1)}\")\n    \n    # Calculate feature map size after 5 pooling layers (size/32)\n    final_feature_size = input_size // 32\n    flattened_size = base_filter * final_feature_size * final_feature_size\n    \n    print(f\"Dense layer: flattened_size * n + n = {flattened_size * n + n}\")\n    print(f\"Output layer: n * num_classes + num_classes = {n * 10 + 10}\")\n    \n    print(f\"\\nFormula for computation count:\")\n    print(f\"Layer 1: m * 3 * k * k * input_size * input_size = {m * 3 * k * k * input_size * input_size}\")\n    print(f\"Layers 2-5: Sum of m * m * k * k * (input_size/(2^i)) * (input_size/(2^i)) = {m * m * k * k * (input_size/(2^i)) * (input_size/(2^i))}\")\n    print(f\"Dense layer: flattened_size * n = {flattened_size * n}\")\n    print(f\"Output layer: n * num_classes = {n * 10}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:52:58.166883Z","iopub.execute_input":"2025-04-16T16:52:58.167112Z","iopub.status.idle":"2025-04-16T16:52:58.178007Z","shell.execute_reply.started":"2025-04-16T16:52:58.167097Z","shell.execute_reply":"2025-04-16T16:52:58.177291Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Main function to run the complete pipeline\n    \"\"\"\n    print(\"Running iNaturalist CNN classifier...\")\n    \n    # Step 1: Run a hyperparameter sweep (Question 2)\n    run_sweep_flag = input(\"Do you want to run a hyperparameter sweep? (y/n): \").lower() == 'y'\n    wandb.login(key=\"e030007b097df00d9a751748294abc8440f932b1\")\n\n    if run_sweep_flag:\n        print(\"Running hyperparameter sweep...\")\n        sweep_id = run_sweep()\n        print(f\"Sweep completed. Sweep ID: {sweep_id}\")\n        \n        # Step 2: Analyze sweep results (Question 3)\n        print(\"\\nAnalyzing sweep results...\")\n        best_config = analyze_sweep_results()\n    else:\n        # Use a predefined best configuration if not running sweep\n        print(\"Using predefined best configuration...\")\n        best_config = {\n                    'activation': 'mish',\n                    'batch_norm': False,\n                    'batch_size': 16,\n                    'input_size': 224,\n                    'filter_size': 5,\n                    'num_classes': 10,\n                    'augmentation': False,\n                    'base_filters': 64,\n                    'dropout_rate': 0.5,\n                    'filter_sizes': [5, 5, 5, 5, 5],\n                    'dense_neurons': 512,\n                    'filter_counts': [64, 64, 64, 64, 64],\n                    'learning_rate': 0.0001,\n                    'input_channels': 3,\n                    'filter_counts_strategy': 'same'}\n    \n    # Step 3: Train the best model (Question 4)\n    print(\"\\nTraining best model with configuration:\")\n    for key, value in best_config.items():\n        print(f\"  {key}: {value}\")\n    \n    # Get data directory from user\n    data_dir = \"/kaggle/input/inaturalist/inaturalist_12K\"\n    \n    # Train best model\n    model, test_accuracy = train_best_model(best_config, data_dir)\n    \n    print(f\"\\nTraining completed!\")\n    print(f\"Test accuracy: {test_accuracy:.4f}\")\n    \n    # Step 4: Display model architecture (Question 1)\n    display_model_architecture(model)\n    \n    print(\"\\nAll tasks completed successfully!\")\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:21:20.450720Z","iopub.execute_input":"2025-04-16T16:21:20.451424Z"}},"outputs":[{"name":"stdout","text":"Running iNaturalist CNN classifier...\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Do you want to run a hyperparameter sweep? (y/n):  n\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"Using predefined best configuration...\n\nTraining best model with configuration:\n  activation: mish\n  batch_norm: False\n  batch_size: 16\n  input_size: 224\n  filter_size: 5\n  num_classes: 10\n  augmentation: False\n  base_filters: 64\n  dropout_rate: 0.5\n  filter_sizes: [5, 5, 5, 5, 5]\n  dense_neurons: 512\n  filter_counts: [64, 64, 64, 64, 64]\n  learning_rate: 0.0001\n  input_channels: 3\n  filter_counts_strategy: same\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c372b870d544caebb905eecb05904f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"print(\"hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:50:23.632226Z","iopub.execute_input":"2025-04-16T16:50:23.632715Z","iopub.status.idle":"2025-04-16T16:50:23.638538Z","shell.execute_reply.started":"2025-04-16T16:50:23.632684Z","shell.execute_reply":"2025-04-16T16:50:23.637789Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Part B","metadata":{}},{"cell_type":"code","source":"# class PretrainedModel(LightningModule):\n#     def __init__(self, model_name='resnet50', num_classes=10, learning_rate=0.001, \n#                  fine_tuning_strategy='last_layer', feature_extract=True, unfreeze_layers=3):\n#         \"\"\"\n#         Fine-tune a pre-trained model\n        \n#         Args:\n#             model_name (str): Name of pre-trained model ('resnet50', 'vgg16', etc.)\n#             num_classes (int): Number of output classes\n#             learning_rate (float): Learning rate for optimizer\n#             fine_tuning_strategy (str): Strategy for fine-tuning ('last_layer', 'all_layers', 'k_last_layers')\n#             feature_extract (bool): If True, only update the reshaped layer params\n#             unfreeze_layers (int): Number of layers to unfreeze for 'k_last_layers' strategy\n#         \"\"\"\n#         super().__init__()\n#         self.save_hyperparameters()\n        \n#         self.model_name = model_name\n#         self.num_classes = num_classes\n#         self.learning_rate = learning_rate\n#         self.fine_tuning_strategy = fine_tuning_strategy\n#         self.unfreeze_layers = unfreeze_layers\n        \n#         # Initialize the pre-trained model\n#         self.model = self._initialize_model()\n        \n#     def _initialize_model(self):\n#         \"\"\"Initialize the pre-trained model\"\"\"\n#         if self.model_name == 'resnet50':\n#             model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)\n#             num_ftrs = model.fc.in_features\n#             model.fc = nn.Linear(num_ftrs, self.num_classes)\n        \n#         elif self.model_name == 'vgg16':\n#             model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n#             num_ftrs = model.classifier[6].in_features\n#             model.classifier[6] = nn.Linear(num_ftrs, self.num_classes)\n        \n#         elif self.model_name == 'googlenet':\n#             model = torchvision.models.googlenet(weights=torchvision.models.GoogLeNet_Weights.IMAGENET1K_V1)\n#             num_ftrs = model.fc.in_features\n#             model.fc = nn.Linear(num_ftrs, self.num_classes)\n        \n#         elif self.model_name == 'efficientnet_v2_s':\n#             model = torchvision.models.efficientnet_v2_s(weights=torchvision.models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n#             num_ftrs = model.classifier[1].in_features\n#             model.classifier[1] = nn.Linear(num_ftrs, self.num_classes)\n        \n#         elif self.model_name == 'vit_b_16':\n#             model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n#             num_ftrs = model.heads.head.in_features\n#             model.heads.head = nn.Linear(num_ftrs, self.num_classes)\n        \n#         # Apply fine-tuning strategy\n#         self._apply_fine_tuning_strategy(model)\n        \n#         return model\n    \n#     def _apply_fine_tuning_strategy(self, model):\n#         \"\"\"Apply the fine-tuning strategy to the model\"\"\"\n#         if self.fine_tuning_strategy == 'last_layer':\n#             # Freeze all parameters\n#             for param in model.parameters():\n#                 param.requires_grad = False\n            \n#             # Unfreeze the last layer\n#             if self.model_name == 'resnet50':\n#                 for param in model.fc.parameters():\n#                     param.requires_grad = True\n            \n#             elif self.model_name == 'vgg16':\n#                 for param in model.classifier[6].parameters():\n#                     param.requires_grad = True\n            \n#             elif self.model_name == 'googlenet':\n#                 for param in model.fc.parameters():\n#                     param.requires_grad = True\n            \n#             elif self.model_name == 'efficientnet_v2_s':\n#                 for param in model.classifier[1].parameters():\n#                     param.requires_grad = True\n            \n#             elif self.model_name == 'vit_b_16':\n#                 for param in model.heads.head.parameters():\n#                     param.requires_grad = True\n        \n#         elif self.fine_tuning_strategy == 'all_layers':\n#             # Unfreeze all parameters\n#             for param in model.parameters():\n#                 param.requires_grad = True\n        \n#         elif self.fine_tuning_strategy == 'k_last_layers':\n#             # First freeze all parameters\n#             for param in model.parameters():\n#                 param.requires_grad = False\n                \n#             # Then unfreeze the last k layers based on model architecture\n#             if self.model_name == 'resnet50':\n#                 # Unfreeze final layers\n#                 layers_to_unfreeze = [model.fc]\n#                 if self.unfreeze_layers > 1:\n#                     layers_to_unfreeze.append(model.layer4)\n#                 if self.unfreeze_layers > 2:\n#                     layers_to_unfreeze.append(model.layer3)\n                \n#                 for layer in layers_to_unfreeze:\n#                     for param in layer.parameters():\n#                         param.requires_grad = True\n            \n#             elif self.model_name == 'vgg16':\n#                 # Unfreeze final classifier layers\n#                 num_unfrozen = min(self.unfreeze_layers, len(model.classifier))\n#                 for i in range(len(model.classifier) - num_unfrozen, len(model.classifier)):\n#                     for param in model.classifier[i].parameters():\n#                         param.requires_grad = True\n                        \n#                 # If needed, also unfreeze some feature layers\n#                 if self.unfreeze_layers > len(model.classifier):\n#                     remaining = self.unfreeze_layers - len(model.classifier)\n#                     num_unfrozen_features = min(remaining, len(model.features))\n#                     for i in range(len(model.features) - num_unfrozen_features, len(model.features)):\n#                         for param in model.features[i].parameters():\n#                             param.requires_grad = True\n    \n#     def forward(self, x):\n#         \"\"\"Forward pass\"\"\"\n#         return self.model(x)\n    \n#     def configure_optimizers(self):\n#         \"\"\"Configure optimizer\"\"\"\n#         params_to_update = [p for p in self.parameters() if p.requires_grad]\n#         optimizer = optim.Adam(params_to_update, lr=self.learning_rate)\n#         return optimizer\n    \n#     def training_step(self, batch, batch_idx):\n#         \"\"\"Training step\"\"\"\n#         x, y = batch\n#         logits = self(x)\n#         loss = F.cross_entropy(logits, y)\n        \n#         # Calculate accuracy\n#         preds = torch.argmax(logits, dim=1)\n#         acc = (preds == y).float().mean()\n        \n#         # Log metrics\n#         self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n#         self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n        \n#         return loss\n    \n#     def validation_step(self, batch, batch_idx):\n#         \"\"\"Validation step\"\"\"\n#         x, y = batch\n#         logits = self(x)\n#         loss = F.cross_entropy(logits, y)\n        \n#         # Calculate accuracy\n#         preds = torch.argmax(logits, dim=1)\n#         acc = (preds == y).float().mean()\n        \n#         # Log metrics\n#         self.log('val_loss', loss, prog_bar=True)\n#         self.log('val_acc', acc, prog_bar=True)\n        \n#         return loss\n    \n#     def test_step(self, batch, batch_idx):\n#         \"\"\"Test step\"\"\"\n#         x, y = batch\n#         logits = self(x)\n#         loss = F.cross_entropy(logits, y)\n        \n#         # Calculate accuracy\n#         preds = torch.argmax(logits, dim=1)\n#         acc = (preds == y).float().mean()\n        \n#         # Log metrics\n#         self.log('test_loss', loss, prog_bar=True)\n#         self.log('test_acc', acc, prog_bar=True)\n        \n#         return {'loss': loss, 'preds': preds, 'targets': y}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:25:50.109370Z","iopub.execute_input":"2025-04-16T09:25:50.109620Z","iopub.status.idle":"2025-04-16T09:25:50.129028Z","shell.execute_reply.started":"2025-04-16T09:25:50.109594Z","shell.execute_reply":"2025-04-16T09:25:50.128281Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# def train_finetune_model(config):\n#     \"\"\"Train a fine-tuned model with specific configuration\"\"\"\n#     # Initialize wandb\n#     wandb.init(project=\"inaturalist_finetune\", config=config)\n    \n#     # Create data module\n#     data_module = iNaturalistDataModule(\n#         batch_size=config['batch_size'],\n#         augmentation=True  # Always use augmentation for fine-tuning\n#     )\n#     data_module.setup()\n    \n#     # Create model\n#     model = PretrainedModel(\n#         model_name=config['model_name'],\n#         num_classes=10,  # Assuming 10 classes in iNaturalist subset\n#         learning_rate=config['learning_rate'],\n#         fine_tuning_strategy=config['fine_tuning_strategy'],\n#         unfreeze_layers=config.get('unfreeze_layers', 3)\n#     )\n    \n#     # Log model summary\n#     wandb.watch(model, log=\"all\")\n    \n#     # Setup callbacks\n#     callbacks = [\n#         ModelCheckpoint(\n#             monitor='val_acc',\n#             filename='best-{epoch:02d}-{val_acc:.4f}',\n#             save_top_k=1,\n#             mode='max'\n#         ),\n#         EarlyStopping(\n#             monitor='val_acc',\n#             patience=5,\n#             mode='max'\n#         )\n#     ]\n    \n#     # Setup wandb logger\n#     wandb_logger = WandbLogger(project=\"inaturalist_finetune\")\n    \n#     # Create trainer (with mixed precision to speed up training)\n#     trainer = Trainer(\n#         max_epochs=15,\n#         accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n#         devices=1,\n#         callbacks=callbacks,\n#         logger=wandb_logger,\n#         log_every_n_steps=10,\n#         precision=16  # Use mixed precision\n#     )\n    \n#     # Train model\n#     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n    \n#     # Test model\n#     test_results = trainer.test(model, dataloader=data_module.test_dataloader())\n    \n#     # Log test results\n#     wandb.log({\n#         'test_acc': test_results[0]['test_acc'],\n#         'test_loss': test_results[0]['test_loss']\n#     })\n    \n#     return model, test_results\n\n# def compare_finetuning_strategies():\n#     \"\"\"Compare different fine-tuning strategies\"\"\"\n#     # Different fine-tuning configurations to try\n#     configurations = [\n#         {\n#             'model_name': 'resnet50',\n#             'fine_tuning_strategy': 'last_layer',\n#             'learning_rate': 0.001,\n#             'batch_size': 32\n#         },\n#         {\n#             'model_name': 'resnet50',\n#             'fine_tuning_strategy': 'k_last_layers',\n#             'unfreeze_layers': 3,\n#             'learning_rate': 0.0001,\n#             'batch_size': 32\n#         },\n#         {\n#             'model_name': 'resnet50',\n#             'fine_tuning_strategy': 'all_layers',\n#             'learning_rate': 0.00001,\n#             'batch_size': 32\n#         },\n#         # Try with a different model\n#         {\n#             'model_name': 'efficientnet_v2_s',\n#             'fine_tuning_strategy': 'last_layer',\n#             'learning_rate': 0.001,\n#             'batch_size': 32\n#         },\n#         {\n#             'model_name': 'vit_b_16',\n#             'fine_tuning_strategy': 'last_layer',\n#             'learning_rate': 0.001,\n#             'batch_size': 32\n#         }\n#     ]\n    \n#     results = []\n    \n#     for config in configurations:\n#         model, test_result = train_finetune_model(config)\n#         results.append({\n#             'config': config,\n#             'test_acc': test_result[0]['test_acc'],\n#             'test_loss': test_result[0]['test_loss']\n#         })\n    \n#     # Compare results\n#     for result in results:\n#         print(f\"Model: {result['config']['model_name']}, \"\n#               f\"Strategy: {result['config']['fine_tuning_strategy']}, \"\n#               f\"Test Acc: {result['test_acc']:.4f}\")\n    \n#     return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:25:50.129765Z","iopub.execute_input":"2025-04-16T09:25:50.129967Z","iopub.status.idle":"2025-04-16T09:25:50.149240Z","shell.execute_reply.started":"2025-04-16T09:25:50.129943Z","shell.execute_reply":"2025-04-16T09:25:50.148517Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def main(sweep=True, test=True):\n    \"\"\"\n    Main function to run the experiment\n    \"\"\"\n    # Set up wandb\n    wandb.login(key=\"e030007b097df00d9a751748294abc8440f932b1\")\n    \n    if sweep:\n        # Run hyperparameter sweep\n        print(\"Starting hyperparameter sweep...\")\n        sweep_id = run_sweep(project_name=\"inaturalist_cnn_sweep\")\n        \n        if test==True:\n            # Analyze sweep results\n            best_config = analyze_sweep_results()\n            \n            if best_config==None:\n                # Define best hyperparameters from previous sweep\n                best_config = {\n                    'activation': 'mish',\n                    'batch_norm': False,\n                    'batch_size': 16,\n                    'input_size': 224,\n                    'filter_size': 5,\n                    'num_classes': 10,\n                    'augmentation': False,\n                    'base_filters': 64,\n                    'dropout_rate': 0.5,\n                    'filter_sizes': [5, 5, 5, 5, 5],\n                    'dense_neurons': 512,\n                    'filter_counts': [64, 64, 64, 64, 64],\n                    'learning_rate': 0.0001,\n                    'input_channels': 3,\n                    'filter_counts_strategy': 'same'}\n                \n        \n            # Initialize the data module\n            data_module = iNaturalistDataModule(\n                data_dir='/kaggle/input/inaturalist/inaturalist_12K',\n                batch_size=best_config['batch_size'],\n                augmentation=best_config['augmentation']\n            )\n            \n            # Initialize the model\n            model = CustomCNN(\n                num_classes=10,\n                activation=best_config['activation'],\n                dense_neurons=best_config['dense_neurons'],\n                dropout_rate=best_config['dropout_rate'],\n                learning_rate=best_config['learning_rate'],\n                batch_norm=best_config['batch_norm']\n            )\n            \n            # Initialize the trainer\n            trainer = Trainer(\n                max_epochs=15,\n                accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n                devices=1,\n                log_every_n_steps=10\n            )\n            \n            # Train the model\n            trainer.fit(model, datamodule=data_module)\n\n            # Test the model\n            trainer.test(model, datamodule=data_module)\n            \n            # Train with best hyperparameters\n            print(\"Training with best hyperparameters...\")\n            model = train_final_model(best_config, data_module)\n            \n            # Visualize results\n            visualize_test_samples(model, data_module.test_dataloader())\n            visualize_filters(model)\n            visualize_guided_backprop(model, data_module.test_dataloader())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:25:50.149985Z","iopub.execute_input":"2025-04-16T09:25:50.150218Z","iopub.status.idle":"2025-04-16T09:25:50.171429Z","shell.execute_reply.started":"2025-04-16T09:25:50.150203Z","shell.execute_reply":"2025-04-16T09:25:50.170629Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"main(sweep=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:26:01.011095Z","iopub.execute_input":"2025-04-16T09:26:01.011652Z","iopub.status.idle":"2025-04-16T11:21:11.149178Z","shell.execute_reply.started":"2025-04-16T09:26:01.011630Z","shell.execute_reply":"2025-04-16T11:21:11.147932Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmm21b044\u001b[0m (\u001b[33mmm21b044-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Starting hyperparameter sweep...\nCreate sweep with ID: r2agkc0r\nSweep URL: https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k0m0xvo1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_counts_strategy: same\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_092612-k0m0xvo1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/k0m0xvo1' target=\"_blank\">jumping-sweep-1</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/k0m0xvo1' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/k0m0xvo1</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'activation' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dense_neurons' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_norm' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cd324988684d6d873807824167d5b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>total_computations</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_acc</td><td>▁▂▂▃▃▄▅▆▆▆▆▆▇▇▇▇███▇</td></tr><tr><td>val_loss</td><td>█▇▆▆▅▅▄▄▃▂▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.3145</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>total_computations</td><td>830607616</td></tr><tr><td>total_params</td><td>552330</td></tr><tr><td>train_acc</td><td>0.33804</td></tr><tr><td>train_loss</td><td>1.88488</td></tr><tr><td>trainer/global_step</td><td>4999</td></tr><tr><td>val_acc</td><td>0.3145</td></tr><tr><td>val_loss</td><td>1.91967</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">jumping-sweep-1</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/k0m0xvo1' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/k0m0xvo1</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_092612-k0m0xvo1/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lzo092sp with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 384\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_counts_strategy: halving\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_094657-lzo092sp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/lzo092sp' target=\"_blank\">sweepy-sweep-2</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/lzo092sp' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/lzo092sp</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'activation' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dense_neurons' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_norm' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28e6422871054f2981bd8e0768fa56b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>total_computations</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_computations</td><td>76550262528</td></tr><tr><td>total_params</td><td>7504970</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sweepy-sweep-2</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/lzo092sp' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/lzo092sp</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_094657-lzo092sp/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run lzo092sp errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_151/318994547.py\", line 99, in train_model_sweep\n    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n    call._call_and_handle_interrupt(\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1056, in _run_stage\n    self.fit_loop.run()\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n    self.advance()\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 150, in run\n    self.advance(data_fetcher)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 320, in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 192, in run\n    self._optimizer_step(batch_idx, closure)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 270, in _optimizer_step\n    call._call_lightning_module_hook(\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1302, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/optimizer.py\", line 154, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 239, in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\", line 123, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n    ret = func(self, *args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\", line 202, in step\n    loss = closure()\n           ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\", line 109, in _wrap_closure\n    closure_result = closure()\n                     ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 146, in __call__\n    self._result = self.closure(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 140, in closure\n    self._backward_fn(step_output.closure_loss)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 241, in backward_fn\n    call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 213, in backward\n    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\", line 73, in backward\n    model.backward(tensor, *args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1097, in backward\n    loss.backward(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.63 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.22 GiB is free. Process 4053 has 12.51 GiB memory in use. Of the allocated memory 10.08 GiB is allocated by PyTorch, and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run lzo092sp errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_151/318994547.py\", line 99, in train_model_sweep\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1056, in _run_stage\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.fit_loop.run()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.epoch_loop.run(self._data_fetcher)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 150, in run\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance(data_fetcher)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 320, in advance\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 192, in run\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._optimizer_step(batch_idx, closure)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 270, in _optimizer_step\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1302, in optimizer_step\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     optimizer.step(closure=optimizer_closure)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/optimizer.py\", line 154, in step\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 239, in optimizer_step\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\", line 123, in optimizer_step\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return optimizer.step(closure=closure, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     out = func(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ret = func(self, *args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\", line 202, in step\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = closure()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\", line 109, in _wrap_closure\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     closure_result = closure()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                      ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 146, in __call__\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._result = self.closure(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 140, in closure\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._backward_fn(step_output.closure_loss)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\", line 241, in backward_fn\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 213, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\", line 73, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model.backward(tensor, *args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\", line 1097, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss.backward(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.autograd.backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     _engine_run_backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.63 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.22 GiB is free. Process 4053 has 12.51 GiB memory in use. Of the allocated memory 10.08 GiB is allocated by PyTorch, and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fauxytqo with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_counts_strategy: halving\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_094716-fauxytqo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/fauxytqo' target=\"_blank\">fluent-sweep-3</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/fauxytqo' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/fauxytqo</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'activation' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dense_neurons' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_norm' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae730fc88ee64e0d9adf8abbff0d5892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e3817c5ce2c484695d59a75fe75e3aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██</td></tr><tr><td>total_computations</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██</td></tr><tr><td>val_acc</td><td>▁▄▄▅▅▄█▇▆██▇▇▇▇</td></tr><tr><td>val_loss</td><td>█▅▅▄▃▄▁▃▄▂▂▄▅▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.352</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>total_computations</td><td>14147288064</td></tr><tr><td>total_params</td><td>1311226</td></tr><tr><td>train_acc</td><td>0.6312</td></tr><tr><td>train_loss</td><td>1.06799</td></tr><tr><td>trainer/global_step</td><td>7499</td></tr><tr><td>val_acc</td><td>0.352</td></tr><tr><td>val_loss</td><td>2.06128</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fluent-sweep-3</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/fauxytqo' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/fauxytqo</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_094716-fauxytqo/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7vf28nbg with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_counts_strategy: same\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_102003-7vf28nbg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/7vf28nbg' target=\"_blank\">toasty-sweep-4</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/7vf28nbg' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/7vf28nbg</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'activation' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dense_neurons' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_norm' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caa39458f1d14cff87bb8550612cdef3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>total_computations</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_acc</td><td>▁▄▅▄▄▆▆▆▇▆▇▇████▇██▇</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▂▂▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.371</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>total_computations</td><td>198121728</td></tr><tr><td>total_params</td><td>433930</td></tr><tr><td>train_acc</td><td>0.53919</td></tr><tr><td>train_loss</td><td>1.31996</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr><tr><td>val_acc</td><td>0.371</td></tr><tr><td>val_loss</td><td>1.84589</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">toasty-sweep-4</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/7vf28nbg' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/7vf28nbg</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_102003-7vf28nbg/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d175wcos with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_counts_strategy: same\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_103700-d175wcos</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/d175wcos' target=\"_blank\">royal-sweep-5</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/d175wcos' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/d175wcos</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'activation' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dense_neurons' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_norm' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cedcfbb1c8e44dbcb09eb049c6e9dcd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>total_computations</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▃▃▃▄▄▄▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▆▆▆▅▅▅▄▄▃▃▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_acc</td><td>▁▃▃▃▆▅▃▆▅▆▇▇▅▇▆█▇▆▆▇</td></tr><tr><td>val_loss</td><td>▄▄▃▃▂▂▂▁▂▂▂▃▄▅▅▆▆███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.3525</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>total_computations</td><td>648782336</td></tr><tr><td>total_params</td><td>913418</td></tr><tr><td>train_acc</td><td>0.81035</td></tr><tr><td>train_loss</td><td>0.57589</td></tr><tr><td>trainer/global_step</td><td>4999</td></tr><tr><td>val_acc</td><td>0.3525</td></tr><tr><td>val_loss</td><td>2.26583</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">royal-sweep-5</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/d175wcos' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/d175wcos</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_103700-d175wcos/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5i20ch3h with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_counts_strategy: doubling\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_105416-5i20ch3h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/5i20ch3h' target=\"_blank\">sparkling-sweep-6</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/5i20ch3h' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/5i20ch3h</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'activation' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dense_neurons' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_norm' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9434b27fca04be89d172ca1df0afebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>total_computations</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▆▆▆▆▅▅▅▄▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_acc</td><td>▁▁▁▂▂▂▃▄▄▃▅▅▇▆▆▆▆▇▅█</td></tr><tr><td>val_loss</td><td>█▇▇▇▇▆▆▅▅▅▃▄▂▂▃▃▂▁▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.342</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>total_computations</td><td>827541248</td></tr><tr><td>total_params</td><td>7517866</td></tr><tr><td>train_acc</td><td>0.38642</td></tr><tr><td>train_loss</td><td>1.70932</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr><tr><td>val_acc</td><td>0.342</td></tr><tr><td>val_loss</td><td>1.91298</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sparkling-sweep-6</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/5i20ch3h' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/5i20ch3h</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_105416-5i20ch3h/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: guh8v1v3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_counts_strategy: doubling\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_111138-guh8v1v3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/guh8v1v3' target=\"_blank\">toasty-sweep-7</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/r2agkc0r</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/guh8v1v3' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/guh8v1v3</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'activation' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dense_neurons' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_norm' was locked by 'sweep' (ignored update).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0751f1e0c51643e58f16d50dac8a606e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██</td></tr><tr><td>total_computations</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▆▄▇▇▇█</td></tr><tr><td>train_loss</td><td>█▂▁▂▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██</td></tr><tr><td>val_acc</td><td>▅▄▄█▇▇▁▅▄</td></tr><tr><td>val_loss</td><td>█▂▅▂▄▃▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.171</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>total_computations</td><td>3154418176</td></tr><tr><td>total_params</td><td>17206090</td></tr><tr><td>train_acc</td><td>0.16627</td></tr><tr><td>train_loss</td><td>2.22374</td></tr><tr><td>trainer/global_step</td><td>2249</td></tr><tr><td>val_acc</td><td>0.171</td></tr><tr><td>val_loss</td><td>2.20611</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">toasty-sweep-7</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/guh8v1v3' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/guh8v1v3</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_111138-guh8v1v3/logs</code>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/apis/normalize.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/apis/public/api.py\u001b[0m in \u001b[0;36msweep\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sweeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sweeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpublic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msweep_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sweeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/apis/public/sweeps.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client, entity, project, sweep_id, attrs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/apis/public/sweeps.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msweep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not find sweep {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msweep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Could not find sweep <Sweep mm21b044/inaturalist_cnn_sweep/r2agkc0r (Unknown State)>","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_151/1719333401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_151/1529765696.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(sweep)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msweep_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inaturalist_cnn_sweep\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Analyze sweep results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0manalyze_sweep_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Define best hyperparameters from previous sweep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_151/318994547.py\u001b[0m in \u001b[0;36manalyze_sweep_results\u001b[0;34m(sweep_id)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m\"\"\"Inspect sweep runs and print insights\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msweep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"mm21b044/inaturalist_cnn_sweep/{sweep_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mruns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msweep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/apis/normalize.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mCommError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/apis/normalize.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Whoa, you found a bug.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/apis/public/api.py\u001b[0m in \u001b[0;36msweep\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msweep_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sweeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sweeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpublic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msweep_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sweeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/apis/public/sweeps.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client, entity, project, sweep_id, attrs)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/apis/public/sweeps.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0msweep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msweep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not find sweep {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msweep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msweep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCommError\u001b[0m: Could not find sweep <Sweep mm21b044/inaturalist_cnn_sweep/r2agkc0r (Unknown State)>"],"ename":"CommError","evalue":"Could not find sweep <Sweep mm21b044/inaturalist_cnn_sweep/r2agkc0r (Unknown State)>","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"wandb.login(key=\"e030007b097df00d9a751748294abc8440f932b1\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.init()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_config = analyze_sweep_results(\"mm21b044-indian-institute-of-technology-madras\", \"inaturalist_cnn_sweep\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the data module\ndata_module = iNaturalistDataModule(\n    data_dir='/kaggle/input/inaturalist/inaturalist_12K',\n    batch_size=best_config['batch_size'],\n    augmentation=best_config['augmentation']\n)\n\n# Initialize the model\nmodel = CustomCNN(\n    num_classes=10,\n    activation=best_config['activation'],\n    dense_neurons=best_config['dense_neurons'],\n    dropout_rate=best_config['dropout_rate'],\n    learning_rate=best_config['learning_rate'],\n    batch_norm=best_config['batch_norm']\n)\n\n# Initialize the trainer\ntrainer = Trainer(\n    max_epochs=15,\n    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n    devices=1,\n    log_every_n_steps=10\n)\n\n# Train the model\ntrainer.fit(model, datamodule=data_module)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_lightning.callbacks import Callback\n\nclass ExceptionCallback(Callback):\n    def on_exception(self, trainer, pl_module, exception):\n        print(f\"An exception occurred: {exception}\")\n        # Add any additional exception handling logic here\n\n# When initializing the Trainer, include the callback\nfrom pytorch_lightning import Trainer\n\ntrainer = Trainer(callbacks=[ExceptionCallback()])\n\n# Test the model\ntrainer.test(model, datamodule=data_module)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize results\nvisualize_test_samples(model, data_module.test_dataloader())\nvisualize_filters(model)\nvisualize_guided_backprop(model, data_module.test_dataloader())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We might not need this function at all\n# Train with best hyperparameters\nprint(\"Training with best hyperparameters...\")\nmodel = train_final_model(best_config, data_module)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}