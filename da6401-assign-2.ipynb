{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:34:00.974401Z",
     "iopub.status.busy": "2025-04-12T00:34:00.973872Z",
     "iopub.status.idle": "2025-04-12T00:35:11.973669Z",
     "shell.execute_reply": "2025-04-12T00:35:11.972774Z",
     "shell.execute_reply.started": "2025-04-12T00:34:00.974382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
      "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.5.1+cu124)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.16)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.19.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install wandb pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:20:16.229716Z",
     "iopub.status.busy": "2025-04-12T04:20:16.229488Z",
     "iopub.status.idle": "2025-04-12T04:20:32.431860Z",
     "shell.execute_reply": "2025-04-12T04:20:32.431099Z",
     "shell.execute_reply.started": "2025-04-12T04:20:16.229693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:20:32.433246Z",
     "iopub.status.busy": "2025-04-12T04:20:32.433033Z",
     "iopub.status.idle": "2025-04-12T04:20:32.451566Z",
     "shell.execute_reply": "2025-04-12T04:20:32.450750Z",
     "shell.execute_reply.started": "2025-04-12T04:20:32.433228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomCNN(LightningModule):\n",
    "    def __init__(self, \n",
    "                 num_classes=10,\n",
    "                 filter_counts=[32, 32, 64, 64, 128],\n",
    "                 filter_sizes=[3, 3, 3, 3, 3],\n",
    "                 activation='relu',\n",
    "                 dense_neurons=512,\n",
    "                 input_channels=3,\n",
    "                 input_size=244,\n",
    "                 dropout_rate=0.5,\n",
    "                 learning_rate=0.001,\n",
    "                 batch_norm=False):\n",
    "        \"\"\"\n",
    "        Custom CNN architecture with flexible hyperparameters\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Number of output classes\n",
    "            filter_counts (list): Number of filters in each conv layer\n",
    "            filter_sizes (list): Size of filters in each conv layer\n",
    "            activation (str): Activation function ('relu', 'gelu', 'silu', 'mish')\n",
    "            dense_neurons (int): Number of neurons in the dense layer\n",
    "            input_channels (int): Number of input channels (3 for RGB)\n",
    "            input_size (int): Size of input images (assumes square)\n",
    "            dropout_rate (float): Dropout rate\n",
    "            learning_rate (float): Learning rate for optimizer\n",
    "            batch_norm (bool): Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Configure activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'silu':\n",
    "            self.activation = nn.SiLU()\n",
    "        elif activation == 'mish':\n",
    "            self.activation = nn.Mish()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        # Build the network\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        # Calculate feature map sizes for computational analysis\n",
    "        feature_size = input_size\n",
    "        feature_sizes = [feature_size]\n",
    "        \n",
    "        # First convolutional block\n",
    "        in_channels = input_channels\n",
    "        for i in range(5):\n",
    "            out_channels = filter_counts[i]\n",
    "            filter_size = filter_sizes[i]\n",
    "            \n",
    "            # Create convolutional block\n",
    "            conv_block = []\n",
    "            \n",
    "            # Convolutional layer\n",
    "            conv_block.append(nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=filter_size//2))\n",
    "            \n",
    "            # Batch normalization (optional)\n",
    "            if batch_norm:\n",
    "                conv_block.append(nn.BatchNorm2d(out_channels))\n",
    "            \n",
    "            # Activation\n",
    "            conv_block.append(self.activation)\n",
    "            \n",
    "            # Max pooling\n",
    "            conv_block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            \n",
    "            # Add block to model\n",
    "            self.conv_layers.append(nn.Sequential(*conv_block))\n",
    "            \n",
    "            # Update feature size (divided by 2 due to max pooling)\n",
    "            feature_size = feature_size // 2\n",
    "            feature_sizes.append(feature_size)\n",
    "            \n",
    "            # Update channels for next layer\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Calculate flattened features size\n",
    "        self.flattened_size = filter_counts[-1] * feature_size * feature_size\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.flattened_size, dense_neurons),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(dense_neurons, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Store additional parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.filter_counts = filter_counts\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.feature_sizes = feature_sizes\n",
    "        \n",
    "        # Calculate parameters and computations\n",
    "        self.total_params = self.calculate_total_params()\n",
    "        self.total_computations = self.calculate_total_computations()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Pass through convolutional layers\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        \n",
    "        # Pass through classifier\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def calculate_total_params(self):\n",
    "        \"\"\"Calculate the total number of parameters in the network\"\"\"\n",
    "        total = 0\n",
    "        \n",
    "        # Convolutional layers parameters\n",
    "        input_channels = 3\n",
    "        for i in range(5):\n",
    "            output_channels = self.filter_counts[i]\n",
    "            filter_size = self.filter_sizes[i]\n",
    "            \n",
    "            # Weight parameters: out_channels * in_channels * filter_height * filter_width\n",
    "            params = output_channels * input_channels * filter_size * filter_size\n",
    "            # Bias parameters: out_channels\n",
    "            params += output_channels\n",
    "            \n",
    "            total += params\n",
    "            input_channels = output_channels\n",
    "        \n",
    "        # Dense layer parameters\n",
    "        # First dense layer: flattened_size * dense_neurons + dense_neurons (bias)\n",
    "        total += self.flattened_size * self.hparams.dense_neurons + self.hparams.dense_neurons\n",
    "        # Output layer: dense_neurons * num_classes + num_classes (bias)\n",
    "        total += self.hparams.dense_neurons * self.num_classes + self.num_classes\n",
    "        \n",
    "        return total\n",
    "    \n",
    "    def calculate_total_computations(self):\n",
    "        \"\"\"Calculate the total number of computations in the network\"\"\"\n",
    "        total = 0\n",
    "        \n",
    "        # Convolutional layers computations\n",
    "        input_channels = 3\n",
    "        for i in range(5):\n",
    "            output_channels = self.filter_counts[i]\n",
    "            filter_size = self.filter_sizes[i]\n",
    "            feature_size = self.feature_sizes[i]\n",
    "            \n",
    "            # Convolution computations: \n",
    "            # out_channels * in_channels * filter_height * filter_width * feature_height * feature_width\n",
    "            comp = output_channels * input_channels * filter_size * filter_size * feature_size * feature_size\n",
    "            \n",
    "            total += comp\n",
    "            input_channels = output_channels\n",
    "        \n",
    "        # Dense layer computations\n",
    "        # First dense layer: flattened_size * dense_neurons\n",
    "        total += self.flattened_size * self.hparams.dense_neurons\n",
    "        # Output layer: dense_neurons * num_classes\n",
    "        total += self.hparams.dense_neurons * self.num_classes\n",
    "        \n",
    "        return total\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizer\"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Test step\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return {'loss': loss, 'preds': preds, 'targets': y}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        \"\"\"Gather predictions after test epoch\"\"\"\n",
    "        all_preds = torch.cat([x['preds'] for x in outputs])\n",
    "        all_targets = torch.cat([x['targets'] for x in outputs])\n",
    "        \n",
    "        # Calculate confusion matrix and class accuracies\n",
    "        conf_matrix = torch.zeros(self.num_classes, self.num_classes)\n",
    "        for t, p in zip(all_targets, all_preds):\n",
    "            conf_matrix[t.long(), p.long()] += 1\n",
    "            \n",
    "        # Log confusion matrix\n",
    "        class_acc = conf_matrix.diag() / conf_matrix.sum(1)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            preds=all_preds.cpu().numpy(),\n",
    "            y_true=all_targets.cpu().numpy(),\n",
    "            class_names=[str(i) for i in range(self.num_classes)])})\n",
    "        \n",
    "        return {'test_acc': (all_preds == all_targets).float().mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:22:44.274278Z",
     "iopub.status.busy": "2025-04-12T04:22:44.273986Z",
     "iopub.status.idle": "2025-04-12T04:22:44.285415Z",
     "shell.execute_reply": "2025-04-12T04:22:44.284642Z",
     "shell.execute_reply.started": "2025-04-12T04:22:44.274256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class iNaturalistDataModule(LightningModule):\n",
    "    def __init__(self, data_dir='/kaggle/input/inaturalist/inaturalist_12K', batch_size=32, num_workers=4, \n",
    "                 input_size=244, val_split=0.2, augmentation=False):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.input_size = input_size\n",
    "        self.val_split = val_split\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Setup data transformations and load datasets\"\"\"\n",
    "        # Define transformations\n",
    "        if self.augmentation:\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(self.input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.Resize((self.input_size, self.input_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((self.input_size, self.input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Load datasets\n",
    "        train_dir = os.path.join(self.data_dir, 'train')\n",
    "        test_dir = os.path.join(self.data_dir, 'val')\n",
    "        \n",
    "        self.train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
    "        self.test_dataset = ImageFolder(root=test_dir, transform=val_transform)\n",
    "        \n",
    "        # Split train set into train and validation\n",
    "        dataset_size = len(self.train_dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Create stratified split\n",
    "        class_indices = defaultdict(list)\n",
    "        for idx, (_, label) in enumerate(self.train_dataset.samples):\n",
    "            class_indices[label].append(idx)\n",
    "        \n",
    "        train_indices = []\n",
    "        val_indices = []\n",
    "        \n",
    "        for class_idx, indices in class_indices.items():\n",
    "            np.random.shuffle(indices)\n",
    "            split_idx = int(len(indices) * (1 - self.val_split))\n",
    "            train_indices.extend(indices[:split_idx])\n",
    "            val_indices.extend(indices[split_idx:])\n",
    "        \n",
    "        # Create samplers for train and validation sets\n",
    "        self.train_sampler = SubsetRandomSampler(train_indices)\n",
    "        self.val_sampler = SubsetRandomSampler(val_indices)\n",
    "        \n",
    "        # # Create a validation dataset with the same transforms as test\n",
    "        # self.val_dataset = ImageFolder(root=train_dir, transform=val_transform)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Return train dataloader\"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.train_sampler,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Return validation dataloader\"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset,  # Use the original train dataset\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.val_sampler,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Return test dataloader\"\"\"\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:34:39.800221Z",
     "iopub.status.busy": "2025-04-12T04:34:39.799695Z",
     "iopub.status.idle": "2025-04-12T04:34:39.814577Z",
     "shell.execute_reply": "2025-04-12T04:34:39.813639Z",
     "shell.execute_reply.started": "2025-04-12T04:34:39.800190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_wandb_sweep():\n",
    "    \"\"\"Define sweep configuration for hyperparameter tuning\"\"\"\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',  # Bayesian optimization\n",
    "        'metric': {\n",
    "            'name': 'val_acc',\n",
    "            'goal': 'maximize'\n",
    "        },\n",
    "        'parameters': {\n",
    "            'filter_counts_strategy': {\n",
    "                'values': ['same', 'doubling', 'halving']  # Different filter count strategies\n",
    "            },\n",
    "            'base_filters': {\n",
    "                'values': [16, 32, 64]  # Base number of filters\n",
    "            },\n",
    "            'filter_size': {\n",
    "                'values': [3, 5]  # Filter sizes\n",
    "            },\n",
    "            'activation': {\n",
    "                'values': ['relu', 'gelu', 'silu', 'mish']  # Different activation functions\n",
    "            },\n",
    "            'dense_neurons': {\n",
    "                'values': [128, 256, 384, 512]  # Number of neurons in dense layer\n",
    "            },\n",
    "            'dropout_rate': {\n",
    "                'values': [0.2, 0.3, 0.5]  # Dropout rate\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'values': [0.0001, 0.001]  # Learning rate\n",
    "            },\n",
    "            'batch_norm': {\n",
    "                'values': [True, False]  # Whether to use batch normalization\n",
    "            },\n",
    "            'batch_size': {\n",
    "                'values': [16, 32]  # Batch size\n",
    "            },\n",
    "            'augmentation': {\n",
    "                'values': [True, False]  # Whether to use data augmentation\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return sweep_config\n",
    "\n",
    "def train_model_sweep():\n",
    "    \"\"\"Training function for sweep\"\"\"\n",
    "    # Initialize wandb\n",
    "    wandb.init()\n",
    "    \n",
    "    # Get hyperparameters from wandb\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Generate filter counts based on strategy\n",
    "    if config.filter_counts_strategy == 'same':\n",
    "        filter_counts = [config.base_filters] * 5\n",
    "    elif config.filter_counts_strategy == 'doubling':\n",
    "        filter_counts = [config.base_filters * (2**i) for i in range(5)]\n",
    "    elif config.filter_counts_strategy == 'halving':\n",
    "        filter_counts = [config.base_filters * (2**(4-i)) for i in range(5)]\n",
    "    \n",
    "    # Generate filter sizes\n",
    "    filter_sizes = [config.filter_size] * 5\n",
    "    \n",
    "    # Create data module\n",
    "    data_module = iNaturalistDataModule(\n",
    "        batch_size=config.batch_size,\n",
    "        augmentation=config.augmentation\n",
    "    )\n",
    "    data_module.setup()\n",
    "    \n",
    "    # Create model with hyperparameters\n",
    "    model = CustomCNN(\n",
    "        num_classes=10,  # Assuming 10 classes in iNaturalist subset\n",
    "        filter_counts=filter_counts,\n",
    "        filter_sizes=filter_sizes,\n",
    "        activation=config.activation,\n",
    "        dense_neurons=config.dense_neurons,\n",
    "        dropout_rate=config.dropout_rate,\n",
    "        learning_rate=config.learning_rate,\n",
    "        batch_norm=config.batch_norm\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            monitor='val_acc',\n",
    "            filename='best-{epoch:02d}-{val_acc:.4f}',\n",
    "            save_top_k=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_acc',\n",
    "            patience=5,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Setup wandb logger\n",
    "    wandb_logger = WandbLogger(project=\"inaturalist_cnn_sweep\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=15,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "        callbacks=callbacks,\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
    "    \n",
    "    # Get best validation accuracy\n",
    "    best_val_acc = trainer.callback_metrics.get('val_acc', 0)\n",
    "    \n",
    "    # Log metrics\n",
    "    wandb.log({\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'total_params': model.total_params,\n",
    "        'total_computations': model.total_computations\n",
    "    })\n",
    "    \n",
    "    return model, best_val_acc\n",
    "\n",
    "def run_sweep():\n",
    "    \"\"\"Run the sweep\"\"\"\n",
    "    # Initialize wandb\n",
    "    wandb.login(key=\"e030007b097df00d9a751748294abc8440f932b1\")\n",
    "    \n",
    "    # Setup sweep\n",
    "    sweep_config = setup_wandb_sweep()\n",
    "    \n",
    "    # Create sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"inaturalist_cnn_sweep\")\n",
    "    \n",
    "    # Run sweep\n",
    "    wandb.agent(sweep_id, function=train_model_sweep, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:34:45.208170Z",
     "iopub.status.busy": "2025-04-12T04:34:45.207925Z",
     "iopub.status.idle": "2025-04-12T04:34:45.222162Z",
     "shell.execute_reply": "2025-04-12T04:34:45.221412Z",
     "shell.execute_reply.started": "2025-04-12T04:34:45.208154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_final_model(config):\n",
    "    \"\"\"Train final model with best hyperparameters\"\"\"\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"inaturalist_cnn_final\", config=config)\n",
    "    \n",
    "    # Generate filter counts based on strategy\n",
    "    if config['filter_counts_strategy'] == 'same':\n",
    "        filter_counts = [config['base_filters']] * 5\n",
    "    elif config['filter_counts_strategy'] == 'doubling':\n",
    "        filter_counts = [config['base_filters'] * (2**i) for i in range(5)]\n",
    "    elif config['filter_counts_strategy'] == 'halving':\n",
    "        filter_counts = [config['base_filters'] * (2**(4-i)) for i in range(5)]\n",
    "    \n",
    "    # Generate filter sizes\n",
    "    filter_sizes = [config['filter_size']] * 5\n",
    "    \n",
    "    # Create data module\n",
    "    data_module = iNaturalistDataModule(\n",
    "        batch_size=config['batch_size'],\n",
    "        augmentation=config['augmentation']\n",
    "    )\n",
    "    data_module.setup()\n",
    "    \n",
    "    # Create model with hyperparameters\n",
    "    model = CustomCNN(\n",
    "        num_classes=10,\n",
    "        filter_counts=filter_counts,\n",
    "        filter_sizes=filter_sizes,\n",
    "        activation=config['activation'],\n",
    "        dense_neurons=config['dense_neurons'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        batch_norm=config['batch_norm']\n",
    "    )\n",
    "    \n",
    "    # Log model summary\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    \n",
    "    # Setup callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            monitor='val_acc',\n",
    "            filename='best-{epoch:02d}-{val_acc:.4f}',\n",
    "            save_top_k=1,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Setup wandb logger\n",
    "    wandb_logger = WandbLogger(project=\"inaturalist_cnn_final\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=15,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "        callbacks=callbacks,\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
    "    \n",
    "    # Test model\n",
    "    test_results = trainer.test(model, dataloader=data_module.test_dataloader())\n",
    "    \n",
    "    # Log test results\n",
    "    wandb.log({\n",
    "        'test_acc': test_results[0]['test_acc'],\n",
    "        'test_loss': test_results[0]['test_loss']\n",
    "    })\n",
    "    \n",
    "    # Log model architecture\n",
    "    wandb.log({\n",
    "        'total_params': model.total_params,\n",
    "        'total_computations': model.total_computations\n",
    "    })\n",
    "    \n",
    "    return model, test_results\n",
    "\n",
    "def visualize_test_samples(model, data_module, num_samples=30):\n",
    "    \"\"\"Visualize test samples with predictions\"\"\"\n",
    "    # Get test dataloader\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    images, labels = next(iter(test_loader))\n",
    "    \n",
    "    # Move to device\n",
    "    model = model.to(device)\n",
    "    images = images.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Convert to numpy for visualization\n",
    "    images = images.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = data_module.test_dataset.classes\n",
    "    \n",
    "    # Visualize images with predictions\n",
    "    fig, axes = plt.subplots(10, 3, figsize=(15, 30))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_samples:\n",
    "            # Transpose image from (C, H, W) to (H, W, C)\n",
    "            img = np.transpose(images[i], (1, 2, 0))\n",
    "            \n",
    "            # Denormalize\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img = std * img + mean\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            # Plot image\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            # Get true and predicted labels\n",
    "            true_label = class_names[labels[i]]\n",
    "            pred_label = class_names[predicted[i]]\n",
    "            \n",
    "            # Set title\n",
    "            if labels[i] == predicted[i]:\n",
    "                ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color='green')\n",
    "            else:\n",
    "                ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color='red')\n",
    "            \n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\"test_predictions\": wandb.Image(plt)})\n",
    "\n",
    "def visualize_filters(model):\n",
    "    \"\"\"Visualize filters in the first convolutional layer\"\"\"\n",
    "    # Get first layer filters\n",
    "    filters = model.conv_layers[0][0].weight.data.cpu().numpy()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "    \n",
    "    # Plot filters\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < filters.shape[0]:\n",
    "            # Normalize filter for visualization\n",
    "            f = filters[i].transpose(1, 2, 0)\n",
    "            f = (f - f.min()) / (f.max() - f.min())\n",
    "            \n",
    "            # Plot filter\n",
    "            ax.imshow(f)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\"first_layer_filters\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:34:56.425943Z",
     "iopub.status.busy": "2025-04-12T04:34:56.425625Z",
     "iopub.status.idle": "2025-04-12T04:34:56.437314Z",
     "shell.execute_reply": "2025-04-12T04:34:56.436604Z",
     "shell.execute_reply.started": "2025-04-12T04:34:56.425921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "def main(part='both', sweep=False, train=False, test=False, data_dir='/kaggle/input/inaturalist/inaturalist_12K', output_dir='/kaggle/working/'):\n",
    "    \"\"\"Main function to run the assignment\"\"\"\n",
    "    print(\"Hello from main()\")\n",
    "    print(\"Starting main function\")\n",
    "\n",
    "    # Create output directory\n",
    "    print(f\"Creating output directory: {output_dir}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Output directory created successfully\")\n",
    "\n",
    "    # Initialize wandb\n",
    "    print(\"Attempting to log in to Wandb\")\n",
    "    wandb.login(key=\"e030007b097df00d9a751748294abc8440f932b1\")\n",
    "    print(\"Successfully logged in to Wandb\")\n",
    "\n",
    "    if part in ['a', 'both']:\n",
    "        print(\"Running Part A: Training from Scratch\")\n",
    "\n",
    "        # Run hyperparameter sweep\n",
    "        if sweep:\n",
    "            print(\"Running hyperparameter sweep...\")\n",
    "            run_sweep()\n",
    "            print(\"Hyperparameter sweep completed\")\n",
    "\n",
    "        # Train final model with best hyperparameters\n",
    "        if train:\n",
    "            print(\"Training final model...\")\n",
    "\n",
    "            # Best hyperparameters from sweep\n",
    "            best_config = {\n",
    "                'filter_counts_strategy': 'doubling',\n",
    "                'base_filters': 32,\n",
    "                'filter_size': 3,\n",
    "                'activation': 'relu',\n",
    "                'dense_neurons': 512,\n",
    "                'dropout_rate': 0.3,\n",
    "                'learning_rate': 0.001,\n",
    "                'batch_norm': True,\n",
    "                'batch_size': 32,\n",
    "                'augmentation': True\n",
    "            }\n",
    "            print(f\"Using best configuration for Part A training: {best_config}\")\n",
    "\n",
    "            # Train model\n",
    "            print(\"Starting training of the final model for Part A\")\n",
    "            model_a, test_results_a = train_final_model(best_config)\n",
    "            print(\"Finished training of the final model for Part A\")\n",
    "            print(f\"Test results for Part A: {test_results_a}\")\n",
    "\n",
    "            # Test and visualize\n",
    "            if test:\n",
    "                print(\"Starting testing and visualization for Part A\")\n",
    "                data_module = iNaturalistDataModule(\n",
    "                    data_dir=data_dir,\n",
    "                    batch_size=best_config['batch_size'],\n",
    "                    augmentation=best_config['augmentation']\n",
    "                )\n",
    "                data_module.setup()\n",
    "                print(\"Data module setup for Part A testing\")\n",
    "\n",
    "                visualize_test_samples(model_a, data_module)\n",
    "                print(\"Test samples visualized for Part A\")\n",
    "                visualize_filters(model_a)\n",
    "                print(\"Filters visualized for Part A\")\n",
    "                print(\"Finished testing and visualization for Part A\")\n",
    "\n",
    "    print(\"Finished main function\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description='DA6401 Assignment 2 - CNN Training')\n",
    "#     parser.add_argument('--part', type=str, default='both', choices=['a', 'b', 'both'],\n",
    "#                         help='Which part of the assignment to run (a, b, or both)')\n",
    "#     parser.add_argument('--sweep', action='store_true', help='Run hyperparameter sweep')\n",
    "#     parser.add_argument('--train', action='store_true', help='Train final model')\n",
    "#     parser.add_argument('--test', action='store_true', help='Test model')\n",
    "#     parser.add_argument('--data_dir', type=str, default='./inaturalist_data',\n",
    "#                         help='Directory containing the dataset')\n",
    "#     parser.add_argument('--output_dir', type=str, default='./output',\n",
    "#                         help='Directory to save output')\n",
    "#     args = parser.parse_args()\n",
    "#     main(args.part, args.sweep, args.train, args.test, args.data_dir, args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T04:34:57.054251Z",
     "iopub.status.busy": "2025-04-12T04:34:57.053590Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from main()\n",
      "Starting main function\n",
      "Creating output directory: /kaggle/working/\n",
      "Output directory created successfully\n",
      "Attempting to log in to Wandb\n",
      "Successfully logged in to Wandb\n",
      "Running Part A: Training from Scratch\n",
      "Running hyperparameter sweep...\n",
      "Create sweep with ID: 8m3xeikm\n",
      "Sweep URL: https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/8m3xeikm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 18xodfhp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_counts_strategy: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250412_043503-18xodfhp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/18xodfhp' target=\"_blank\">eager-sweep-1</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/8m3xeikm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/8m3xeikm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/8m3xeikm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/sweeps/8m3xeikm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/18xodfhp' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/18xodfhp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'activation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dense_neurons' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_norm' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375feaec4dd24053a51e1c3a731c63e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>total_computations</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▇▆▆▆▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>val_acc</td><td>▁▃▄▅▆▅▅▇▄███▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>█▆▆▅▄▃▄▃▆▁▁▁▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.3755</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>total_computations</td><td>36201728</td></tr><tr><td>total_params</td><td>76170</td></tr><tr><td>train_acc</td><td>0.52344</td></tr><tr><td>train_loss</td><td>1.35995</td></tr><tr><td>trainer/global_step</td><td>8499</td></tr><tr><td>val_acc</td><td>0.3755</td></tr><tr><td>val_loss</td><td>1.82469</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-1</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/18xodfhp' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep/runs/18xodfhp</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/inaturalist_cnn_sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250412_043503-18xodfhp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dv7slyrk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_counts_strategy: halving\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "main(part='a', sweep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "main(part='a', train=True, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "main(part='b', train=True, test=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7118869,
     "sourceId": 11371580,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7118997,
     "sourceId": 11371756,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
