{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-04-11T09:47:49.830946Z","iopub.execute_input":"2025-04-11T09:47:49.831424Z","iopub.status.idle":"2025-04-11T09:47:49.836723Z","shell.execute_reply.started":"2025-04-11T09:47:49.831388Z","shell.execute_reply":"2025-04-11T09:47:49.835653Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Part A","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install wandb pytorch-lightning\n\n# Import necessary libraries\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport math\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seed()\n\n# Configure device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:47:50.696775Z","iopub.execute_input":"2025-04-11T09:47:50.697157Z","iopub.status.idle":"2025-04-11T09:48:09.848096Z","shell.execute_reply.started":"2025-04-11T09:47:50.697125Z","shell.execute_reply":"2025-04-11T09:48:09.846986Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.5.0.post0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.1+cu121)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.67.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.9.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.6.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.9)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.10)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nUsing device: cpu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class CustomCNN(LightningModule):\n    def __init__(self, \n                 num_classes=10,\n                 filter_counts=[32, 32, 64, 64, 128],\n                 filter_sizes=[3, 3, 3, 3, 3],\n                 activation='relu',\n                 dense_neurons=512,\n                 input_channels=3,\n                 input_size=224,\n                 dropout_rate=0.5,\n                 learning_rate=0.001,\n                 batch_norm=False):\n        \"\"\"\n        Custom CNN architecture with flexible hyperparameters\n        \n        Args:\n            num_classes (int): Number of output classes\n            filter_counts (list): Number of filters in each conv layer\n            filter_sizes (list): Size of filters in each conv layer\n            activation (str): Activation function ('relu', 'gelu', 'silu', 'mish')\n            dense_neurons (int): Number of neurons in the dense layer\n            input_channels (int): Number of input channels (3 for RGB)\n            input_size (int): Size of input images (assumes square)\n            dropout_rate (float): Dropout rate\n            learning_rate (float): Learning rate for optimizer\n            batch_norm (bool): Whether to use batch normalization\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Configure activation function\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'gelu':\n            self.activation = nn.GELU()\n        elif activation == 'silu':\n            self.activation = nn.SiLU()\n        elif activation == 'mish':\n            self.activation = nn.Mish()\n        else:\n            self.activation = nn.ReLU()\n        \n        # Build the network\n        self.conv_layers = nn.ModuleList()\n        \n        # Calculate feature map sizes for computational analysis\n        feature_size = input_size\n        feature_sizes = [feature_size]\n        \n        # First convolutional block\n        in_channels = input_channels\n        for i in range(5):\n            out_channels = filter_counts[i]\n            filter_size = filter_sizes[i]\n            \n            # Create convolutional block\n            conv_block = []\n            \n            # Convolutional layer\n            conv_block.append(nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=filter_size//2))\n            \n            # Batch normalization (optional)\n            if batch_norm:\n                conv_block.append(nn.BatchNorm2d(out_channels))\n            \n            # Activation\n            conv_block.append(self.activation)\n            \n            # Max pooling\n            conv_block.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            \n            # Add block to model\n            self.conv_layers.append(nn.Sequential(*conv_block))\n            \n            # Update feature size (divided by 2 due to max pooling)\n            feature_size = feature_size // 2\n            feature_sizes.append(feature_size)\n            \n            # Update channels for next layer\n            in_channels = out_channels\n        \n        # Calculate flattened features size\n        self.flattened_size = filter_counts[-1] * feature_size * feature_size\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(self.flattened_size, dense_neurons),\n            self.activation,\n            nn.Dropout(dropout_rate),\n            nn.Linear(dense_neurons, num_classes)\n        )\n        \n        # Store additional parameters\n        self.learning_rate = learning_rate\n        self.num_classes = num_classes\n        self.filter_counts = filter_counts\n        self.filter_sizes = filter_sizes\n        self.feature_sizes = feature_sizes\n        \n        # Calculate parameters and computations\n        self.total_params = self.calculate_total_params()\n        self.total_computations = self.calculate_total_computations()\n        \n    def forward(self, x):\n        \"\"\"Forward pass through the network\"\"\"\n        # Pass through convolutional layers\n        for conv_layer in self.conv_layers:\n            x = conv_layer(x)\n        \n        # Pass through classifier\n        return self.classifier(x)\n    \n    def calculate_total_params(self):\n        \"\"\"Calculate the total number of parameters in the network\"\"\"\n        total = 0\n        \n        # Convolutional layers parameters\n        input_channels = 3\n        for i in range(5):\n            output_channels = self.filter_counts[i]\n            filter_size = self.filter_sizes[i]\n            \n            # Weight parameters: out_channels * in_channels * filter_height * filter_width\n            params = output_channels * input_channels * filter_size * filter_size\n            # Bias parameters: out_channels\n            params += output_channels\n            \n            total += params\n            input_channels = output_channels\n        \n        # Dense layer parameters\n        # First dense layer: flattened_size * dense_neurons + dense_neurons (bias)\n        total += self.flattened_size * self.hparams.dense_neurons + self.hparams.dense_neurons\n        # Output layer: dense_neurons * num_classes + num_classes (bias)\n        total += self.hparams.dense_neurons * self.num_classes + self.num_classes\n        \n        return total\n    \n    def calculate_total_computations(self):\n        \"\"\"Calculate the total number of computations in the network\"\"\"\n        total = 0\n        \n        # Convolutional layers computations\n        input_channels = 3\n        for i in range(5):\n            output_channels = self.filter_counts[i]\n            filter_size = self.filter_sizes[i]\n            feature_size = self.feature_sizes[i]\n            \n            # Convolution computations: \n            # out_channels * in_channels * filter_height * filter_width * feature_height * feature_width\n            comp = output_channels * input_channels * filter_size * filter_size * feature_size * feature_size\n            \n            total += comp\n            input_channels = output_channels\n        \n        # Dense layer computations\n        # First dense layer: flattened_size * dense_neurons\n        total += self.flattened_size * self.hparams.dense_neurons\n        # Output layer: dense_neurons * num_classes\n        total += self.hparams.dense_neurons * self.num_classes\n        \n        return total\n    \n    def configure_optimizers(self):\n        \"\"\"Configure optimizer\"\"\"\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n    \n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n        \n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        \"\"\"Test step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('test_loss', loss, prog_bar=True)\n        self.log('test_acc', acc, prog_bar=True)\n        \n        return {'loss': loss, 'preds': preds, 'targets': y}\n    \n    def test_epoch_end(self, outputs):\n        \"\"\"Gather predictions after test epoch\"\"\"\n        all_preds = torch.cat([x['preds'] for x in outputs])\n        all_targets = torch.cat([x['targets'] for x in outputs])\n        \n        # Calculate confusion matrix and class accuracies\n        conf_matrix = torch.zeros(self.num_classes, self.num_classes)\n        for t, p in zip(all_targets, all_preds):\n            conf_matrix[t.long(), p.long()] += 1\n            \n        # Log confusion matrix\n        class_acc = conf_matrix.diag() / conf_matrix.sum(1)\n        \n        # Log to wandb\n        wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n            preds=all_preds.cpu().numpy(),\n            y_true=all_targets.cpu().numpy(),\n            class_names=[str(i) for i in range(self.num_classes)])})\n        \n        return {'test_acc': (all_preds == all_targets).float().mean()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:49:17.303051Z","iopub.execute_input":"2025-04-11T09:49:17.303450Z","iopub.status.idle":"2025-04-11T09:49:17.325784Z","shell.execute_reply.started":"2025-04-11T09:49:17.303419Z","shell.execute_reply":"2025-04-11T09:49:17.324791Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class iNaturalistDataModule(LightningModule):\n    def __init__(self, data_dir='./inaturalist_data', batch_size=32, num_workers=4, \n                 input_size=224, val_split=0.2, augmentation=False):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.input_size = input_size\n        self.val_split = val_split\n        self.augmentation = augmentation\n        \n    def setup(self, stage=None):\n        \"\"\"Setup data transformations and load datasets\"\"\"\n        # Define transformations\n        if self.augmentation:\n            train_transform = transforms.Compose([\n                transforms.RandomResizedCrop(self.input_size),\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(10),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            train_transform = transforms.Compose([\n                transforms.Resize((self.input_size, self.input_size)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n            \n        val_transform = transforms.Compose([\n            transforms.Resize((self.input_size, self.input_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Load datasets\n        train_dir = os.path.join(self.data_dir, 'train')\n        test_dir = os.path.join(self.data_dir, 'test')\n        \n        self.train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n        self.test_dataset = ImageFolder(root=test_dir, transform=val_transform)\n        \n        # Split train set into train and validation\n        dataset_size = len(self.train_dataset)\n        indices = list(range(dataset_size))\n        np.random.shuffle(indices)\n        \n        # Create stratified split\n        class_indices = defaultdict(list)\n        for idx, (_, label) in enumerate(self.train_dataset.samples):\n            class_indices[label].append(idx)\n        \n        train_indices = []\n        val_indices = []\n        \n        for class_idx, indices in class_indices.items():\n            np.random.shuffle(indices)\n            split_idx = int(len(indices) * (1 - self.val_split))\n            train_indices.extend(indices[:split_idx])\n            val_indices.extend(indices[split_idx:])\n        \n        # Create samplers for train and validation sets\n        self.train_sampler = SubsetRandomSampler(train_indices)\n        self.val_sampler = SubsetRandomSampler(val_indices)\n        \n        # Create a validation dataset with the same transforms as test\n        self.val_dataset = ImageFolder(root=train_dir, transform=val_transform)\n        \n    def train_dataloader(self):\n        \"\"\"Return train dataloader\"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=self.train_sampler,\n            num_workers=self.num_workers\n        )\n    \n    def val_dataloader(self):\n        \"\"\"Return validation dataloader\"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            sampler=self.val_sampler,\n            num_workers=self.num_workers\n        )\n    \n    def test_dataloader(self):\n        \"\"\"Return test dataloader\"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:53:34.392855Z","iopub.execute_input":"2025-04-11T09:53:34.393261Z","iopub.status.idle":"2025-04-11T09:53:34.406986Z","shell.execute_reply.started":"2025-04-11T09:53:34.393231Z","shell.execute_reply":"2025-04-11T09:53:34.405546Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def setup_wandb_sweep():\n    \"\"\"Define sweep configuration for hyperparameter tuning\"\"\"\n    sweep_config = {\n        'method': 'bayes',  # Bayesian optimization\n        'metric': {\n            'name': 'val_acc',\n            'goal': 'maximize'\n        },\n        'parameters': {\n            'filter_counts_strategy': {\n                'values': ['same', 'doubling', 'halving']  # Different filter count strategies\n            },\n            'base_filters': {\n                'values': [16, 32, 64]  # Base number of filters\n            },\n            'filter_size': {\n                'values': [3, 5]  # Filter sizes\n            },\n            'activation': {\n                'values': ['relu', 'gelu', 'silu', 'mish']  # Different activation functions\n            },\n            'dense_neurons': {\n                'values': [128, 256, 512]  # Number of neurons in dense layer\n            },\n            'dropout_rate': {\n                'values': [0.2, 0.3, 0.5]  # Dropout rate\n            },\n            'learning_rate': {\n                'values': [0.0001, 0.001, 0.01]  # Learning rate\n            },\n            'batch_norm': {\n                'values': [True, False]  # Whether to use batch normalization\n            },\n            'batch_size': {\n                'values': [16, 32, 64]  # Batch size\n            },\n            'augmentation': {\n                'values': [True, False]  # Whether to use data augmentation\n            }\n        }\n    }\n    \n    return sweep_config\n\ndef train_model_sweep():\n    \"\"\"Training function for sweep\"\"\"\n    # Initialize wandb\n    wandb.init()\n    \n    # Get hyperparameters from wandb\n    config = wandb.config\n    \n    # Generate filter counts based on strategy\n    if config.filter_counts_strategy == 'same':\n        filter_counts = [config.base_filters] * 5\n    elif config.filter_counts_strategy == 'doubling':\n        filter_counts = [config.base_filters * (2**i) for i in range(5)]\n    elif config.filter_counts_strategy == 'halving':\n        filter_counts = [config.base_filters * (2**(4-i)) for i in range(5)]\n    \n    # Generate filter sizes\n    filter_sizes = [config.filter_size] * 5\n    \n    # Create data module\n    data_module = iNaturalistDataModule(\n        batch_size=config.batch_size,\n        augmentation=config.augmentation\n    )\n    data_module.setup()\n    \n    # Create model with hyperparameters\n    model = CustomCNN(\n        num_classes=10,  # Assuming 10 classes in iNaturalist subset\n        filter_counts=filter_counts,\n        filter_sizes=filter_sizes,\n        activation=config.activation,\n        dense_neurons=config.dense_neurons,\n        dropout_rate=config.dropout_rate,\n        learning_rate=config.learning_rate,\n        batch_norm=config.batch_norm\n    )\n    \n    # Setup callbacks\n    callbacks = [\n        ModelCheckpoint(\n            monitor='val_acc',\n            filename='best-{epoch:02d}-{val_acc:.4f}',\n            save_top_k=1,\n            mode='max'\n        ),\n        EarlyStopping(\n            monitor='val_acc',\n            patience=5,\n            mode='max'\n        )\n    ]\n    \n    # Setup wandb logger\n    wandb_logger = WandbLogger(project=\"inaturalist_cnn_sweep\")\n    \n    # Create trainer\n    trainer = Trainer(\n        max_epochs=20,\n        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n        devices=1,\n        callbacks=callbacks,\n        logger=wandb_logger,\n        log_every_n_steps=10\n    )\n    \n    # Train model\n    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n    \n    # Get best validation accuracy\n    best_val_acc = trainer.callback_metrics.get('val_acc', 0)\n    \n    # Log metrics\n    wandb.log({\n        'best_val_acc': best_val_acc,\n        'total_params': model.total_params,\n        'total_computations': model.total_computations\n    })\n    \n    return model, best_val_acc\n\ndef run_sweep():\n    \"\"\"Run the sweep\"\"\"\n    # Initialize wandb\n    wandb.login()\n    \n    # Setup sweep\n    sweep_config = setup_wandb_sweep()\n    \n    # Create sweep\n    sweep_id = wandb.sweep(sweep_config, project=\"inaturalist_cnn_sweep\")\n    \n    # Run sweep\n    wandb.agent(sweep_id, function=train_model_sweep, count=30)  # Run 30 experiments","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_final_model(config):\n    \"\"\"Train final model with best hyperparameters\"\"\"\n    # Initialize wandb\n    wandb.init(project=\"inaturalist_cnn_final\", config=config)\n    \n    # Generate filter counts based on strategy\n    if config['filter_counts_strategy'] == 'same':\n        filter_counts = [config['base_filters']] * 5\n    elif config['filter_counts_strategy'] == 'doubling':\n        filter_counts = [config['base_filters'] * (2**i) for i in range(5)]\n    elif config['filter_counts_strategy'] == 'halving':\n        filter_counts = [config['base_filters'] * (2**(4-i)) for i in range(5)]\n    \n    # Generate filter sizes\n    filter_sizes = [config['filter_size']] * 5\n    \n    # Create data module\n    data_module = iNaturalistDataModule(\n        batch_size=config['batch_size'],\n        augmentation=config['augmentation']\n    )\n    data_module.setup()\n    \n    # Create model with hyperparameters\n    model = CustomCNN(\n        num_classes=10,  # Assuming 10 classes in iNaturalist subset\n        filter_counts=filter_counts,\n        filter_sizes=filter_sizes,\n        activation=config['activation'],\n        dense_neurons=config['dense_neurons'],\n        dropout_rate=config['dropout_rate'],\n        learning_rate=config['learning_rate'],\n        batch_norm=config['batch_norm']\n    )\n    \n    # Log model summary\n    wandb.watch(model, log=\"all\")\n    \n    # Setup callbacks\n    callbacks = [\n        ModelCheckpoint(\n            monitor='val_acc',\n            filename='best-{epoch:02d}-{val_acc:.4f}',\n            save_top_k=1,\n            mode='max'\n        )\n    ]\n    \n    # Setup wandb logger\n    wandb_logger = WandbLogger(project=\"inaturalist_cnn_final\")\n    \n    # Create trainer\n    trainer = Trainer(\n        max_epochs=50,\n        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n        devices=1,\n        callbacks=callbacks,\n        logger=wandb_logger,\n        log_every_n_steps=10\n    )\n    \n    # Train model\n    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n    \n    # Test model\n    test_results = trainer.test(model, dataloader=data_module.test_dataloader())\n    \n    # Log test results\n    wandb.log({\n        'test_acc': test_results[0]['test_acc'],\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Log model architecture\n    wandb.log({\n        'total_params': model.total_params,\n        'total_computations': model.total_computations\n    })\n    \n    return model, test_results\n\ndef visualize_test_samples(model, data_module, num_samples=30):\n    \"\"\"Visualize test samples with predictions\"\"\"\n    # Get test dataloader\n    test_loader = data_module.test_dataloader()\n    \n    # Get a batch of test data\n    images, labels = next(iter(test_loader))\n    \n    # Move to device\n    model = model.to(device)\n    images = images.to(device)\n    \n    # Make predictions\n    with torch.no_grad():\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n    \n    # Convert to numpy for visualization\n    images = images.cpu().numpy()\n    labels = labels.cpu().numpy()\n    predicted = predicted.cpu().numpy()\n    \n    # Get class names\n    class_names = data_module.test_dataset.classes\n    \n    # Visualize images with predictions\n    fig, axes = plt.subplots(10, 3, figsize=(15, 30))\n    \n    for i, ax in enumerate(axes.flat):\n        if i < num_samples:\n            # Transpose image from (C, H, W) to (H, W, C)\n            img = np.transpose(images[i], (1, 2, 0))\n            \n            # Denormalize\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            img = std * img + mean\n            img = np.clip(img, 0, 1)\n            \n            # Plot image\n            ax.imshow(img)\n            \n            # Get true and predicted labels\n            true_label = class_names[labels[i]]\n            pred_label = class_names[predicted[i]]\n            \n            # Set title\n            if labels[i] == predicted[i]:\n                ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color='green')\n            else:\n                ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color='red')\n            \n            ax.axis('off')\n    \n    plt.tight_layout()\n    \n    # Log to wandb\n    wandb.log({\"test_predictions\": wandb.Image(plt)})\n\ndef visualize_filters(model):\n    \"\"\"Visualize filters in the first convolutional layer\"\"\"\n    # Get first layer filters\n    filters = model.conv_layers[0][0].weight.data.cpu().numpy()\n    \n    # Create figure\n    fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n    \n    # Plot filters\n    for i, ax in enumerate(axes.flat):\n        if i < filters.shape[0]:\n            # Normalize filter for visualization\n            f = filters[i].transpose(1, 2, 0)\n            f = (f - f.min()) / (f.max() - f.min())\n            \n            # Plot filter\n            ax.imshow(f)\n            ax.axis('off')\n    \n    plt.tight_layout()\n    \n    # Log to wandb\n    wandb.log({\"first_layer_filters\": wandb.Image(plt)})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part B","metadata":{}},{"cell_type":"code","source":"class PretrainedModel(LightningModule):\n    def __init__(self, model_name='resnet50', num_classes=10, learning_rate=0.001, \n                 fine_tuning_strategy='last_layer', feature_extract=True, unfreeze_layers=3):\n        \"\"\"\n        Fine-tune a pre-trained model\n        \n        Args:\n            model_name (str): Name of pre-trained model ('resnet50', 'vgg16', etc.)\n            num_classes (int): Number of output classes\n            learning_rate (float): Learning rate for optimizer\n            fine_tuning_strategy (str): Strategy for fine-tuning ('last_layer', 'all_layers', 'k_last_layers')\n            feature_extract (bool): If True, only update the reshaped layer params\n            unfreeze_layers (int): Number of layers to unfreeze for 'k_last_layers' strategy\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.model_name = model_name\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n        self.fine_tuning_strategy = fine_tuning_strategy\n        self.unfreeze_layers = unfreeze_layers\n        \n        # Initialize the pre-trained model\n        self.model = self._initialize_model()\n        \n    def _initialize_model(self):\n        \"\"\"Initialize the pre-trained model\"\"\"\n        if self.model_name == 'resnet50':\n            model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)\n            num_ftrs = model.fc.in_features\n            model.fc = nn.Linear(num_ftrs, self.num_classes)\n        \n        elif self.model_name == 'vgg16':\n            model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n            num_ftrs = model.classifier[6].in_features\n            model.classifier[6] = nn.Linear(num_ftrs, self.num_classes)\n        \n        elif self.model_name == 'googlenet':\n            model = torchvision.models.googlenet(weights=torchvision.models.GoogLeNet_Weights.IMAGENET1K_V1)\n            num_ftrs = model.fc.in_features\n            model.fc = nn.Linear(num_ftrs, self.num_classes)\n        \n        elif self.model_name == 'efficientnet_v2_s':\n            model = torchvision.models.efficientnet_v2_s(weights=torchvision.models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n            num_ftrs = model.classifier[1].in_features\n            model.classifier[1] = nn.Linear(num_ftrs, self.num_classes)\n        \n        elif self.model_name == 'vit_b_16':\n            model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n            num_ftrs = model.heads.head.in_features\n            model.heads.head = nn.Linear(num_ftrs, self.num_classes)\n        \n        # Apply fine-tuning strategy\n        self._apply_fine_tuning_strategy(model)\n        \n        return model\n    \n    def _apply_fine_tuning_strategy(self, model):\n        \"\"\"Apply the fine-tuning strategy to the model\"\"\"\n        if self.fine_tuning_strategy == 'last_layer':\n            # Freeze all parameters\n            for param in model.parameters():\n                param.requires_grad = False\n            \n            # Unfreeze the last layer\n            if self.model_name == 'resnet50':\n                for param in model.fc.parameters():\n                    param.requires_grad = True\n            \n            elif self.model_name == 'vgg16':\n                for param in model.classifier[6].parameters():\n                    param.requires_grad = True\n            \n            elif self.model_name == 'googlenet':\n                for param in model.fc.parameters():\n                    param.requires_grad = True\n            \n            elif self.model_name == 'efficientnet_v2_s':\n                for param in model.classifier[1].parameters():\n                    param.requires_grad = True\n            \n            elif self.model_name == 'vit_b_16':\n                for param in model.heads.head.parameters():\n                    param.requires_grad = True\n        \n        elif self.fine_tuning_strategy == 'all_layers':\n            # Unfreeze all parameters\n            for param in model.parameters():\n                param.requires_grad = True\n        \n        elif self.fine_tuning_strategy == 'k_last_layers':\n            # First freeze all parameters\n            for param in model.parameters():\n                param.requires_grad = False\n                \n            # Then unfreeze the last k layers based on model architecture\n            if self.model_name == 'resnet50':\n                # Unfreeze final layers\n                layers_to_unfreeze = [model.fc]\n                if self.unfreeze_layers > 1:\n                    layers_to_unfreeze.append(model.layer4)\n                if self.unfreeze_layers > 2:\n                    layers_to_unfreeze.append(model.layer3)\n                \n                for layer in layers_to_unfreeze:\n                    for param in layer.parameters():\n                        param.requires_grad = True\n            \n            elif self.model_name == 'vgg16':\n                # Unfreeze final classifier layers\n                num_unfrozen = min(self.unfreeze_layers, len(model.classifier))\n                for i in range(len(model.classifier) - num_unfrozen, len(model.classifier)):\n                    for param in model.classifier[i].parameters():\n                        param.requires_grad = True\n                        \n                # If needed, also unfreeze some feature layers\n                if self.unfreeze_layers > len(model.classifier):\n                    remaining = self.unfreeze_layers - len(model.classifier)\n                    num_unfrozen_features = min(remaining, len(model.features))\n                    for i in range(len(model.features) - num_unfrozen_features, len(model.features)):\n                        for param in model.features[i].parameters():\n                            param.requires_grad = True\n    \n    def forward(self, x):\n        \"\"\"Forward pass\"\"\"\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        \"\"\"Configure optimizer\"\"\"\n        params_to_update = [p for p in self.parameters() if p.requires_grad]\n        optimizer = optim.Adam(params_to_update, lr=self.learning_rate)\n        return optimizer\n    \n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n        \n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        \"\"\"Test step\"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log metrics\n        self.log('test_loss', loss, prog_bar=True)\n        self.log('test_acc', acc, prog_bar=True)\n        \n        return {'loss': loss, 'preds': preds, 'targets': y}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_finetune_model(config):\n    \"\"\"Train a fine-tuned model with specific configuration\"\"\"\n    # Initialize wandb\n    wandb.init(project=\"inaturalist_finetune\", config=config)\n    \n    # Create data module\n    data_module = iNaturalistDataModule(\n        batch_size=config['batch_size'],\n        augmentation=True  # Always use augmentation for fine-tuning\n    )\n    data_module.setup()\n    \n    # Create model\n    model = PretrainedModel(\n        model_name=config['model_name'],\n        num_classes=10,  # Assuming 10 classes in iNaturalist subset\n        learning_rate=config['learning_rate'],\n        fine_tuning_strategy=config['fine_tuning_strategy'],\n        unfreeze_layers=config.get('unfreeze_layers', 3)\n    )\n    \n    # Log model summary\n    wandb.watch(model, log=\"all\")\n    \n    # Setup callbacks\n    callbacks = [\n        ModelCheckpoint(\n            monitor='val_acc',\n            filename='best-{epoch:02d}-{val_acc:.4f}',\n            save_top_k=1,\n            mode='max'\n        ),\n        EarlyStopping(\n            monitor='val_acc',\n            patience=5,\n            mode='max'\n        )\n    ]\n    \n    # Setup wandb logger\n    wandb_logger = WandbLogger(project=\"inaturalist_finetune\")\n    \n    # Create trainer (with mixed precision to speed up training)\n    trainer = Trainer(\n        max_epochs=20,\n        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n        devices=1,\n        callbacks=callbacks,\n        logger=wandb_logger,\n        log_every_n_steps=10,\n        precision=16  # Use mixed precision\n    )\n    \n    # Train model\n    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n    \n    # Test model\n    test_results = trainer.test(model, dataloader=data_module.test_dataloader())\n    \n    # Log test results\n    wandb.log({\n        'test_acc': test_results[0]['test_acc'],\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    return model, test_results\n\ndef compare_finetuning_strategies():\n    \"\"\"Compare different fine-tuning strategies\"\"\"\n    # Different fine-tuning configurations to try\n    configurations = [\n        {\n            'model_name': 'resnet50',\n            'fine_tuning_strategy': 'last_layer',\n            'learning_rate': 0.001,\n            'batch_size': 32\n        },\n        {\n            'model_name': 'resnet50',\n            'fine_tuning_strategy': 'k_last_layers',\n            'unfreeze_layers': 3,\n            'learning_rate': 0.0001,\n            'batch_size': 32\n        },\n        {\n            'model_name': 'resnet50',\n            'fine_tuning_strategy': 'all_layers',\n            'learning_rate': 0.00001,\n            'batch_size': 32\n        },\n        # Try with a different model\n        {\n            'model_name': 'efficientnet_v2_s',\n            'fine_tuning_strategy': 'last_layer',\n            'learning_rate': 0.001,\n            'batch_size': 32\n        },\n        {\n            'model_name': 'vit_b_16',\n            'fine_tuning_strategy': 'last_layer',\n            'learning_rate': 0.001,\n            'batch_size': 32\n        }\n    ]\n    \n    results = []\n    \n    for config in configurations:\n        model, test_result = train_finetune_model(config)\n        results.append({\n            'config': config,\n            'test_acc': test_result[0]['test_acc'],\n            'test_loss': test_result[0]['test_loss']\n        })\n    \n    # Compare results\n    for result in results:\n        print(f\"Model: {result['config']['model_name']}, \"\n              f\"Strategy: {result['config']['fine_tuning_strategy']}, \"\n              f\"Test Acc: {result['test_acc']:.4f}\")\n    \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"# Main script to run the assignment\n\nimport os\nimport argparse\nimport torch\nimport wandb\nfrom datetime import datetime\n\n# Import custom modules\n# Note: Make sure all the previous code blocks are saved in appropriate Python files\n\ndef main():\n    \"\"\"Main function to run the assignment\"\"\"\n    parser = argparse.ArgumentParser(description='DA6401 Assignment 2 - CNN Training')\n    parser.add_argument('--part', type=str, default='both', choices=['a', 'b', 'both'], \n                        help='Which part of the assignment to run (a, b, or both)')\n    parser.add_argument('--sweep', action='store_true', help='Run hyperparameter sweep')\n    parser.add_argument('--train', action='store_true', help='Train final model')\n    parser.add_argument('--test', action='store_true', help='Test model')\n    parser.add_argument('--data_dir', type=str, default='./inaturalist_data', \n                        help='Directory containing the dataset')\n    parser.add_argument('--output_dir', type=str, default='./output', \n                        help='Directory to save output')\n    args = parser.parse_args()\n    \n    # Create output directory\n    os.makedirs(args.output_dir, exist_ok=True)\n    \n    # Initialize wandb\n    wandb.login()\n    \n    if args.part in ['a', 'both']:\n        print(\"Running Part A: Training from Scratch\")\n        \n        # Run hyperparameter sweep\n        if args.sweep:\n            print(\"Running hyperparameter sweep...\")\n            run_sweep()\n        \n        # Train final model with best hyperparameters\n        if args.train:\n            print(\"Training final model...\")\n            \n            # Best hyperparameters from sweep\n            best_config = {\n                'filter_counts_strategy': 'doubling',\n                'base_filters': 32,\n                'filter_size': 3,\n                'activation': 'relu',\n                'dense_neurons': 512,\n                'dropout_rate': 0.3,\n                'learning_rate': 0.001,\n                'batch_norm': True,\n                'batch_size': 32,\n                'augmentation': True\n            }\n            \n            # Train model\n            model_a, test_results_a = train_final_model(best_config)\n            \n            # Test and visualize\n            if args.test:\n                data_module = iNaturalistDataModule(\n                    data_dir=args.data_dir,\n                    batch_size=best_config['batch_size'],\n                    augmentation=best_config['augmentation']\n                )\n                data_module.setup()\n                \n                visualize_test_samples(model_a, data_module)\n                visualize_filters(model_a)\n    \n    if args.part in ['b', 'both']:\n        print(\"Running Part B: Fine-tuning Pre-trained Model\")\n        \n        # Train and compare different fine-tuning strategies\n        if args.train:\n            print(\"Training and comparing fine-tuning strategies...\")\n            results = compare_finetuning_strategies()\n            \n            # Log comparison results\n            wandb.init(project=\"inaturalist_finetune_comparison\")\n            \n            # Create a table for the results\n            table = wandb.Table(columns=[\"Model\", \"Strategy\", \"Test Accuracy\"])\n            \n            for result in results:\n                table.add_data(\n                    result['config']['model_name'],\n                    result['config']['fine_tuning_strategy'],\n                    result['test_acc']\n                )\n            \n            wandb.log({\"finetuning_comparison\": table})\n            \n            # Find best model\n            best_result = max(results, key=lambda x: x['test_acc'])\n            print(f\"Best fine-tuning result: {best_result}\")\n            \n            # Test and visualize best model\n            if args.test:\n                # Train the best model again\n                best_model, _ = train_finetune_model(best_result['config'])\n                \n                # Setup data module\n                data_module = iNaturalistDataModule(\n                    data_dir=args.data_dir,\n                    batch_size=best_result['config']['batch_size'],\n                    augmentation=True\n                )\n                data_module.setup()\n                \n                # Visualize test samples\n                visualize_test_samples(best_model, data_module)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}